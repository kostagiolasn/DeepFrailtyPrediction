{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, UpSampling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the random seeds\n",
    "random.seed(1)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "\n",
    "# Description: This function is responsible for loading our data.\n",
    "# Args in: filepath - the path of the .mat file containing the data\n",
    "# Returns: inputs - the data, labels - the labels corresponding to the data, \n",
    "#            patients - the patients corresponding to the data\n",
    "\n",
    "    mat = scipy.io.loadmat(filepath)\n",
    "    inputs = mat['Xrec'][:]\n",
    "    labels = mat['Y']\n",
    "    patients = mat['patientID']\n",
    "\n",
    "    labels = np.einsum('ij->ji', labels)\n",
    "    labels = [label for sublist in labels for label in sublist]\n",
    "    patients = np.einsum('ij->ji', patients)\n",
    "    patients = [patient for sublist in patients for patient in sublist]\n",
    "\n",
    "    return inputs, labels, patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_duplicates(duplicates_list, patients):\n",
    "\n",
    "# Description: This function is responsible for filtering out\n",
    "# some patients that are found to be present in more than one classes.\n",
    "# Args in & Returns are self-explanatory.\n",
    "    \n",
    "    patients = list(patients)\n",
    "    \n",
    "    for duplicate in duplicates_list:\n",
    "        patients = list(filter(lambda a: a != duplicate, patients))\n",
    "    \n",
    "    return np.asarray(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, labels, patients = load_data('/home/nikos/Desktop/Zacharaki/PARAFAC missing values 0_90/ReconstructedTensorAndFeatures90Missing_StrSGD.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = filter_duplicates([1002, 1104], patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(inputs, targets, patients, patients_for_val):\n",
    "\n",
    "# Description: This function is responsible for splitting our data into training and test data\n",
    "# Args in: patients_for_val - list containing the patients for our test data\n",
    "# Returns: the data and labels of our training and test data\n",
    "\n",
    "    X_train_size = X_val_size = 0\n",
    "\n",
    "    patients_for_train = [item for item in list(np.unique(patients)) if item not in patients_for_val]\n",
    "    idpatients_val = []\n",
    "    for patient in patients_for_val:\n",
    "        idpatient = [i for i, x in enumerate(patients) if x == patient]\n",
    "        idpatients_val.append(idpatient)\n",
    "    idpatients_val = [item for sublist in idpatients_val for item in sublist]\n",
    "    X_val = [inputs[i] for i in idpatients_val]\n",
    "    y_val = [targets[i] for i in idpatients_val]\n",
    "\n",
    "    idpatients_train = list(set([i for i in range(inputs.shape[0])]) - set(idpatients_val))\n",
    "    inputs = [inputs[i] for i in idpatients_train]\n",
    "    targets = [targets[i] for i in idpatients_train]\n",
    "\n",
    "    return np.asarray(inputs), np.asarray(targets), np.asarray(X_val), np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(inputs, labels, patients, [1106, 1107, 2097])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "\n",
    "# Description: This function preprocesses our data. \n",
    "# We want to ensure that our training data have zero mean and unit variance. \n",
    "# We also use subtract the same mean from the test data and then devide them by\n",
    "# the same standard deviation. We do that to ensure that no information about the\n",
    "# test set distribution is known ahead of time.\n",
    "\n",
    "# Args in & Returns are self-explanatory.\n",
    "\n",
    "    X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis = 0)\n",
    "    \n",
    "    X_train, X_val, train_ground, valid_ground = train_test_split(X_train, X_train, test_size = 0.1, random_state = 13)\n",
    "\n",
    "    X_test = (X_test -  np.mean(X_train, axis=0)) / np.std(X_train, axis = 0)\n",
    "    \n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1] * X_val.shape[2]))\n",
    "    train_ground = np.reshape(train_ground, (train_ground.shape[0], train_ground.shape[1] * train_ground.shape[2]))\n",
    "    valid_ground = np.reshape(valid_ground, (valid_ground.shape[0], valid_ground.shape[1] * valid_ground.shape[2]))\n",
    "    \n",
    "    return X_train, X_val, train_ground, valid_ground, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, train_ground, valid_ground, X_test = preprocess_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10500)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                336032    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10500)             346500    \n",
      "=================================================================\n",
      "Total params: 682,532\n",
      "Trainable params: 682,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 16512 samples, validate on 1835 samples\n",
      "Epoch 1/30\n",
      " - 2s - loss: 0.7392 - mean_squared_error: 0.7392 - val_loss: 0.6225 - val_mean_squared_error: 0.6225\n",
      "Epoch 2/30\n",
      " - 2s - loss: 0.6591 - mean_squared_error: 0.6591 - val_loss: 0.6166 - val_mean_squared_error: 0.6166\n",
      "Epoch 3/30\n",
      " - 2s - loss: 0.6559 - mean_squared_error: 0.6559 - val_loss: 0.6168 - val_mean_squared_error: 0.6168\n",
      "Epoch 4/30\n",
      " - 3s - loss: 0.6548 - mean_squared_error: 0.6548 - val_loss: 0.6140 - val_mean_squared_error: 0.6140\n",
      "Epoch 5/30\n",
      " - 3s - loss: 0.6530 - mean_squared_error: 0.6530 - val_loss: 0.6128 - val_mean_squared_error: 0.6128\n",
      "Epoch 6/30\n",
      " - 3s - loss: 0.6520 - mean_squared_error: 0.6520 - val_loss: 0.6123 - val_mean_squared_error: 0.6123\n",
      "Epoch 7/30\n",
      " - 3s - loss: 0.6510 - mean_squared_error: 0.6510 - val_loss: 0.6106 - val_mean_squared_error: 0.6106\n",
      "Epoch 8/30\n",
      " - 2s - loss: 0.6493 - mean_squared_error: 0.6493 - val_loss: 0.6105 - val_mean_squared_error: 0.6105\n",
      "Epoch 9/30\n",
      " - 2s - loss: 0.6480 - mean_squared_error: 0.6480 - val_loss: 0.6094 - val_mean_squared_error: 0.6094\n",
      "Epoch 10/30\n",
      " - 2s - loss: 0.6474 - mean_squared_error: 0.6474 - val_loss: 0.6081 - val_mean_squared_error: 0.6081\n",
      "Epoch 11/30\n",
      " - 2s - loss: 0.6460 - mean_squared_error: 0.6460 - val_loss: 0.6059 - val_mean_squared_error: 0.6059\n",
      "Epoch 12/30\n",
      " - 2s - loss: 0.6448 - mean_squared_error: 0.6448 - val_loss: 0.6050 - val_mean_squared_error: 0.6050\n",
      "Epoch 13/30\n",
      " - 2s - loss: 0.6439 - mean_squared_error: 0.6439 - val_loss: 0.6044 - val_mean_squared_error: 0.6044\n",
      "Epoch 14/30\n",
      " - 3s - loss: 0.6433 - mean_squared_error: 0.6433 - val_loss: 0.6040 - val_mean_squared_error: 0.6040\n",
      "Epoch 15/30\n",
      " - 2s - loss: 0.6426 - mean_squared_error: 0.6426 - val_loss: 0.6030 - val_mean_squared_error: 0.6030\n",
      "Epoch 16/30\n",
      " - 3s - loss: 0.6426 - mean_squared_error: 0.6426 - val_loss: 0.6028 - val_mean_squared_error: 0.6028\n",
      "Epoch 17/30\n",
      " - 3s - loss: 0.6414 - mean_squared_error: 0.6414 - val_loss: 0.6020 - val_mean_squared_error: 0.6020\n",
      "Epoch 18/30\n",
      " - 3s - loss: 0.6407 - mean_squared_error: 0.6407 - val_loss: 0.6013 - val_mean_squared_error: 0.6013\n",
      "Epoch 19/30\n",
      " - 3s - loss: 0.6403 - mean_squared_error: 0.6403 - val_loss: 0.6004 - val_mean_squared_error: 0.6004\n",
      "Epoch 20/30\n",
      " - 2s - loss: 0.6399 - mean_squared_error: 0.6399 - val_loss: 0.6001 - val_mean_squared_error: 0.6001\n",
      "Epoch 21/30\n",
      " - 3s - loss: 0.6396 - mean_squared_error: 0.6396 - val_loss: 0.6000 - val_mean_squared_error: 0.6000\n",
      "Epoch 22/30\n",
      " - 2s - loss: 0.6390 - mean_squared_error: 0.6390 - val_loss: 0.5995 - val_mean_squared_error: 0.5995\n",
      "Epoch 23/30\n",
      " - 2s - loss: 0.6390 - mean_squared_error: 0.6390 - val_loss: 0.5988 - val_mean_squared_error: 0.5988\n",
      "Epoch 24/30\n",
      " - 2s - loss: 0.6382 - mean_squared_error: 0.6382 - val_loss: 0.5987 - val_mean_squared_error: 0.5987\n",
      "Epoch 25/30\n",
      " - 3s - loss: 0.6381 - mean_squared_error: 0.6381 - val_loss: 0.5980 - val_mean_squared_error: 0.5980\n",
      "Epoch 26/30\n",
      " - 2s - loss: 0.6380 - mean_squared_error: 0.6380 - val_loss: 0.5982 - val_mean_squared_error: 0.5982\n",
      "Epoch 27/30\n",
      " - 2s - loss: 0.6375 - mean_squared_error: 0.6375 - val_loss: 0.5982 - val_mean_squared_error: 0.5982\n",
      "Epoch 28/30\n",
      " - 2s - loss: 0.6376 - mean_squared_error: 0.6376 - val_loss: 0.5976 - val_mean_squared_error: 0.5976\n",
      "Epoch 29/30\n",
      " - 2s - loss: 0.6371 - mean_squared_error: 0.6371 - val_loss: 0.5977 - val_mean_squared_error: 0.5977\n",
      "Epoch 30/30\n",
      " - 3s - loss: 0.6370 - mean_squared_error: 0.6370 - val_loss: 0.5969 - val_mean_squared_error: 0.5969\n"
     ]
    }
   ],
   "source": [
    "input_signal = Input(shape = (X_train.shape[1],))\n",
    "encoded = Dense(32, activation = 'relu')(input_signal)\n",
    "\n",
    "# decoder\n",
    "decoded = Dense(1500*7, activation = 'sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input = input_signal, output = decoded)\n",
    "\n",
    "encoder = Model(input = input_signal, output = encoded)\n",
    "encoded_input = Input(shape=(32,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=['mse'])\n",
    "print(autoencoder.summary())\n",
    "history = autoencoder.fit(X_train, train_ground, epochs=30, batch_size=128, \n",
    "                                    validation_data=(X_val, valid_ground), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_X_train = [encoder.predict(X_train), encoder.predict(X_val)]\n",
    "encoder_X_train = [i for sublist in encoder_X_train for i in sublist]\n",
    "encoder_X_test = encoder.predict(X_test)\n",
    "\n",
    "encoder_X_train = np.asarray(encoder_X_train)\n",
    "encoder_X_test = np.asarray(encoder_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = list(zip(encoder_X_train, y_train))\n",
    "random.shuffle(c)\n",
    "encoder_X_train, y_train = zip(*c)\n",
    "\n",
    "c = list(zip(encoder_X_test, y_test))\n",
    "random.shuffle(c)\n",
    "encoder_X_test, y_test = zip(*c)\n",
    "\n",
    "encoder_X_train = np.asarray(encoder_X_train)\n",
    "encoder_X_test = np.asarray(encoder_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(labels, n_class = 6):\n",
    "    \"\"\" One-hot encoding \"\"\"\n",
    "    expansion = np.eye(n_class)\n",
    "    y = []\n",
    "    for i in labels:\n",
    "        y.append(expansion[int(i)])\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import keras.backend as K\n",
    "\n",
    "class BalancedAccuracy(Callback):\n",
    "    def __init__(self, train_data, validation_data):\n",
    "        super(BalancedAccuracy, self).__init__()\n",
    "        self.acas = []\n",
    "        self.validation_data = validation_data\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        X_val = self.validation_data[0]\n",
    "        y_val = self.validation_data[1]\n",
    "\n",
    "        X_train = self.train_data[0]\n",
    "        y_train = self.train_data[1]\n",
    "\n",
    "        y_val_pred = self.model.predict(X_val)\n",
    "        y_train_pred = self.model.predict(X_train)\n",
    "\n",
    "        val_score = self.eval_avg_class_acc(y_val, y_val_pred)\n",
    "        train_score = self.eval_avg_class_acc(y_train, y_train_pred)\n",
    "\n",
    "        self.acas.append([val_score])\n",
    "        self.acas.append([train_score])\n",
    "\n",
    "        print(\"\\nBalanced Accuracy - train: %.3f \\t val: %.3f\"%(train_score, val_score))        \n",
    "        \n",
    "    def eval_avg_class_acc(self, y_true, y_pred):\n",
    "\n",
    "        # decode one-hot to single labels\n",
    "        y_pred = y_pred.round()\n",
    "        y_pred = [ np.argmax(pred, axis = 0) for pred in y_pred ]\n",
    "        y_true = [ np.argmax(label, axis = 0) for label in y_true ]\n",
    "\n",
    "        cf = confusion_matrix(y_true, y_pred)\n",
    "        if np.unique(y_true).shape[0] == 2:\n",
    "            sensitivity = float(cf[1][1]) / float((cf[1][1] + cf[1][0]))\n",
    "            specificity = float(cf[0][0]) / float((cf[0][1] + cf[0][0]))\n",
    "\n",
    "            balanced_acc = (sensitivity + specificity) / 2\n",
    "        else:\n",
    "            balanced_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        return balanced_acc\n",
    "\n",
    "def weighted_categorical_crossentropy(y_true, y_pred, weights):\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    # scale predictions so that the class probas of each sample sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calc\n",
    "    loss = y_true * K.log(y_pred) * weights\n",
    "    loss = -K.sum(loss, -1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Weighted loss function to tackle class imbalance in the dataset\n",
    "\n",
    "weights = np.array([float(len(y_train)) / float(list(y_train).count(0)), float(len(y_train)) / float(list(y_train).count(1)), float(len(y_train)) / float(list(y_train).count(2))])\n",
    "w_cat_crossentropy = partial(weighted_categorical_crossentropy, weights = weights)\n",
    "w_cat_crossentropy.__name__ = 'weighted_categorical_crossentropy'\n",
    "        \n",
    "balanced_accuracy = BalancedAccuracy(train_data = (encoder_X_train, y_train), validation_data = (encoder_X_test, y_test))\n",
    "CALLBACKS = [balanced_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18347 samples, validate on 1393 samples\n",
      "Epoch 1/500\n",
      " - 1s - loss: 1.9111 - acc: 0.3170 - val_loss: 1.2565 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.465 \t val: 0.250\n",
      "Epoch 2/500\n",
      " - 1s - loss: 1.8441 - acc: 0.3209 - val_loss: 1.2275 - val_acc: 0.2477\n",
      "\n",
      "Balanced Accuracy - train: 0.510 \t val: 0.265\n",
      "Epoch 3/500\n",
      " - 1s - loss: 1.7783 - acc: 0.3227 - val_loss: 1.2022 - val_acc: 0.2419\n",
      "\n",
      "Balanced Accuracy - train: 0.556 \t val: 0.324\n",
      "Epoch 4/500\n",
      " - 1s - loss: 1.7063 - acc: 0.3312 - val_loss: 1.1743 - val_acc: 0.2391\n",
      "\n",
      "Balanced Accuracy - train: 0.598 \t val: 0.467\n",
      "Epoch 5/500\n",
      " - 1s - loss: 1.6895 - acc: 0.3306 - val_loss: 1.1537 - val_acc: 0.2362\n",
      "\n",
      "Balanced Accuracy - train: 0.637 \t val: 0.464\n",
      "Epoch 6/500\n",
      " - 0s - loss: 1.6398 - acc: 0.3366 - val_loss: 1.1357 - val_acc: 0.2333\n",
      "\n",
      "Balanced Accuracy - train: 0.678 \t val: 0.463\n",
      "Epoch 7/500\n",
      " - 0s - loss: 1.6102 - acc: 0.3398 - val_loss: 1.1228 - val_acc: 0.2376\n",
      "\n",
      "Balanced Accuracy - train: 0.708 \t val: 0.463\n",
      "Epoch 8/500\n",
      " - 1s - loss: 1.6109 - acc: 0.3369 - val_loss: 1.1127 - val_acc: 0.2692\n",
      "\n",
      "Balanced Accuracy - train: 0.729 \t val: 0.463\n",
      "Epoch 9/500\n",
      " - 1s - loss: 1.5834 - acc: 0.3384 - val_loss: 1.1025 - val_acc: 0.3754\n",
      "\n",
      "Balanced Accuracy - train: 0.753 \t val: 0.463\n",
      "Epoch 10/500\n",
      " - 1s - loss: 1.5741 - acc: 0.3439 - val_loss: 1.0960 - val_acc: 0.4300\n",
      "\n",
      "Balanced Accuracy - train: 0.772 \t val: 0.463\n",
      "Epoch 11/500\n",
      " - 1s - loss: 1.5683 - acc: 0.3378 - val_loss: 1.0902 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.785 \t val: 0.463\n",
      "Epoch 12/500\n",
      " - 1s - loss: 1.5398 - acc: 0.3446 - val_loss: 1.0871 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.794 \t val: 0.463\n",
      "Epoch 13/500\n",
      " - 0s - loss: 1.5331 - acc: 0.3403 - val_loss: 1.0849 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.802 \t val: 0.463\n",
      "Epoch 14/500\n",
      " - 1s - loss: 1.5178 - acc: 0.3509 - val_loss: 1.0841 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.807 \t val: 0.463\n",
      "Epoch 15/500\n",
      " - 1s - loss: 1.5158 - acc: 0.3488 - val_loss: 1.0828 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.807 \t val: 0.463\n",
      "Epoch 16/500\n",
      " - 1s - loss: 1.4960 - acc: 0.3542 - val_loss: 1.0822 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.811 \t val: 0.463\n",
      "Epoch 17/500\n",
      " - 1s - loss: 1.4899 - acc: 0.3505 - val_loss: 1.0825 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.814 \t val: 0.463\n",
      "Epoch 18/500\n",
      " - 1s - loss: 1.4680 - acc: 0.3609 - val_loss: 1.0839 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.813 \t val: 0.463\n",
      "Epoch 19/500\n",
      " - 1s - loss: 1.4668 - acc: 0.3596 - val_loss: 1.0855 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.814 \t val: 0.463\n",
      "Epoch 20/500\n",
      " - 0s - loss: 1.4752 - acc: 0.3567 - val_loss: 1.0866 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.816 \t val: 0.463\n",
      "Epoch 21/500\n",
      " - 1s - loss: 1.4675 - acc: 0.3623 - val_loss: 1.0881 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.811 \t val: 0.463\n",
      "Epoch 22/500\n",
      " - 1s - loss: 1.4567 - acc: 0.3619 - val_loss: 1.0895 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.808 \t val: 0.463\n",
      "Epoch 23/500\n",
      " - 1s - loss: 1.4416 - acc: 0.3594 - val_loss: 1.0902 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.808 \t val: 0.463\n",
      "Epoch 24/500\n",
      " - 0s - loss: 1.4386 - acc: 0.3638 - val_loss: 1.0916 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.809 \t val: 0.463\n",
      "Epoch 25/500\n",
      " - 1s - loss: 1.4341 - acc: 0.3651 - val_loss: 1.0933 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.810 \t val: 0.463\n",
      "Epoch 26/500\n",
      " - 1s - loss: 1.4328 - acc: 0.3580 - val_loss: 1.0939 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.806 \t val: 0.463\n",
      "Epoch 27/500\n",
      " - 1s - loss: 1.4270 - acc: 0.3663 - val_loss: 1.0942 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.806 \t val: 0.463\n",
      "Epoch 28/500\n",
      " - 0s - loss: 1.4209 - acc: 0.3650 - val_loss: 1.0959 - val_acc: 0.4702\n",
      "\n",
      "Balanced Accuracy - train: 0.805 \t val: 0.463\n",
      "Epoch 29/500\n",
      " - 0s - loss: 1.4069 - acc: 0.3701 - val_loss: 1.0963 - val_acc: 0.4702\n",
      "\n",
      "Balanced Accuracy - train: 0.802 \t val: 0.463\n",
      "Epoch 30/500\n",
      " - 1s - loss: 1.3966 - acc: 0.3692 - val_loss: 1.0974 - val_acc: 0.4709\n",
      "\n",
      "Balanced Accuracy - train: 0.806 \t val: 0.463\n",
      "Epoch 31/500\n",
      " - 1s - loss: 1.4058 - acc: 0.3673 - val_loss: 1.1007 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.802 \t val: 0.463\n",
      "Epoch 32/500\n",
      " - 0s - loss: 1.3930 - acc: 0.3774 - val_loss: 1.0999 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.795 \t val: 0.463\n",
      "Epoch 33/500\n",
      " - 1s - loss: 1.3938 - acc: 0.3704 - val_loss: 1.1024 - val_acc: 0.4465\n",
      "\n",
      "Balanced Accuracy - train: 0.795 \t val: 0.463\n",
      "Epoch 34/500\n",
      " - 0s - loss: 1.3874 - acc: 0.3760 - val_loss: 1.1037 - val_acc: 0.4027\n",
      "\n",
      "Balanced Accuracy - train: 0.788 \t val: 0.463\n",
      "Epoch 35/500\n",
      " - 0s - loss: 1.3722 - acc: 0.3790 - val_loss: 1.1052 - val_acc: 0.2943\n",
      "\n",
      "Balanced Accuracy - train: 0.793 \t val: 0.463\n",
      "Epoch 36/500\n",
      " - 1s - loss: 1.3695 - acc: 0.3768 - val_loss: 1.1073 - val_acc: 0.1816\n",
      "\n",
      "Balanced Accuracy - train: 0.788 \t val: 0.463\n",
      "Epoch 37/500\n",
      " - 1s - loss: 1.3761 - acc: 0.3754 - val_loss: 1.1086 - val_acc: 0.1579\n",
      "\n",
      "Balanced Accuracy - train: 0.784 \t val: 0.463\n",
      "Epoch 38/500\n",
      " - 1s - loss: 1.3648 - acc: 0.3782 - val_loss: 1.1094 - val_acc: 0.1587\n",
      "\n",
      "Balanced Accuracy - train: 0.782 \t val: 0.463\n",
      "Epoch 39/500\n",
      " - 0s - loss: 1.3541 - acc: 0.3860 - val_loss: 1.1107 - val_acc: 0.1716\n",
      "\n",
      "Balanced Accuracy - train: 0.778 \t val: 0.463\n",
      "Epoch 40/500\n",
      " - 1s - loss: 1.3575 - acc: 0.3798 - val_loss: 1.1108 - val_acc: 0.1780\n",
      "\n",
      "Balanced Accuracy - train: 0.777 \t val: 0.463\n",
      "Epoch 41/500\n",
      " - 1s - loss: 1.3490 - acc: 0.3874 - val_loss: 1.1112 - val_acc: 0.1759\n",
      "\n",
      "Balanced Accuracy - train: 0.766 \t val: 0.463\n",
      "Epoch 42/500\n",
      " - 1s - loss: 1.3540 - acc: 0.3835 - val_loss: 1.1113 - val_acc: 0.1752\n",
      "\n",
      "Balanced Accuracy - train: 0.769 \t val: 0.463\n",
      "Epoch 43/500\n",
      " - 0s - loss: 1.3418 - acc: 0.3873 - val_loss: 1.1127 - val_acc: 0.1816\n",
      "\n",
      "Balanced Accuracy - train: 0.763 \t val: 0.463\n",
      "Epoch 44/500\n",
      " - 0s - loss: 1.3350 - acc: 0.3882 - val_loss: 1.1134 - val_acc: 0.1924\n",
      "\n",
      "Balanced Accuracy - train: 0.767 \t val: 0.463\n",
      "Epoch 45/500\n",
      " - 0s - loss: 1.3317 - acc: 0.3851 - val_loss: 1.1143 - val_acc: 0.1931\n",
      "\n",
      "Balanced Accuracy - train: 0.765 \t val: 0.463\n",
      "Epoch 46/500\n",
      " - 0s - loss: 1.3243 - acc: 0.3894 - val_loss: 1.1157 - val_acc: 0.2010\n",
      "\n",
      "Balanced Accuracy - train: 0.755 \t val: 0.463\n",
      "Epoch 47/500\n",
      " - 0s - loss: 1.3294 - acc: 0.3860 - val_loss: 1.1159 - val_acc: 0.2082\n",
      "\n",
      "Balanced Accuracy - train: 0.757 \t val: 0.463\n",
      "Epoch 48/500\n",
      " - 0s - loss: 1.3194 - acc: 0.3895 - val_loss: 1.1176 - val_acc: 0.2161\n",
      "\n",
      "Balanced Accuracy - train: 0.745 \t val: 0.463\n",
      "Epoch 49/500\n",
      " - 0s - loss: 1.3170 - acc: 0.3891 - val_loss: 1.1194 - val_acc: 0.2233\n",
      "\n",
      "Balanced Accuracy - train: 0.752 \t val: 0.463\n",
      "Epoch 50/500\n",
      " - 1s - loss: 1.3185 - acc: 0.3956 - val_loss: 1.1196 - val_acc: 0.2261\n",
      "\n",
      "Balanced Accuracy - train: 0.742 \t val: 0.463\n",
      "Epoch 51/500\n",
      " - 1s - loss: 1.3189 - acc: 0.3882 - val_loss: 1.1204 - val_acc: 0.2254\n",
      "\n",
      "Balanced Accuracy - train: 0.742 \t val: 0.463\n",
      "Epoch 52/500\n",
      " - 0s - loss: 1.3067 - acc: 0.3943 - val_loss: 1.1200 - val_acc: 0.2247\n",
      "\n",
      "Balanced Accuracy - train: 0.730 \t val: 0.463\n",
      "Epoch 53/500\n",
      " - 1s - loss: 1.3125 - acc: 0.3960 - val_loss: 1.1190 - val_acc: 0.2182\n",
      "\n",
      "Balanced Accuracy - train: 0.734 \t val: 0.463\n",
      "Epoch 54/500\n",
      " - 1s - loss: 1.2897 - acc: 0.3952 - val_loss: 1.1189 - val_acc: 0.2197\n",
      "\n",
      "Balanced Accuracy - train: 0.729 \t val: 0.463\n",
      "Epoch 55/500\n",
      " - 0s - loss: 1.2897 - acc: 0.4010 - val_loss: 1.1202 - val_acc: 0.2211\n",
      "\n",
      "Balanced Accuracy - train: 0.726 \t val: 0.463\n",
      "Epoch 56/500\n",
      " - 0s - loss: 1.2977 - acc: 0.3994 - val_loss: 1.1215 - val_acc: 0.2233\n",
      "\n",
      "Balanced Accuracy - train: 0.725 \t val: 0.463\n",
      "Epoch 57/500\n",
      " - 0s - loss: 1.3064 - acc: 0.3897 - val_loss: 1.1224 - val_acc: 0.2247\n",
      "\n",
      "Balanced Accuracy - train: 0.722 \t val: 0.463\n",
      "Epoch 58/500\n",
      " - 0s - loss: 1.2919 - acc: 0.3968 - val_loss: 1.1224 - val_acc: 0.2247\n",
      "\n",
      "Balanced Accuracy - train: 0.717 \t val: 0.463\n",
      "Epoch 59/500\n",
      " - 0s - loss: 1.2903 - acc: 0.3944 - val_loss: 1.1229 - val_acc: 0.2247\n",
      "\n",
      "Balanced Accuracy - train: 0.716 \t val: 0.463\n",
      "Epoch 60/500\n",
      " - 0s - loss: 1.2850 - acc: 0.3991 - val_loss: 1.1228 - val_acc: 0.2261\n",
      "\n",
      "Balanced Accuracy - train: 0.720 \t val: 0.463\n",
      "Epoch 61/500\n",
      " - 0s - loss: 1.2804 - acc: 0.4006 - val_loss: 1.1243 - val_acc: 0.2347\n",
      "\n",
      "Balanced Accuracy - train: 0.714 \t val: 0.463\n",
      "Epoch 62/500\n",
      " - 0s - loss: 1.2761 - acc: 0.4020 - val_loss: 1.1258 - val_acc: 0.2369\n",
      "\n",
      "Balanced Accuracy - train: 0.713 \t val: 0.463\n",
      "Epoch 63/500\n",
      " - 0s - loss: 1.2756 - acc: 0.3990 - val_loss: 1.1282 - val_acc: 0.2434\n",
      "\n",
      "Balanced Accuracy - train: 0.713 \t val: 0.463\n",
      "Epoch 64/500\n",
      " - 0s - loss: 1.2818 - acc: 0.4004 - val_loss: 1.1290 - val_acc: 0.2434\n",
      "\n",
      "Balanced Accuracy - train: 0.700 \t val: 0.463\n",
      "Epoch 65/500\n",
      " - 1s - loss: 1.2571 - acc: 0.4101 - val_loss: 1.1284 - val_acc: 0.2419\n",
      "\n",
      "Balanced Accuracy - train: 0.695 \t val: 0.463\n",
      "Epoch 66/500\n",
      " - 1s - loss: 1.2643 - acc: 0.4001 - val_loss: 1.1288 - val_acc: 0.2448\n",
      "\n",
      "Balanced Accuracy - train: 0.699 \t val: 0.463\n",
      "Epoch 67/500\n",
      " - 1s - loss: 1.2621 - acc: 0.4036 - val_loss: 1.1295 - val_acc: 0.2448\n",
      "\n",
      "Balanced Accuracy - train: 0.704 \t val: 0.463\n",
      "Epoch 68/500\n",
      " - 1s - loss: 1.2495 - acc: 0.4055 - val_loss: 1.1304 - val_acc: 0.2462\n",
      "\n",
      "Balanced Accuracy - train: 0.697 \t val: 0.463\n",
      "Epoch 69/500\n",
      " - 1s - loss: 1.2580 - acc: 0.4067 - val_loss: 1.1307 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.702 \t val: 0.463\n",
      "Epoch 70/500\n",
      " - 0s - loss: 1.2411 - acc: 0.4105 - val_loss: 1.1332 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.688 \t val: 0.463\n",
      "Epoch 71/500\n",
      " - 0s - loss: 1.2508 - acc: 0.4027 - val_loss: 1.1339 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.680 \t val: 0.463\n",
      "Epoch 72/500\n",
      " - 0s - loss: 1.2515 - acc: 0.4062 - val_loss: 1.1333 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.687 \t val: 0.463\n",
      "Epoch 73/500\n",
      " - 0s - loss: 1.2418 - acc: 0.4094 - val_loss: 1.1335 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.683 \t val: 0.463\n",
      "Epoch 74/500\n",
      " - 0s - loss: 1.2478 - acc: 0.4085 - val_loss: 1.1347 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.674 \t val: 0.463\n",
      "Epoch 75/500\n",
      " - 0s - loss: 1.2467 - acc: 0.4075 - val_loss: 1.1320 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.671 \t val: 0.463\n",
      "Epoch 76/500\n",
      " - 0s - loss: 1.2288 - acc: 0.4092 - val_loss: 1.1308 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.671 \t val: 0.463\n",
      "Epoch 77/500\n",
      " - 0s - loss: 1.2382 - acc: 0.4104 - val_loss: 1.1310 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.675 \t val: 0.463\n",
      "Epoch 78/500\n",
      " - 1s - loss: 1.2362 - acc: 0.4100 - val_loss: 1.1324 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.671 \t val: 0.463\n",
      "Epoch 79/500\n",
      " - 1s - loss: 1.2368 - acc: 0.4122 - val_loss: 1.1333 - val_acc: 0.2498\n",
      "\n",
      "Balanced Accuracy - train: 0.674 \t val: 0.463\n",
      "Epoch 80/500\n",
      " - 1s - loss: 1.2277 - acc: 0.4088 - val_loss: 1.1329 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.674 \t val: 0.463\n",
      "Epoch 81/500\n",
      " - 0s - loss: 1.2314 - acc: 0.4112 - val_loss: 1.1331 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.672 \t val: 0.463\n",
      "Epoch 82/500\n",
      " - 0s - loss: 1.2233 - acc: 0.4119 - val_loss: 1.1331 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.675 \t val: 0.463\n",
      "Epoch 83/500\n",
      " - 1s - loss: 1.2147 - acc: 0.4121 - val_loss: 1.1339 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.680 \t val: 0.463\n",
      "Epoch 84/500\n",
      " - 1s - loss: 1.2265 - acc: 0.4117 - val_loss: 1.1350 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.463\n",
      "Epoch 85/500\n",
      " - 0s - loss: 1.2235 - acc: 0.4090 - val_loss: 1.1348 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.671 \t val: 0.463\n",
      "Epoch 86/500\n",
      " - 0s - loss: 1.2173 - acc: 0.4063 - val_loss: 1.1353 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.463\n",
      "Epoch 87/500\n",
      " - 0s - loss: 1.2143 - acc: 0.4109 - val_loss: 1.1353 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.662 \t val: 0.463\n",
      "Epoch 88/500\n",
      " - 0s - loss: 1.2069 - acc: 0.4183 - val_loss: 1.1355 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.661 \t val: 0.463\n",
      "Epoch 89/500\n",
      " - 0s - loss: 1.2095 - acc: 0.4163 - val_loss: 1.1360 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.666 \t val: 0.463\n",
      "Epoch 90/500\n",
      " - 0s - loss: 1.2250 - acc: 0.4137 - val_loss: 1.1360 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.666 \t val: 0.463\n",
      "Epoch 91/500\n",
      " - 0s - loss: 1.2029 - acc: 0.4196 - val_loss: 1.1360 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.463\n",
      "Epoch 92/500\n",
      " - 1s - loss: 1.2006 - acc: 0.4164 - val_loss: 1.1368 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.660 \t val: 0.463\n",
      "Epoch 93/500\n",
      " - 1s - loss: 1.2099 - acc: 0.4127 - val_loss: 1.1368 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.659 \t val: 0.463\n",
      "Epoch 94/500\n",
      " - 1s - loss: 1.2087 - acc: 0.4092 - val_loss: 1.1359 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.657 \t val: 0.463\n",
      "Epoch 95/500\n",
      " - 0s - loss: 1.1943 - acc: 0.4119 - val_loss: 1.1357 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.661 \t val: 0.463\n",
      "Epoch 96/500\n",
      " - 0s - loss: 1.2047 - acc: 0.4137 - val_loss: 1.1371 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.463\n",
      "Epoch 97/500\n",
      " - 1s - loss: 1.1985 - acc: 0.4137 - val_loss: 1.1388 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.672 \t val: 0.463\n",
      "Epoch 98/500\n",
      " - 1s - loss: 1.1818 - acc: 0.4232 - val_loss: 1.1385 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.666 \t val: 0.463\n",
      "Epoch 99/500\n",
      " - 0s - loss: 1.1984 - acc: 0.4135 - val_loss: 1.1395 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.662 \t val: 0.463\n",
      "Epoch 100/500\n",
      " - 0s - loss: 1.1885 - acc: 0.4153 - val_loss: 1.1401 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.669 \t val: 0.463\n",
      "Epoch 101/500\n",
      " - 0s - loss: 1.1764 - acc: 0.4230 - val_loss: 1.1409 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.669 \t val: 0.463\n",
      "Epoch 102/500\n",
      " - 0s - loss: 1.1907 - acc: 0.4177 - val_loss: 1.1415 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.667 \t val: 0.463\n",
      "Epoch 103/500\n",
      " - 1s - loss: 1.1912 - acc: 0.4127 - val_loss: 1.1418 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.665 \t val: 0.463\n",
      "Epoch 104/500\n",
      " - 1s - loss: 1.1865 - acc: 0.4197 - val_loss: 1.1409 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.665 \t val: 0.463\n",
      "Epoch 105/500\n",
      " - 1s - loss: 1.1792 - acc: 0.4222 - val_loss: 1.1412 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.672 \t val: 0.463\n",
      "Epoch 106/500\n",
      " - 1s - loss: 1.1845 - acc: 0.4123 - val_loss: 1.1429 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.673 \t val: 0.463\n",
      "Epoch 107/500\n",
      " - 0s - loss: 1.1794 - acc: 0.4220 - val_loss: 1.1431 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.667 \t val: 0.463\n",
      "Epoch 108/500\n",
      " - 0s - loss: 1.1764 - acc: 0.4172 - val_loss: 1.1433 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.669 \t val: 0.463\n",
      "Epoch 109/500\n",
      " - 0s - loss: 1.1736 - acc: 0.4193 - val_loss: 1.1426 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.672 \t val: 0.463\n",
      "Epoch 110/500\n",
      " - 1s - loss: 1.1716 - acc: 0.4207 - val_loss: 1.1429 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.663 \t val: 0.463\n",
      "Epoch 111/500\n",
      " - 0s - loss: 1.1625 - acc: 0.4221 - val_loss: 1.1433 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.463\n",
      "Epoch 112/500\n",
      " - 0s - loss: 1.1703 - acc: 0.4193 - val_loss: 1.1430 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.674 \t val: 0.463\n",
      "Epoch 113/500\n",
      " - 0s - loss: 1.1626 - acc: 0.4237 - val_loss: 1.1438 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.676 \t val: 0.463\n",
      "Epoch 114/500\n",
      " - 0s - loss: 1.1664 - acc: 0.4230 - val_loss: 1.1427 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.669 \t val: 0.463\n",
      "Epoch 115/500\n",
      " - 0s - loss: 1.1605 - acc: 0.4279 - val_loss: 1.1426 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.677 \t val: 0.463\n",
      "Epoch 116/500\n",
      " - 0s - loss: 1.1631 - acc: 0.4203 - val_loss: 1.1420 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.672 \t val: 0.463\n",
      "Epoch 117/500\n",
      " - 0s - loss: 1.1578 - acc: 0.4223 - val_loss: 1.1416 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.669 \t val: 0.463\n",
      "Epoch 118/500\n",
      " - 0s - loss: 1.1555 - acc: 0.4234 - val_loss: 1.1428 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.673 \t val: 0.463\n",
      "Epoch 119/500\n",
      " - 1s - loss: 1.1591 - acc: 0.4172 - val_loss: 1.1436 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.678 \t val: 0.463\n",
      "Epoch 120/500\n",
      " - 1s - loss: 1.1541 - acc: 0.4239 - val_loss: 1.1442 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.681 \t val: 0.463\n",
      "Epoch 121/500\n",
      " - 1s - loss: 1.1548 - acc: 0.4249 - val_loss: 1.1432 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.680 \t val: 0.463\n",
      "Epoch 122/500\n",
      " - 1s - loss: 1.1575 - acc: 0.4189 - val_loss: 1.1440 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.677 \t val: 0.463\n",
      "Epoch 123/500\n",
      " - 1s - loss: 1.1535 - acc: 0.4225 - val_loss: 1.1441 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.676 \t val: 0.463\n",
      "Epoch 124/500\n",
      " - 1s - loss: 1.1445 - acc: 0.4221 - val_loss: 1.1432 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.676 \t val: 0.463\n",
      "Epoch 125/500\n",
      " - 0s - loss: 1.1461 - acc: 0.4278 - val_loss: 1.1432 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.682 \t val: 0.463\n",
      "Epoch 126/500\n",
      " - 0s - loss: 1.1395 - acc: 0.4249 - val_loss: 1.1442 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.681 \t val: 0.463\n",
      "Epoch 127/500\n",
      " - 0s - loss: 1.1541 - acc: 0.4201 - val_loss: 1.1442 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.678 \t val: 0.463\n",
      "Epoch 128/500\n",
      " - 0s - loss: 1.1434 - acc: 0.4257 - val_loss: 1.1426 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.681 \t val: 0.463\n",
      "Epoch 129/500\n",
      " - 0s - loss: 1.1413 - acc: 0.4260 - val_loss: 1.1426 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.690 \t val: 0.463\n",
      "Epoch 130/500\n",
      " - 0s - loss: 1.1442 - acc: 0.4201 - val_loss: 1.1426 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.696 \t val: 0.463\n",
      "Epoch 131/500\n",
      " - 0s - loss: 1.1372 - acc: 0.4274 - val_loss: 1.1426 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.696 \t val: 0.463\n",
      "Epoch 132/500\n",
      " - 0s - loss: 1.1262 - acc: 0.4322 - val_loss: 1.1437 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.693 \t val: 0.463\n",
      "Epoch 133/500\n",
      " - 1s - loss: 1.1422 - acc: 0.4214 - val_loss: 1.1445 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.694 \t val: 0.463\n",
      "Epoch 134/500\n",
      " - 1s - loss: 1.1365 - acc: 0.4291 - val_loss: 1.1432 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.690 \t val: 0.463\n",
      "Epoch 135/500\n",
      " - 1s - loss: 1.1315 - acc: 0.4310 - val_loss: 1.1439 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.702 \t val: 0.463\n",
      "Epoch 136/500\n",
      " - 0s - loss: 1.1350 - acc: 0.4292 - val_loss: 1.1441 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.699 \t val: 0.463\n",
      "Epoch 137/500\n",
      " - 0s - loss: 1.1360 - acc: 0.4255 - val_loss: 1.1437 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.701 \t val: 0.463\n",
      "Epoch 138/500\n",
      " - 0s - loss: 1.1257 - acc: 0.4257 - val_loss: 1.1442 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.700 \t val: 0.463\n",
      "Epoch 139/500\n",
      " - 1s - loss: 1.1333 - acc: 0.4265 - val_loss: 1.1429 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.700 \t val: 0.463\n",
      "Epoch 140/500\n",
      " - 0s - loss: 1.1292 - acc: 0.4233 - val_loss: 1.1442 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.712 \t val: 0.463\n",
      "Epoch 141/500\n",
      " - 0s - loss: 1.1209 - acc: 0.4325 - val_loss: 1.1453 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.711 \t val: 0.463\n",
      "Epoch 142/500\n",
      " - 0s - loss: 1.1297 - acc: 0.4261 - val_loss: 1.1447 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.709 \t val: 0.463\n",
      "Epoch 143/500\n",
      " - 0s - loss: 1.1249 - acc: 0.4321 - val_loss: 1.1454 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.701 \t val: 0.463\n",
      "Epoch 144/500\n",
      " - 0s - loss: 1.1205 - acc: 0.4326 - val_loss: 1.1442 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.706 \t val: 0.463\n",
      "Epoch 145/500\n",
      " - 0s - loss: 1.1192 - acc: 0.4284 - val_loss: 1.1447 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.712 \t val: 0.463\n",
      "Epoch 146/500\n",
      " - 0s - loss: 1.1173 - acc: 0.4302 - val_loss: 1.1435 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.710 \t val: 0.463\n",
      "Epoch 147/500\n",
      " - 0s - loss: 1.1205 - acc: 0.4355 - val_loss: 1.1439 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.720 \t val: 0.463\n",
      "Epoch 148/500\n",
      " - 0s - loss: 1.1178 - acc: 0.4341 - val_loss: 1.1441 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.715 \t val: 0.463\n",
      "Epoch 149/500\n",
      " - 1s - loss: 1.1188 - acc: 0.4315 - val_loss: 1.1439 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.716 \t val: 0.463\n",
      "Epoch 150/500\n",
      " - 1s - loss: 1.1145 - acc: 0.4290 - val_loss: 1.1446 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.722 \t val: 0.463\n",
      "Epoch 151/500\n",
      " - 1s - loss: 1.1161 - acc: 0.4320 - val_loss: 1.1444 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.723 \t val: 0.463\n",
      "Epoch 152/500\n",
      " - 0s - loss: 1.1154 - acc: 0.4321 - val_loss: 1.1443 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.719 \t val: 0.463\n",
      "Epoch 153/500\n",
      " - 0s - loss: 1.1101 - acc: 0.4308 - val_loss: 1.1449 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.724 \t val: 0.463\n",
      "Epoch 154/500\n",
      " - 1s - loss: 1.1174 - acc: 0.4298 - val_loss: 1.1450 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.720 \t val: 0.463\n",
      "Epoch 155/500\n",
      " - 1s - loss: 1.1090 - acc: 0.4310 - val_loss: 1.1446 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.726 \t val: 0.463\n",
      "Epoch 156/500\n",
      " - 0s - loss: 1.1074 - acc: 0.4369 - val_loss: 1.1442 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.726 \t val: 0.463\n",
      "Epoch 157/500\n",
      " - 0s - loss: 1.1148 - acc: 0.4279 - val_loss: 1.1450 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.726 \t val: 0.463\n",
      "Epoch 158/500\n",
      " - 0s - loss: 1.1058 - acc: 0.4371 - val_loss: 1.1441 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.729 \t val: 0.463\n",
      "Epoch 159/500\n",
      " - 0s - loss: 1.1098 - acc: 0.4310 - val_loss: 1.1445 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.728 \t val: 0.463\n",
      "Epoch 160/500\n",
      " - 0s - loss: 1.1076 - acc: 0.4373 - val_loss: 1.1457 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.732 \t val: 0.463\n",
      "Epoch 161/500\n",
      " - 0s - loss: 1.1058 - acc: 0.4341 - val_loss: 1.1447 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.732 \t val: 0.463\n",
      "Epoch 162/500\n",
      " - 1s - loss: 1.1055 - acc: 0.4352 - val_loss: 1.1450 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.739 \t val: 0.463\n",
      "Epoch 163/500\n",
      " - 1s - loss: 1.1063 - acc: 0.4302 - val_loss: 1.1449 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.736 \t val: 0.463\n",
      "Epoch 164/500\n",
      " - 1s - loss: 1.1048 - acc: 0.4289 - val_loss: 1.1454 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.731 \t val: 0.463\n",
      "Epoch 165/500\n",
      " - 1s - loss: 1.1052 - acc: 0.4315 - val_loss: 1.1451 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.738 \t val: 0.463\n",
      "Epoch 166/500\n",
      " - 1s - loss: 1.1032 - acc: 0.4322 - val_loss: 1.1442 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.736 \t val: 0.463\n",
      "Epoch 167/500\n",
      " - 0s - loss: 1.1019 - acc: 0.4322 - val_loss: 1.1439 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.733 \t val: 0.463\n",
      "Epoch 168/500\n",
      " - 0s - loss: 1.1002 - acc: 0.4386 - val_loss: 1.1435 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.746 \t val: 0.463\n",
      "Epoch 169/500\n",
      " - 1s - loss: 1.0987 - acc: 0.4337 - val_loss: 1.1432 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.749 \t val: 0.463\n",
      "Epoch 170/500\n",
      " - 1s - loss: 1.0965 - acc: 0.4324 - val_loss: 1.1434 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.747 \t val: 0.463\n",
      "Epoch 171/500\n",
      " - 1s - loss: 1.1040 - acc: 0.4278 - val_loss: 1.1436 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.754 \t val: 0.463\n",
      "Epoch 172/500\n",
      " - 1s - loss: 1.0931 - acc: 0.4360 - val_loss: 1.1437 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.750 \t val: 0.463\n",
      "Epoch 173/500\n",
      " - 1s - loss: 1.0925 - acc: 0.4370 - val_loss: 1.1428 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.745 \t val: 0.463\n",
      "Epoch 174/500\n",
      " - 0s - loss: 1.0920 - acc: 0.4385 - val_loss: 1.1426 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.746 \t val: 0.463\n",
      "Epoch 175/500\n",
      " - 0s - loss: 1.0937 - acc: 0.4344 - val_loss: 1.1431 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.750 \t val: 0.463\n",
      "Epoch 176/500\n",
      " - 0s - loss: 1.0860 - acc: 0.4391 - val_loss: 1.1432 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.752 \t val: 0.463\n",
      "Epoch 177/500\n",
      " - 0s - loss: 1.0909 - acc: 0.4343 - val_loss: 1.1422 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.752 \t val: 0.463\n",
      "Epoch 178/500\n",
      " - 0s - loss: 1.0839 - acc: 0.4370 - val_loss: 1.1429 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.757 \t val: 0.463\n",
      "Epoch 179/500\n",
      " - 0s - loss: 1.0861 - acc: 0.4401 - val_loss: 1.1432 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.756 \t val: 0.463\n",
      "Epoch 180/500\n",
      " - 0s - loss: 1.0904 - acc: 0.4341 - val_loss: 1.1437 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.760 \t val: 0.463\n",
      "Epoch 181/500\n",
      " - 0s - loss: 1.0940 - acc: 0.4298 - val_loss: 1.1439 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.764 \t val: 0.463\n",
      "Epoch 182/500\n",
      " - 1s - loss: 1.0920 - acc: 0.4313 - val_loss: 1.1445 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.760 \t val: 0.463\n",
      "Epoch 183/500\n",
      " - 1s - loss: 1.0856 - acc: 0.4338 - val_loss: 1.1448 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.764 \t val: 0.463\n",
      "Epoch 184/500\n",
      " - 0s - loss: 1.0829 - acc: 0.4361 - val_loss: 1.1445 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.765 \t val: 0.463\n",
      "Epoch 185/500\n",
      " - 0s - loss: 1.0836 - acc: 0.4373 - val_loss: 1.1445 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.764 \t val: 0.463\n",
      "Epoch 186/500\n",
      " - 1s - loss: 1.0833 - acc: 0.4385 - val_loss: 1.1447 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.769 \t val: 0.463\n",
      "Epoch 187/500\n",
      " - 1s - loss: 1.0827 - acc: 0.4354 - val_loss: 1.1450 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.771 \t val: 0.463\n",
      "Epoch 188/500\n",
      " - 1s - loss: 1.0807 - acc: 0.4357 - val_loss: 1.1453 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.767 \t val: 0.463\n",
      "Epoch 189/500\n",
      " - 0s - loss: 1.0795 - acc: 0.4350 - val_loss: 1.1461 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.771 \t val: 0.463\n",
      "Epoch 190/500\n",
      " - 0s - loss: 1.0846 - acc: 0.4335 - val_loss: 1.1455 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.776 \t val: 0.463\n",
      "Epoch 191/500\n",
      " - 0s - loss: 1.0761 - acc: 0.4408 - val_loss: 1.1461 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.772 \t val: 0.463\n",
      "Epoch 192/500\n",
      " - 0s - loss: 1.0790 - acc: 0.4365 - val_loss: 1.1457 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.773 \t val: 0.463\n",
      "Epoch 193/500\n",
      " - 0s - loss: 1.0795 - acc: 0.4342 - val_loss: 1.1458 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.781 \t val: 0.463\n",
      "Epoch 194/500\n",
      " - 0s - loss: 1.0779 - acc: 0.4397 - val_loss: 1.1453 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.781 \t val: 0.463\n",
      "Epoch 195/500\n",
      " - 0s - loss: 1.0745 - acc: 0.4407 - val_loss: 1.1453 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.783 \t val: 0.463\n",
      "Epoch 196/500\n",
      " - 0s - loss: 1.0791 - acc: 0.4415 - val_loss: 1.1464 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.784 \t val: 0.463\n",
      "Epoch 197/500\n",
      " - 1s - loss: 1.0810 - acc: 0.4395 - val_loss: 1.1455 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.782 \t val: 0.463\n",
      "Epoch 198/500\n",
      " - 1s - loss: 1.0721 - acc: 0.4404 - val_loss: 1.1461 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.788 \t val: 0.463\n",
      "Epoch 199/500\n",
      " - 0s - loss: 1.0726 - acc: 0.4405 - val_loss: 1.1464 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.787 \t val: 0.463\n",
      "Epoch 200/500\n",
      " - 0s - loss: 1.0714 - acc: 0.4413 - val_loss: 1.1461 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.792 \t val: 0.463\n",
      "Epoch 201/500\n",
      " - 0s - loss: 1.0682 - acc: 0.4421 - val_loss: 1.1460 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.789 \t val: 0.463\n",
      "Epoch 202/500\n",
      " - 1s - loss: 1.0727 - acc: 0.4379 - val_loss: 1.1464 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.790 \t val: 0.463\n",
      "Epoch 203/500\n",
      " - 1s - loss: 1.0721 - acc: 0.4403 - val_loss: 1.1459 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.792 \t val: 0.463\n",
      "Epoch 204/500\n",
      " - 1s - loss: 1.0715 - acc: 0.4367 - val_loss: 1.1456 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.793 \t val: 0.463\n",
      "Epoch 205/500\n",
      " - 0s - loss: 1.0663 - acc: 0.4443 - val_loss: 1.1455 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.792 \t val: 0.463\n",
      "Epoch 206/500\n",
      " - 0s - loss: 1.0699 - acc: 0.4407 - val_loss: 1.1457 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.795 \t val: 0.463\n",
      "Epoch 207/500\n",
      " - 0s - loss: 1.0686 - acc: 0.4406 - val_loss: 1.1456 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.799 \t val: 0.463\n",
      "Epoch 208/500\n",
      " - 0s - loss: 1.0679 - acc: 0.4438 - val_loss: 1.1461 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.797 \t val: 0.463\n",
      "Epoch 209/500\n",
      " - 0s - loss: 1.0660 - acc: 0.4460 - val_loss: 1.1467 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.801 \t val: 0.463\n",
      "Epoch 210/500\n",
      " - 0s - loss: 1.0614 - acc: 0.4436 - val_loss: 1.1470 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.804 \t val: 0.463\n",
      "Epoch 211/500\n",
      " - 0s - loss: 1.0668 - acc: 0.4463 - val_loss: 1.1477 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.803 \t val: 0.463\n",
      "Epoch 212/500\n",
      " - 0s - loss: 1.0631 - acc: 0.4432 - val_loss: 1.1475 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.801 \t val: 0.463\n",
      "Epoch 213/500\n",
      " - 1s - loss: 1.0656 - acc: 0.4360 - val_loss: 1.1477 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.805 \t val: 0.463\n",
      "Epoch 214/500\n",
      " - 1s - loss: 1.0630 - acc: 0.4436 - val_loss: 1.1469 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.805 \t val: 0.463\n",
      "Epoch 215/500\n",
      " - 1s - loss: 1.0579 - acc: 0.4434 - val_loss: 1.1471 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.808 \t val: 0.463\n",
      "Epoch 216/500\n",
      " - 1s - loss: 1.0614 - acc: 0.4412 - val_loss: 1.1472 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.808 \t val: 0.463\n",
      "Epoch 217/500\n",
      " - 1s - loss: 1.0639 - acc: 0.4434 - val_loss: 1.1474 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.812 \t val: 0.463\n",
      "Epoch 218/500\n",
      " - 1s - loss: 1.0606 - acc: 0.4401 - val_loss: 1.1467 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.815 \t val: 0.463\n",
      "Epoch 219/500\n",
      " - 1s - loss: 1.0587 - acc: 0.4432 - val_loss: 1.1482 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.818 \t val: 0.463\n",
      "Epoch 220/500\n",
      " - 0s - loss: 1.0580 - acc: 0.4482 - val_loss: 1.1484 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.818 \t val: 0.463\n",
      "Epoch 221/500\n",
      " - 0s - loss: 1.0600 - acc: 0.4413 - val_loss: 1.1486 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.819 \t val: 0.463\n",
      "Epoch 222/500\n",
      " - 0s - loss: 1.0583 - acc: 0.4432 - val_loss: 1.1488 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.819 \t val: 0.463\n",
      "Epoch 223/500\n",
      " - 0s - loss: 1.0579 - acc: 0.4466 - val_loss: 1.1482 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.815 \t val: 0.463\n",
      "Epoch 224/500\n",
      " - 0s - loss: 1.0571 - acc: 0.4457 - val_loss: 1.1478 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.819 \t val: 0.463\n",
      "Epoch 225/500\n",
      " - 0s - loss: 1.0535 - acc: 0.4464 - val_loss: 1.1484 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.817 \t val: 0.463\n",
      "Epoch 226/500\n",
      " - 0s - loss: 1.0586 - acc: 0.4406 - val_loss: 1.1485 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.819 \t val: 0.463\n",
      "Epoch 227/500\n",
      " - 0s - loss: 1.0526 - acc: 0.4481 - val_loss: 1.1482 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.824 \t val: 0.463\n",
      "Epoch 228/500\n",
      " - 0s - loss: 1.0533 - acc: 0.4495 - val_loss: 1.1485 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.825 \t val: 0.463\n",
      "Epoch 229/500\n",
      " - 0s - loss: 1.0549 - acc: 0.4498 - val_loss: 1.1491 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.827 \t val: 0.463\n",
      "Epoch 230/500\n",
      " - 1s - loss: 1.0536 - acc: 0.4411 - val_loss: 1.1492 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.822 \t val: 0.463\n",
      "Epoch 231/500\n",
      " - 1s - loss: 1.0522 - acc: 0.4432 - val_loss: 1.1496 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.830 \t val: 0.463\n",
      "Epoch 232/500\n",
      " - 0s - loss: 1.0508 - acc: 0.4467 - val_loss: 1.1499 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.827 \t val: 0.463\n",
      "Epoch 233/500\n",
      " - 0s - loss: 1.0583 - acc: 0.4442 - val_loss: 1.1500 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.825 \t val: 0.463\n",
      "Epoch 234/500\n",
      " - 0s - loss: 1.0495 - acc: 0.4486 - val_loss: 1.1496 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.829 \t val: 0.463\n",
      "Epoch 235/500\n",
      " - 1s - loss: 1.0505 - acc: 0.4504 - val_loss: 1.1500 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.829 \t val: 0.463\n",
      "Epoch 236/500\n",
      " - 1s - loss: 1.0455 - acc: 0.4513 - val_loss: 1.1505 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.832 \t val: 0.463\n",
      "Epoch 237/500\n",
      " - 1s - loss: 1.0565 - acc: 0.4408 - val_loss: 1.1500 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.833 \t val: 0.463\n",
      "Epoch 238/500\n",
      " - 0s - loss: 1.0516 - acc: 0.4431 - val_loss: 1.1502 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.834 \t val: 0.463\n",
      "Epoch 239/500\n",
      " - 0s - loss: 1.0488 - acc: 0.4492 - val_loss: 1.1505 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.834 \t val: 0.463\n",
      "Epoch 240/500\n",
      " - 0s - loss: 1.0455 - acc: 0.4488 - val_loss: 1.1504 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.835 \t val: 0.463\n",
      "Epoch 241/500\n",
      " - 1s - loss: 1.0510 - acc: 0.4473 - val_loss: 1.1495 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.835 \t val: 0.463\n",
      "Epoch 242/500\n",
      " - 1s - loss: 1.0491 - acc: 0.4534 - val_loss: 1.1491 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.841 \t val: 0.463\n",
      "Epoch 243/500\n",
      " - 1s - loss: 1.0482 - acc: 0.4505 - val_loss: 1.1497 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.837 \t val: 0.463\n",
      "Epoch 244/500\n",
      " - 0s - loss: 1.0518 - acc: 0.4429 - val_loss: 1.1501 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.840 \t val: 0.463\n",
      "Epoch 245/500\n",
      " - 0s - loss: 1.0473 - acc: 0.4509 - val_loss: 1.1502 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.842 \t val: 0.463\n",
      "Epoch 246/500\n",
      " - 0s - loss: 1.0470 - acc: 0.4510 - val_loss: 1.1501 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.843 \t val: 0.463\n",
      "Epoch 247/500\n",
      " - 0s - loss: 1.0513 - acc: 0.4469 - val_loss: 1.1492 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.841 \t val: 0.463\n",
      "Epoch 248/500\n",
      " - 0s - loss: 1.0470 - acc: 0.4518 - val_loss: 1.1497 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.848 \t val: 0.463\n",
      "Epoch 249/500\n",
      " - 0s - loss: 1.0423 - acc: 0.4510 - val_loss: 1.1494 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.845 \t val: 0.463\n",
      "Epoch 250/500\n",
      " - 0s - loss: 1.0491 - acc: 0.4518 - val_loss: 1.1488 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.845 \t val: 0.463\n",
      "Epoch 251/500\n",
      " - 1s - loss: 1.0458 - acc: 0.4493 - val_loss: 1.1489 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.844 \t val: 0.463\n",
      "Epoch 252/500\n",
      " - 0s - loss: 1.0414 - acc: 0.4508 - val_loss: 1.1496 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.844 \t val: 0.463\n",
      "Epoch 253/500\n",
      " - 1s - loss: 1.0426 - acc: 0.4509 - val_loss: 1.1495 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.846 \t val: 0.463\n",
      "Epoch 254/500\n",
      " - 0s - loss: 1.0502 - acc: 0.4491 - val_loss: 1.1494 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.844 \t val: 0.463\n",
      "Epoch 255/500\n",
      " - 0s - loss: 1.0468 - acc: 0.4520 - val_loss: 1.1497 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.849 \t val: 0.463\n",
      "Epoch 256/500\n",
      " - 0s - loss: 1.0422 - acc: 0.4530 - val_loss: 1.1500 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.850 \t val: 0.463\n",
      "Epoch 257/500\n",
      " - 0s - loss: 1.0464 - acc: 0.4503 - val_loss: 1.1507 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.852 \t val: 0.463\n",
      "Epoch 258/500\n",
      " - 0s - loss: 1.0393 - acc: 0.4556 - val_loss: 1.1507 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.849 \t val: 0.463\n",
      "Epoch 259/500\n",
      " - 0s - loss: 1.0447 - acc: 0.4539 - val_loss: 1.1501 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.850 \t val: 0.463\n",
      "Epoch 260/500\n",
      " - 0s - loss: 1.0448 - acc: 0.4496 - val_loss: 1.1506 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.855 \t val: 0.463\n",
      "Epoch 261/500\n",
      " - 0s - loss: 1.0367 - acc: 0.4555 - val_loss: 1.1504 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.851 \t val: 0.463\n",
      "Epoch 262/500\n",
      " - 0s - loss: 1.0402 - acc: 0.4535 - val_loss: 1.1503 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.853 \t val: 0.463\n",
      "Epoch 263/500\n",
      " - 0s - loss: 1.0414 - acc: 0.4512 - val_loss: 1.1503 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.853 \t val: 0.463\n",
      "Epoch 264/500\n",
      " - 0s - loss: 1.0435 - acc: 0.4488 - val_loss: 1.1506 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.856 \t val: 0.463\n",
      "Epoch 265/500\n",
      " - 0s - loss: 1.0377 - acc: 0.4496 - val_loss: 1.1501 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.856 \t val: 0.463\n",
      "Epoch 266/500\n",
      " - 1s - loss: 1.0391 - acc: 0.4534 - val_loss: 1.1496 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.854 \t val: 0.463\n",
      "Epoch 267/500\n",
      " - 0s - loss: 1.0408 - acc: 0.4553 - val_loss: 1.1497 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.855 \t val: 0.463\n",
      "Epoch 268/500\n",
      " - 0s - loss: 1.0391 - acc: 0.4496 - val_loss: 1.1494 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.857 \t val: 0.463\n",
      "Epoch 269/500\n",
      " - 0s - loss: 1.0377 - acc: 0.4563 - val_loss: 1.1494 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.857 \t val: 0.463\n",
      "Epoch 270/500\n",
      " - 0s - loss: 1.0416 - acc: 0.4517 - val_loss: 1.1495 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.858 \t val: 0.463\n",
      "Epoch 271/500\n",
      " - 1s - loss: 1.0392 - acc: 0.4583 - val_loss: 1.1492 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.854 \t val: 0.463\n",
      "Epoch 272/500\n",
      " - 1s - loss: 1.0386 - acc: 0.4569 - val_loss: 1.1495 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.856 \t val: 0.463\n",
      "Epoch 273/500\n",
      " - 0s - loss: 1.0378 - acc: 0.4527 - val_loss: 1.1486 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.855 \t val: 0.463\n",
      "Epoch 274/500\n",
      " - 0s - loss: 1.0369 - acc: 0.4589 - val_loss: 1.1492 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.858 \t val: 0.463\n",
      "Epoch 275/500\n",
      " - 0s - loss: 1.0346 - acc: 0.4598 - val_loss: 1.1500 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.863 \t val: 0.463\n",
      "Epoch 276/500\n",
      " - 0s - loss: 1.0365 - acc: 0.4533 - val_loss: 1.1497 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.863 \t val: 0.463\n",
      "Epoch 277/500\n",
      " - 0s - loss: 1.0410 - acc: 0.4507 - val_loss: 1.1499 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.863 \t val: 0.463\n",
      "Epoch 278/500\n",
      " - 0s - loss: 1.0326 - acc: 0.4547 - val_loss: 1.1508 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.868 \t val: 0.463\n",
      "Epoch 279/500\n",
      " - 1s - loss: 1.0355 - acc: 0.4581 - val_loss: 1.1511 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.865 \t val: 0.463\n",
      "Epoch 280/500\n",
      " - 1s - loss: 1.0333 - acc: 0.4596 - val_loss: 1.1511 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.866 \t val: 0.463\n",
      "Epoch 281/500\n",
      " - 1s - loss: 1.0375 - acc: 0.4568 - val_loss: 1.1513 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.867 \t val: 0.463\n",
      "Epoch 282/500\n",
      " - 1s - loss: 1.0372 - acc: 0.4534 - val_loss: 1.1517 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.871 \t val: 0.463\n",
      "Epoch 283/500\n",
      " - 0s - loss: 1.0355 - acc: 0.4547 - val_loss: 1.1520 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.872 \t val: 0.463\n",
      "Epoch 284/500\n",
      " - 0s - loss: 1.0364 - acc: 0.4531 - val_loss: 1.1520 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.872 \t val: 0.463\n",
      "Epoch 285/500\n",
      " - 0s - loss: 1.0332 - acc: 0.4575 - val_loss: 1.1520 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.869 \t val: 0.463\n",
      "Epoch 286/500\n",
      " - 0s - loss: 1.0356 - acc: 0.4582 - val_loss: 1.1526 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.870 \t val: 0.463\n",
      "Epoch 287/500\n",
      " - 0s - loss: 1.0339 - acc: 0.4532 - val_loss: 1.1524 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.872 \t val: 0.463\n",
      "Epoch 288/500\n",
      " - 0s - loss: 1.0332 - acc: 0.4606 - val_loss: 1.1524 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.874 \t val: 0.463\n",
      "Epoch 289/500\n",
      " - 1s - loss: 1.0339 - acc: 0.4583 - val_loss: 1.1517 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.867 \t val: 0.463\n",
      "Epoch 290/500\n",
      " - 0s - loss: 1.0332 - acc: 0.4594 - val_loss: 1.1516 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.872 \t val: 0.463\n",
      "Epoch 291/500\n",
      " - 1s - loss: 1.0312 - acc: 0.4610 - val_loss: 1.1515 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.872 \t val: 0.463\n",
      "Epoch 292/500\n",
      " - 1s - loss: 1.0310 - acc: 0.4629 - val_loss: 1.1517 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.877 \t val: 0.463\n",
      "Epoch 293/500\n",
      " - 0s - loss: 1.0325 - acc: 0.4594 - val_loss: 1.1516 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.879 \t val: 0.463\n",
      "Epoch 294/500\n",
      " - 1s - loss: 1.0307 - acc: 0.4627 - val_loss: 1.1518 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.880 \t val: 0.463\n",
      "Epoch 295/500\n",
      " - 0s - loss: 1.0306 - acc: 0.4606 - val_loss: 1.1519 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.875 \t val: 0.463\n",
      "Epoch 296/500\n",
      " - 0s - loss: 1.0303 - acc: 0.4582 - val_loss: 1.1517 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.878 \t val: 0.463\n",
      "Epoch 297/500\n",
      " - 0s - loss: 1.0305 - acc: 0.4620 - val_loss: 1.1521 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.879 \t val: 0.463\n",
      "Epoch 298/500\n",
      " - 0s - loss: 1.0295 - acc: 0.4573 - val_loss: 1.1517 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.882 \t val: 0.463\n",
      "Epoch 299/500\n",
      " - 0s - loss: 1.0311 - acc: 0.4586 - val_loss: 1.1518 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.879 \t val: 0.463\n",
      "Epoch 300/500\n",
      " - 0s - loss: 1.0310 - acc: 0.4597 - val_loss: 1.1520 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.882 \t val: 0.463\n",
      "Epoch 301/500\n",
      " - 0s - loss: 1.0325 - acc: 0.4587 - val_loss: 1.1521 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.882 \t val: 0.463\n",
      "Epoch 302/500\n",
      " - 0s - loss: 1.0333 - acc: 0.4606 - val_loss: 1.1523 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.881 \t val: 0.463\n",
      "Epoch 303/500\n",
      " - 0s - loss: 1.0286 - acc: 0.4593 - val_loss: 1.1523 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.879 \t val: 0.463\n",
      "Epoch 304/500\n",
      " - 1s - loss: 1.0309 - acc: 0.4593 - val_loss: 1.1526 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.881 \t val: 0.463\n",
      "Epoch 305/500\n",
      " - 1s - loss: 1.0311 - acc: 0.4572 - val_loss: 1.1522 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.879 \t val: 0.463\n",
      "Epoch 306/500\n",
      " - 1s - loss: 1.0261 - acc: 0.4626 - val_loss: 1.1519 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.882 \t val: 0.463\n",
      "Epoch 307/500\n",
      " - 0s - loss: 1.0296 - acc: 0.4560 - val_loss: 1.1522 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.883 \t val: 0.463\n",
      "Epoch 308/500\n",
      " - 0s - loss: 1.0307 - acc: 0.4590 - val_loss: 1.1518 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.880 \t val: 0.463\n",
      "Epoch 309/500\n",
      " - 1s - loss: 1.0285 - acc: 0.4620 - val_loss: 1.1516 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.880 \t val: 0.463\n",
      "Epoch 310/500\n",
      " - 1s - loss: 1.0310 - acc: 0.4658 - val_loss: 1.1520 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.883 \t val: 0.463\n",
      "Epoch 311/500\n",
      " - 0s - loss: 1.0293 - acc: 0.4602 - val_loss: 1.1524 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.880 \t val: 0.463\n",
      "Epoch 312/500\n",
      " - 0s - loss: 1.0279 - acc: 0.4611 - val_loss: 1.1526 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.887 \t val: 0.463\n",
      "Epoch 313/500\n",
      " - 0s - loss: 1.0320 - acc: 0.4577 - val_loss: 1.1526 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.884 \t val: 0.463\n",
      "Epoch 314/500\n",
      " - 0s - loss: 1.0257 - acc: 0.4624 - val_loss: 1.1529 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.885 \t val: 0.463\n",
      "Epoch 315/500\n",
      " - 0s - loss: 1.0285 - acc: 0.4589 - val_loss: 1.1529 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.884 \t val: 0.463\n",
      "Epoch 316/500\n",
      " - 0s - loss: 1.0290 - acc: 0.4622 - val_loss: 1.1522 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.886 \t val: 0.463\n",
      "Epoch 317/500\n",
      " - 0s - loss: 1.0277 - acc: 0.4612 - val_loss: 1.1513 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.885 \t val: 0.463\n",
      "Epoch 318/500\n",
      " - 0s - loss: 1.0280 - acc: 0.4626 - val_loss: 1.1521 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.886 \t val: 0.463\n",
      "Epoch 319/500\n",
      " - 0s - loss: 1.0262 - acc: 0.4634 - val_loss: 1.1525 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.885 \t val: 0.463\n",
      "Epoch 320/500\n",
      " - 0s - loss: 1.0263 - acc: 0.4643 - val_loss: 1.1525 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.885 \t val: 0.463\n",
      "Epoch 321/500\n",
      " - 1s - loss: 1.0260 - acc: 0.4624 - val_loss: 1.1527 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.890 \t val: 0.463\n",
      "Epoch 322/500\n",
      " - 1s - loss: 1.0255 - acc: 0.4606 - val_loss: 1.1526 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.889 \t val: 0.463\n",
      "Epoch 323/500\n",
      " - 1s - loss: 1.0281 - acc: 0.4611 - val_loss: 1.1531 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.886 \t val: 0.463\n",
      "Epoch 324/500\n",
      " - 1s - loss: 1.0220 - acc: 0.4609 - val_loss: 1.1531 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.888 \t val: 0.463\n",
      "Epoch 325/500\n",
      " - 0s - loss: 1.0267 - acc: 0.4658 - val_loss: 1.1529 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.891 \t val: 0.463\n",
      "Epoch 326/500\n",
      " - 1s - loss: 1.0217 - acc: 0.4620 - val_loss: 1.1532 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.891 \t val: 0.463\n",
      "Epoch 327/500\n",
      " - 1s - loss: 1.0246 - acc: 0.4625 - val_loss: 1.1530 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.894 \t val: 0.463\n",
      "Epoch 328/500\n",
      " - 1s - loss: 1.0231 - acc: 0.4613 - val_loss: 1.1533 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.895 \t val: 0.463\n",
      "Epoch 329/500\n",
      " - 0s - loss: 1.0268 - acc: 0.4601 - val_loss: 1.1535 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.896 \t val: 0.463\n",
      "Epoch 330/500\n",
      " - 0s - loss: 1.0265 - acc: 0.4622 - val_loss: 1.1531 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.893 \t val: 0.463\n",
      "Epoch 331/500\n",
      " - 0s - loss: 1.0243 - acc: 0.4572 - val_loss: 1.1530 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.893 \t val: 0.463\n",
      "Epoch 332/500\n",
      " - 0s - loss: 1.0227 - acc: 0.4649 - val_loss: 1.1531 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.893 \t val: 0.463\n",
      "Epoch 333/500\n",
      " - 0s - loss: 1.0235 - acc: 0.4643 - val_loss: 1.1533 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.898 \t val: 0.463\n",
      "Epoch 334/500\n",
      " - 0s - loss: 1.0246 - acc: 0.4630 - val_loss: 1.1530 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 335/500\n",
      " - 0s - loss: 1.0216 - acc: 0.4667 - val_loss: 1.1531 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.897 \t val: 0.463\n",
      "Epoch 336/500\n",
      " - 0s - loss: 1.0247 - acc: 0.4627 - val_loss: 1.1532 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.898 \t val: 0.463\n",
      "Epoch 337/500\n",
      " - 1s - loss: 1.0225 - acc: 0.4687 - val_loss: 1.1536 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 338/500\n",
      " - 1s - loss: 1.0241 - acc: 0.4615 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 339/500\n",
      " - 1s - loss: 1.0225 - acc: 0.4655 - val_loss: 1.1540 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.898 \t val: 0.463\n",
      "Epoch 340/500\n",
      " - 0s - loss: 1.0240 - acc: 0.4666 - val_loss: 1.1543 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 341/500\n",
      " - 1s - loss: 1.0266 - acc: 0.4619 - val_loss: 1.1542 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 342/500\n",
      " - 1s - loss: 1.0226 - acc: 0.4621 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 343/500\n",
      " - 1s - loss: 1.0239 - acc: 0.4670 - val_loss: 1.1540 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 344/500\n",
      " - 0s - loss: 1.0225 - acc: 0.4633 - val_loss: 1.1542 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.898 \t val: 0.463\n",
      "Epoch 345/500\n",
      " - 0s - loss: 1.0236 - acc: 0.4642 - val_loss: 1.1540 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 346/500\n",
      " - 0s - loss: 1.0215 - acc: 0.4684 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.898 \t val: 0.463\n",
      "Epoch 347/500\n",
      " - 0s - loss: 1.0235 - acc: 0.4661 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 348/500\n",
      " - 0s - loss: 1.0214 - acc: 0.4659 - val_loss: 1.1532 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 349/500\n",
      " - 0s - loss: 1.0220 - acc: 0.4615 - val_loss: 1.1534 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 350/500\n",
      " - 0s - loss: 1.0234 - acc: 0.4655 - val_loss: 1.1535 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 351/500\n",
      " - 1s - loss: 1.0209 - acc: 0.4696 - val_loss: 1.1532 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 352/500\n",
      " - 1s - loss: 1.0193 - acc: 0.4698 - val_loss: 1.1536 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 353/500\n",
      " - 0s - loss: 1.0262 - acc: 0.4608 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 354/500\n",
      " - 0s - loss: 1.0230 - acc: 0.4694 - val_loss: 1.1538 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 355/500\n",
      " - 0s - loss: 1.0199 - acc: 0.4654 - val_loss: 1.1536 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 356/500\n",
      " - 1s - loss: 1.0219 - acc: 0.4708 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 357/500\n",
      " - 1s - loss: 1.0199 - acc: 0.4646 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 358/500\n",
      " - 0s - loss: 1.0194 - acc: 0.4692 - val_loss: 1.1545 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 359/500\n",
      " - 0s - loss: 1.0235 - acc: 0.4624 - val_loss: 1.1545 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 360/500\n",
      " - 0s - loss: 1.0211 - acc: 0.4678 - val_loss: 1.1544 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 361/500\n",
      " - 0s - loss: 1.0216 - acc: 0.4676 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 362/500\n",
      " - 0s - loss: 1.0190 - acc: 0.4632 - val_loss: 1.1544 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 363/500\n",
      " - 0s - loss: 1.0186 - acc: 0.4683 - val_loss: 1.1540 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 364/500\n",
      " - 1s - loss: 1.0208 - acc: 0.4646 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 365/500\n",
      " - 1s - loss: 1.0222 - acc: 0.4635 - val_loss: 1.1537 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 366/500\n",
      " - 1s - loss: 1.0190 - acc: 0.4684 - val_loss: 1.1542 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 367/500\n",
      " - 0s - loss: 1.0173 - acc: 0.4693 - val_loss: 1.1543 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 368/500\n",
      " - 0s - loss: 1.0206 - acc: 0.4669 - val_loss: 1.1536 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 369/500\n",
      " - 0s - loss: 1.0193 - acc: 0.4680 - val_loss: 1.1540 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.897 \t val: 0.463\n",
      "Epoch 370/500\n",
      " - 1s - loss: 1.0179 - acc: 0.4671 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 371/500\n",
      " - 1s - loss: 1.0213 - acc: 0.4671 - val_loss: 1.1540 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 372/500\n",
      " - 1s - loss: 1.0200 - acc: 0.4685 - val_loss: 1.1543 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 373/500\n",
      " - 0s - loss: 1.0190 - acc: 0.4642 - val_loss: 1.1541 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 374/500\n",
      " - 0s - loss: 1.0210 - acc: 0.4621 - val_loss: 1.1540 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 375/500\n",
      " - 0s - loss: 1.0184 - acc: 0.4674 - val_loss: 1.1545 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 376/500\n",
      " - 0s - loss: 1.0177 - acc: 0.4693 - val_loss: 1.1551 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 377/500\n",
      " - 0s - loss: 1.0193 - acc: 0.4704 - val_loss: 1.1552 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 378/500\n",
      " - 0s - loss: 1.0202 - acc: 0.4711 - val_loss: 1.1549 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 379/500\n",
      " - 0s - loss: 1.0179 - acc: 0.4678 - val_loss: 1.1552 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 380/500\n",
      " - 1s - loss: 1.0207 - acc: 0.4686 - val_loss: 1.1554 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 381/500\n",
      " - 1s - loss: 1.0166 - acc: 0.4702 - val_loss: 1.1552 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 382/500\n",
      " - 0s - loss: 1.0195 - acc: 0.4663 - val_loss: 1.1555 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.908 \t val: 0.463\n",
      "Epoch 383/500\n",
      " - 0s - loss: 1.0175 - acc: 0.4699 - val_loss: 1.1556 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 384/500\n",
      " - 0s - loss: 1.0192 - acc: 0.4657 - val_loss: 1.1553 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 385/500\n",
      " - 0s - loss: 1.0190 - acc: 0.4681 - val_loss: 1.1551 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 386/500\n",
      " - 0s - loss: 1.0172 - acc: 0.4660 - val_loss: 1.1557 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 387/500\n",
      " - 0s - loss: 1.0180 - acc: 0.4644 - val_loss: 1.1555 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 388/500\n",
      " - 1s - loss: 1.0170 - acc: 0.4704 - val_loss: 1.1553 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 389/500\n",
      " - 1s - loss: 1.0178 - acc: 0.4661 - val_loss: 1.1554 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 390/500\n",
      " - 1s - loss: 1.0160 - acc: 0.4691 - val_loss: 1.1556 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 391/500\n",
      " - 0s - loss: 1.0179 - acc: 0.4708 - val_loss: 1.1555 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 392/500\n",
      " - 1s - loss: 1.0204 - acc: 0.4653 - val_loss: 1.1554 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 393/500\n",
      " - 0s - loss: 1.0165 - acc: 0.4709 - val_loss: 1.1557 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 394/500\n",
      " - 1s - loss: 1.0184 - acc: 0.4690 - val_loss: 1.1558 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 395/500\n",
      " - 1s - loss: 1.0166 - acc: 0.4711 - val_loss: 1.1561 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 396/500\n",
      " - 0s - loss: 1.0182 - acc: 0.4662 - val_loss: 1.1555 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 397/500\n",
      " - 0s - loss: 1.0194 - acc: 0.4658 - val_loss: 1.1553 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 398/500\n",
      " - 0s - loss: 1.0181 - acc: 0.4675 - val_loss: 1.1558 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 399/500\n",
      " - 0s - loss: 1.0168 - acc: 0.4659 - val_loss: 1.1553 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 400/500\n",
      " - 0s - loss: 1.0175 - acc: 0.4646 - val_loss: 1.1550 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 401/500\n",
      " - 0s - loss: 1.0176 - acc: 0.4699 - val_loss: 1.1555 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 402/500\n",
      " - 1s - loss: 1.0165 - acc: 0.4704 - val_loss: 1.1555 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 403/500\n",
      " - 1s - loss: 1.0176 - acc: 0.4663 - val_loss: 1.1559 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 404/500\n",
      " - 0s - loss: 1.0167 - acc: 0.4679 - val_loss: 1.1554 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 405/500\n",
      " - 0s - loss: 1.0181 - acc: 0.4699 - val_loss: 1.1555 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 406/500\n",
      " - 0s - loss: 1.0179 - acc: 0.4700 - val_loss: 1.1556 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 407/500\n",
      " - 0s - loss: 1.0153 - acc: 0.4718 - val_loss: 1.1556 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 408/500\n",
      " - 1s - loss: 1.0173 - acc: 0.4715 - val_loss: 1.1559 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 409/500\n",
      " - 1s - loss: 1.0164 - acc: 0.4723 - val_loss: 1.1557 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 410/500\n",
      " - 0s - loss: 1.0162 - acc: 0.4706 - val_loss: 1.1561 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 411/500\n",
      " - 0s - loss: 1.0173 - acc: 0.4684 - val_loss: 1.1558 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 412/500\n",
      " - 0s - loss: 1.0182 - acc: 0.4694 - val_loss: 1.1555 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 413/500\n",
      " - 0s - loss: 1.0167 - acc: 0.4654 - val_loss: 1.1557 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.908 \t val: 0.463\n",
      "Epoch 414/500\n",
      " - 0s - loss: 1.0158 - acc: 0.4732 - val_loss: 1.1559 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 415/500\n",
      " - 1s - loss: 1.0154 - acc: 0.4666 - val_loss: 1.1564 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 416/500\n",
      " - 0s - loss: 1.0146 - acc: 0.4688 - val_loss: 1.1561 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.909 \t val: 0.463\n",
      "Epoch 417/500\n",
      " - 0s - loss: 1.0173 - acc: 0.4696 - val_loss: 1.1565 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.910 \t val: 0.463\n",
      "Epoch 418/500\n",
      " - 1s - loss: 1.0151 - acc: 0.4689 - val_loss: 1.1567 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.909 \t val: 0.463\n",
      "Epoch 419/500\n",
      " - 1s - loss: 1.0148 - acc: 0.4739 - val_loss: 1.1569 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.909 \t val: 0.463\n",
      "Epoch 420/500\n",
      " - 0s - loss: 1.0148 - acc: 0.4678 - val_loss: 1.1566 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.909 \t val: 0.463\n",
      "Epoch 421/500\n",
      " - 0s - loss: 1.0166 - acc: 0.4689 - val_loss: 1.1569 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.908 \t val: 0.463\n",
      "Epoch 422/500\n",
      " - 0s - loss: 1.0157 - acc: 0.4699 - val_loss: 1.1571 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 423/500\n",
      " - 1s - loss: 1.0156 - acc: 0.4700 - val_loss: 1.1571 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 424/500\n",
      " - 1s - loss: 1.0151 - acc: 0.4719 - val_loss: 1.1571 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 425/500\n",
      " - 1s - loss: 1.0145 - acc: 0.4740 - val_loss: 1.1569 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 426/500\n",
      " - 0s - loss: 1.0158 - acc: 0.4707 - val_loss: 1.1572 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 427/500\n",
      " - 0s - loss: 1.0151 - acc: 0.4685 - val_loss: 1.1573 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 428/500\n",
      " - 1s - loss: 1.0129 - acc: 0.4727 - val_loss: 1.1575 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 429/500\n",
      " - 1s - loss: 1.0133 - acc: 0.4745 - val_loss: 1.1573 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 430/500\n",
      " - 0s - loss: 1.0152 - acc: 0.4710 - val_loss: 1.1573 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 431/500\n",
      " - 0s - loss: 1.0174 - acc: 0.4683 - val_loss: 1.1578 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 432/500\n",
      " - 0s - loss: 1.0162 - acc: 0.4699 - val_loss: 1.1575 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 433/500\n",
      " - 1s - loss: 1.0156 - acc: 0.4702 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.909 \t val: 0.463\n",
      "Epoch 434/500\n",
      " - 0s - loss: 1.0165 - acc: 0.4717 - val_loss: 1.1575 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.908 \t val: 0.463\n",
      "Epoch 435/500\n",
      " - 1s - loss: 1.0133 - acc: 0.4739 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 436/500\n",
      " - 1s - loss: 1.0147 - acc: 0.4708 - val_loss: 1.1575 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 437/500\n",
      " - 0s - loss: 1.0145 - acc: 0.4692 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 438/500\n",
      " - 0s - loss: 1.0152 - acc: 0.4700 - val_loss: 1.1575 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.908 \t val: 0.463\n",
      "Epoch 439/500\n",
      " - 0s - loss: 1.0151 - acc: 0.4721 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 440/500\n",
      " - 0s - loss: 1.0148 - acc: 0.4706 - val_loss: 1.1578 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 441/500\n",
      " - 0s - loss: 1.0135 - acc: 0.4716 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 442/500\n",
      " - 1s - loss: 1.0151 - acc: 0.4713 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 443/500\n",
      " - 0s - loss: 1.0134 - acc: 0.4691 - val_loss: 1.1576 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 444/500\n",
      " - 0s - loss: 1.0132 - acc: 0.4713 - val_loss: 1.1573 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 445/500\n",
      " - 0s - loss: 1.0153 - acc: 0.4724 - val_loss: 1.1570 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.908 \t val: 0.463\n",
      "Epoch 446/500\n",
      " - 0s - loss: 1.0148 - acc: 0.4699 - val_loss: 1.1578 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 447/500\n",
      " - 1s - loss: 1.0154 - acc: 0.4716 - val_loss: 1.1576 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 448/500\n",
      " - 0s - loss: 1.0150 - acc: 0.4720 - val_loss: 1.1578 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 449/500\n",
      " - 1s - loss: 1.0148 - acc: 0.4714 - val_loss: 1.1576 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 450/500\n",
      " - 1s - loss: 1.0132 - acc: 0.4742 - val_loss: 1.1581 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 451/500\n",
      " - 1s - loss: 1.0141 - acc: 0.4734 - val_loss: 1.1583 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.907 \t val: 0.463\n",
      "Epoch 452/500\n",
      " - 0s - loss: 1.0135 - acc: 0.4710 - val_loss: 1.1589 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 453/500\n",
      " - 0s - loss: 1.0140 - acc: 0.4743 - val_loss: 1.1581 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 454/500\n",
      " - 1s - loss: 1.0130 - acc: 0.4705 - val_loss: 1.1580 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 455/500\n",
      " - 1s - loss: 1.0149 - acc: 0.4710 - val_loss: 1.1578 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 456/500\n",
      " - 0s - loss: 1.0148 - acc: 0.4699 - val_loss: 1.1579 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 457/500\n",
      " - 0s - loss: 1.0137 - acc: 0.4715 - val_loss: 1.1580 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 458/500\n",
      " - 0s - loss: 1.0124 - acc: 0.4741 - val_loss: 1.1578 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 459/500\n",
      " - 0s - loss: 1.0133 - acc: 0.4746 - val_loss: 1.1576 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 460/500\n",
      " - 0s - loss: 1.0128 - acc: 0.4714 - val_loss: 1.1573 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 461/500\n",
      " - 0s - loss: 1.0148 - acc: 0.4705 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 462/500\n",
      " - 0s - loss: 1.0143 - acc: 0.4689 - val_loss: 1.1572 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 463/500\n",
      " - 0s - loss: 1.0134 - acc: 0.4708 - val_loss: 1.1571 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 464/500\n",
      " - 0s - loss: 1.0143 - acc: 0.4711 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 465/500\n",
      " - 0s - loss: 1.0131 - acc: 0.4709 - val_loss: 1.1577 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 466/500\n",
      " - 1s - loss: 1.0146 - acc: 0.4741 - val_loss: 1.1576 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 467/500\n",
      " - 0s - loss: 1.0114 - acc: 0.4730 - val_loss: 1.1572 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 468/500\n",
      " - 0s - loss: 1.0137 - acc: 0.4741 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 469/500\n",
      " - 0s - loss: 1.0119 - acc: 0.4697 - val_loss: 1.1569 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 470/500\n",
      " - 1s - loss: 1.0148 - acc: 0.4724 - val_loss: 1.1568 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 471/500\n",
      " - 1s - loss: 1.0101 - acc: 0.4743 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 472/500\n",
      " - 0s - loss: 1.0142 - acc: 0.4742 - val_loss: 1.1576 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 473/500\n",
      " - 0s - loss: 1.0130 - acc: 0.4718 - val_loss: 1.1579 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 474/500\n",
      " - 0s - loss: 1.0114 - acc: 0.4744 - val_loss: 1.1578 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 475/500\n",
      " - 1s - loss: 1.0132 - acc: 0.4699 - val_loss: 1.1580 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 476/500\n",
      " - 1s - loss: 1.0128 - acc: 0.4735 - val_loss: 1.1584 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 477/500\n",
      " - 0s - loss: 1.0130 - acc: 0.4724 - val_loss: 1.1585 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 478/500\n",
      " - 1s - loss: 1.0127 - acc: 0.4751 - val_loss: 1.1582 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 479/500\n",
      " - 1s - loss: 1.0122 - acc: 0.4697 - val_loss: 1.1577 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 480/500\n",
      " - 0s - loss: 1.0111 - acc: 0.4700 - val_loss: 1.1585 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 481/500\n",
      " - 0s - loss: 1.0108 - acc: 0.4723 - val_loss: 1.1587 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 482/500\n",
      " - 0s - loss: 1.0122 - acc: 0.4742 - val_loss: 1.1585 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.905 \t val: 0.463\n",
      "Epoch 483/500\n",
      " - 0s - loss: 1.0126 - acc: 0.4715 - val_loss: 1.1585 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.904 \t val: 0.463\n",
      "Epoch 484/500\n",
      " - 0s - loss: 1.0136 - acc: 0.4688 - val_loss: 1.1583 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 485/500\n",
      " - 0s - loss: 1.0118 - acc: 0.4700 - val_loss: 1.1580 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 486/500\n",
      " - 1s - loss: 1.0129 - acc: 0.4741 - val_loss: 1.1579 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 487/500\n",
      " - 1s - loss: 1.0101 - acc: 0.4736 - val_loss: 1.1577 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 488/500\n",
      " - 0s - loss: 1.0146 - acc: 0.4717 - val_loss: 1.1575 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 489/500\n",
      " - 0s - loss: 1.0112 - acc: 0.4713 - val_loss: 1.1579 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 490/500\n",
      " - 0s - loss: 1.0139 - acc: 0.4712 - val_loss: 1.1579 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 491/500\n",
      " - 0s - loss: 1.0114 - acc: 0.4710 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 492/500\n",
      " - 1s - loss: 1.0125 - acc: 0.4728 - val_loss: 1.1580 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 493/500\n",
      " - 1s - loss: 1.0108 - acc: 0.4709 - val_loss: 1.1580 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 494/500\n",
      " - 1s - loss: 1.0126 - acc: 0.4708 - val_loss: 1.1576 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 495/500\n",
      " - 1s - loss: 1.0116 - acc: 0.4742 - val_loss: 1.1580 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 496/500\n",
      " - 0s - loss: 1.0117 - acc: 0.4721 - val_loss: 1.1577 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.901 \t val: 0.463\n",
      "Epoch 497/500\n",
      " - 1s - loss: 1.0111 - acc: 0.4727 - val_loss: 1.1577 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.898 \t val: 0.463\n",
      "Epoch 498/500\n",
      " - 0s - loss: 1.0122 - acc: 0.4729 - val_loss: 1.1574 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.899 \t val: 0.463\n",
      "Epoch 499/500\n",
      " - 1s - loss: 1.0107 - acc: 0.4738 - val_loss: 1.1571 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.898 \t val: 0.463\n",
      "Epoch 500/500\n",
      " - 1s - loss: 1.0113 - acc: 0.4738 - val_loss: 1.1572 - val_acc: 0.2505\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f788d136dd8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, kernel_initializer=\"uniform\", input_shape = (32,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "adam = Adam(lr = 0.00001)\n",
    "y_train_one_hot = one_hot(y_train, 3)\n",
    "y_test_one_hot = one_hot(y_test, 3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(encoder_X_train, y_train_one_hot, epochs=500, batch_size=128, validation_data=(encoder_X_test, y_test_one_hot), \n",
    "          verbose=2, shuffle=True, callbacks=CALLBACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[  0 645   0]\n",
      " [  0 349   0]\n",
      " [  0 399   0]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEYCAYAAADLZOR0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7892067e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEYCAYAAAApuP8NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f788d22fe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = model.predict(encoder_X_test)\n",
    "\n",
    "# decode one-hot to single labels\n",
    "preds = [ np.argmax(pred, axis = 0) for pred in preds ]\n",
    "labels = [ np.argmax(label, axis = 0) for label in y_test_one_hot ]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(labels, preds)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"non-frail\", \"pre-frail\", \"frail\"],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"non-frail\", \"pre-frail\", \"frail\"], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
