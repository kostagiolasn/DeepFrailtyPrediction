{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, UpSampling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the random seeds\n",
    "random.seed(1)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "\n",
    "# Description: This function is responsible for loading our data.\n",
    "# Args in: filepath - the path of the .mat file containing the data\n",
    "# Returns: inputs - the data, labels - the labels corresponding to the data, \n",
    "#            patients - the patients corresponding to the data\n",
    "\n",
    "    mat = scipy.io.loadmat(filepath)\n",
    "    inputs = mat['Xrec'][:]\n",
    "    labels = mat['Y']\n",
    "    patients = mat['patientID']\n",
    "\n",
    "    labels = np.einsum('ij->ji', labels)\n",
    "    labels = [label for sublist in labels for label in sublist]\n",
    "    patients = np.einsum('ij->ji', patients)\n",
    "    patients = [patient for sublist in patients for patient in sublist]\n",
    "\n",
    "    return inputs, labels, patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_duplicates(duplicates_list, patients):\n",
    "\n",
    "# Description: This function is responsible for filtering out\n",
    "# some patients that are found to be present in more than one classes.\n",
    "# Args in & Returns are self-explanatory.\n",
    "    \n",
    "    patients = list(patients)\n",
    "    \n",
    "    for duplicate in duplicates_list:\n",
    "        patients = list(filter(lambda a: a != duplicate, patients))\n",
    "    \n",
    "    return np.asarray(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, labels, patients = load_data('/home/nikos/Desktop/Zacharaki/PARAFAC missing values 0_90/ReconstructedTensorAndFeatures90Missing_StrSGD.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = filter_duplicates([1002, 1104], patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(inputs, targets, patients, patients_for_val):\n",
    "\n",
    "# Description: This function is responsible for splitting our data into training and test data\n",
    "# Args in: patients_for_val - list containing the patients for our test data\n",
    "# Returns: the data and labels of our training and test data\n",
    "\n",
    "    X_train_size = X_val_size = 0\n",
    "\n",
    "    patients_for_train = [item for item in list(np.unique(patients)) if item not in patients_for_val]\n",
    "    idpatients_val = []\n",
    "    for patient in patients_for_val:\n",
    "        idpatient = [i for i, x in enumerate(patients) if x == patient]\n",
    "        idpatients_val.append(idpatient)\n",
    "    idpatients_val = [item for sublist in idpatients_val for item in sublist]\n",
    "    X_val = [inputs[i] for i in idpatients_val]\n",
    "    y_val = [targets[i] for i in idpatients_val]\n",
    "\n",
    "    idpatients_train = list(set([i for i in range(inputs.shape[0])]) - set(idpatients_val))\n",
    "    inputs = [inputs[i] for i in idpatients_train]\n",
    "    targets = [targets[i] for i in idpatients_train]\n",
    "\n",
    "    return np.asarray(inputs), np.asarray(targets), np.asarray(X_val), np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(inputs, labels, patients, [1106, 1107, 2097])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "\n",
    "# Description: This function preprocesses our data. \n",
    "# We want to ensure that our training data have zero mean and unit variance. \n",
    "# We also use subtract the same mean from the test data and then devide them by\n",
    "# the same standard deviation. We do that to ensure that no information about the\n",
    "# test set distribution is known ahead of time.\n",
    "\n",
    "# Args in & Returns are self-explanatory.\n",
    "\n",
    "    X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis = 0)\n",
    "    \n",
    "    X_train, X_val, train_ground, valid_ground = train_test_split(X_train, X_train, test_size = 0.1, random_state = 13)\n",
    "\n",
    "    X_test = (X_test -  np.mean(X_train, axis=0)) / np.std(X_train, axis = 0)\n",
    "    \n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1] * X_val.shape[2]))\n",
    "    train_ground = np.reshape(train_ground, (train_ground.shape[0], train_ground.shape[1] * train_ground.shape[2]))\n",
    "    valid_ground = np.reshape(valid_ground, (valid_ground.shape[0], valid_ground.shape[1] * valid_ground.shape[2]))\n",
    "    \n",
    "    return X_train, X_val, train_ground, valid_ground, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, train_ground, valid_ground, X_test = preprocess_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10500)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                336032    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10500)             346500    \n",
      "=================================================================\n",
      "Total params: 682,532\n",
      "Trainable params: 682,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 16512 samples, validate on 1835 samples\n",
      "Epoch 1/30\n",
      " - 2s - loss: 0.7392 - mean_squared_error: 0.7392 - val_loss: 0.6225 - val_mean_squared_error: 0.6225\n",
      "Epoch 2/30\n",
      " - 2s - loss: 0.6591 - mean_squared_error: 0.6591 - val_loss: 0.6166 - val_mean_squared_error: 0.6166\n",
      "Epoch 3/30\n",
      " - 2s - loss: 0.6559 - mean_squared_error: 0.6559 - val_loss: 0.6168 - val_mean_squared_error: 0.6168\n",
      "Epoch 4/30\n",
      " - 3s - loss: 0.6548 - mean_squared_error: 0.6548 - val_loss: 0.6140 - val_mean_squared_error: 0.6140\n",
      "Epoch 5/30\n",
      " - 3s - loss: 0.6530 - mean_squared_error: 0.6530 - val_loss: 0.6128 - val_mean_squared_error: 0.6128\n",
      "Epoch 6/30\n",
      " - 3s - loss: 0.6520 - mean_squared_error: 0.6520 - val_loss: 0.6123 - val_mean_squared_error: 0.6123\n",
      "Epoch 7/30\n",
      " - 3s - loss: 0.6510 - mean_squared_error: 0.6510 - val_loss: 0.6106 - val_mean_squared_error: 0.6106\n",
      "Epoch 8/30\n",
      " - 2s - loss: 0.6493 - mean_squared_error: 0.6493 - val_loss: 0.6105 - val_mean_squared_error: 0.6105\n",
      "Epoch 9/30\n",
      " - 2s - loss: 0.6480 - mean_squared_error: 0.6480 - val_loss: 0.6094 - val_mean_squared_error: 0.6094\n",
      "Epoch 10/30\n",
      " - 2s - loss: 0.6474 - mean_squared_error: 0.6474 - val_loss: 0.6081 - val_mean_squared_error: 0.6081\n",
      "Epoch 11/30\n",
      " - 2s - loss: 0.6460 - mean_squared_error: 0.6460 - val_loss: 0.6059 - val_mean_squared_error: 0.6059\n",
      "Epoch 12/30\n",
      " - 2s - loss: 0.6448 - mean_squared_error: 0.6448 - val_loss: 0.6050 - val_mean_squared_error: 0.6050\n",
      "Epoch 13/30\n",
      " - 2s - loss: 0.6439 - mean_squared_error: 0.6439 - val_loss: 0.6044 - val_mean_squared_error: 0.6044\n",
      "Epoch 14/30\n",
      " - 3s - loss: 0.6433 - mean_squared_error: 0.6433 - val_loss: 0.6040 - val_mean_squared_error: 0.6040\n",
      "Epoch 15/30\n",
      " - 2s - loss: 0.6426 - mean_squared_error: 0.6426 - val_loss: 0.6030 - val_mean_squared_error: 0.6030\n",
      "Epoch 16/30\n",
      " - 3s - loss: 0.6426 - mean_squared_error: 0.6426 - val_loss: 0.6028 - val_mean_squared_error: 0.6028\n",
      "Epoch 17/30\n",
      " - 3s - loss: 0.6414 - mean_squared_error: 0.6414 - val_loss: 0.6020 - val_mean_squared_error: 0.6020\n",
      "Epoch 18/30\n",
      " - 3s - loss: 0.6407 - mean_squared_error: 0.6407 - val_loss: 0.6013 - val_mean_squared_error: 0.6013\n",
      "Epoch 19/30\n",
      " - 3s - loss: 0.6403 - mean_squared_error: 0.6403 - val_loss: 0.6004 - val_mean_squared_error: 0.6004\n",
      "Epoch 20/30\n",
      " - 2s - loss: 0.6399 - mean_squared_error: 0.6399 - val_loss: 0.6001 - val_mean_squared_error: 0.6001\n",
      "Epoch 21/30\n",
      " - 3s - loss: 0.6396 - mean_squared_error: 0.6396 - val_loss: 0.6000 - val_mean_squared_error: 0.6000\n",
      "Epoch 22/30\n",
      " - 2s - loss: 0.6390 - mean_squared_error: 0.6390 - val_loss: 0.5995 - val_mean_squared_error: 0.5995\n",
      "Epoch 23/30\n",
      " - 2s - loss: 0.6390 - mean_squared_error: 0.6390 - val_loss: 0.5988 - val_mean_squared_error: 0.5988\n",
      "Epoch 24/30\n",
      " - 2s - loss: 0.6382 - mean_squared_error: 0.6382 - val_loss: 0.5987 - val_mean_squared_error: 0.5987\n",
      "Epoch 25/30\n",
      " - 3s - loss: 0.6381 - mean_squared_error: 0.6381 - val_loss: 0.5980 - val_mean_squared_error: 0.5980\n",
      "Epoch 26/30\n",
      " - 2s - loss: 0.6380 - mean_squared_error: 0.6380 - val_loss: 0.5982 - val_mean_squared_error: 0.5982\n",
      "Epoch 27/30\n",
      " - 2s - loss: 0.6375 - mean_squared_error: 0.6375 - val_loss: 0.5982 - val_mean_squared_error: 0.5982\n",
      "Epoch 28/30\n",
      " - 2s - loss: 0.6376 - mean_squared_error: 0.6376 - val_loss: 0.5976 - val_mean_squared_error: 0.5976\n",
      "Epoch 29/30\n",
      " - 2s - loss: 0.6371 - mean_squared_error: 0.6371 - val_loss: 0.5977 - val_mean_squared_error: 0.5977\n",
      "Epoch 30/30\n",
      " - 3s - loss: 0.6370 - mean_squared_error: 0.6370 - val_loss: 0.5969 - val_mean_squared_error: 0.5969\n"
     ]
    }
   ],
   "source": [
    "input_signal = Input(shape = (X_train.shape[1],))\n",
    "encoded = Dense(32, activation = 'relu')(input_signal)\n",
    "\n",
    "# decoder\n",
    "decoded = Dense(1500*7, activation = 'sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input = input_signal, output = decoded)\n",
    "\n",
    "encoder = Model(input = input_signal, output = encoded)\n",
    "encoded_input = Input(shape=(32,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=['mse'])\n",
    "print(autoencoder.summary())\n",
    "history = autoencoder.fit(X_train, train_ground, epochs=30, batch_size=128, \n",
    "                                    validation_data=(X_val, valid_ground), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_X_train = [encoder.predict(X_train), encoder.predict(X_val)]\n",
    "encoder_X_train = [i for sublist in encoder_X_train for i in sublist]\n",
    "encoder_X_test = encoder.predict(X_test)\n",
    "\n",
    "encoder_X_train = np.asarray(encoder_X_train)\n",
    "encoder_X_test = np.asarray(encoder_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = list(zip(encoder_X_train, y_train))\n",
    "random.shuffle(c)\n",
    "encoder_X_train, y_train = zip(*c)\n",
    "\n",
    "c = list(zip(encoder_X_test, y_test))\n",
    "random.shuffle(c)\n",
    "encoder_X_test, y_test = zip(*c)\n",
    "\n",
    "encoder_X_train = np.asarray(encoder_X_train)\n",
    "encoder_X_test = np.asarray(encoder_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(labels, n_class = 6):\n",
    "    \"\"\" One-hot encoding \"\"\"\n",
    "    expansion = np.eye(n_class)\n",
    "    y = []\n",
    "    for i in labels:\n",
    "        y.append(expansion[int(i)])\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import keras.backend as K\n",
    "\n",
    "class BalancedAccuracy(Callback):\n",
    "    def __init__(self, train_data, validation_data):\n",
    "        super(BalancedAccuracy, self).__init__()\n",
    "        self.acas = []\n",
    "        self.validation_data = validation_data\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        X_val = self.validation_data[0]\n",
    "        y_val = self.validation_data[1]\n",
    "\n",
    "        X_train = self.train_data[0]\n",
    "        y_train = self.train_data[1]\n",
    "\n",
    "        y_val_pred = self.model.predict(X_val)\n",
    "        y_train_pred = self.model.predict(X_train)\n",
    "\n",
    "        val_score = self.eval_avg_class_acc(y_val, y_val_pred)\n",
    "        train_score = self.eval_avg_class_acc(y_train, y_train_pred)\n",
    "\n",
    "        self.acas.append([val_score])\n",
    "        self.acas.append([train_score])\n",
    "\n",
    "        print(\"\\nBalanced Accuracy - train: %.3f \\t val: %.3f\"%(train_score, val_score))        \n",
    "        \n",
    "    def eval_avg_class_acc(self, y_true, y_pred):\n",
    "\n",
    "        # decode one-hot to single labels\n",
    "        y_pred = y_pred.round()\n",
    "        y_pred = [ np.argmax(pred, axis = 0) for pred in y_pred ]\n",
    "        y_true = [ np.argmax(label, axis = 0) for label in y_true ]\n",
    "\n",
    "        cf = confusion_matrix(y_true, y_pred)\n",
    "        if np.unique(y_true).shape[0] == 2:\n",
    "            sensitivity = float(cf[1][1]) / float((cf[1][1] + cf[1][0]))\n",
    "            specificity = float(cf[0][0]) / float((cf[0][1] + cf[0][0]))\n",
    "\n",
    "            balanced_acc = (sensitivity + specificity) / 2\n",
    "        else:\n",
    "            balanced_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        return balanced_acc\n",
    "\n",
    "def weighted_categorical_crossentropy(y_true, y_pred, weights):\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    # scale predictions so that the class probas of each sample sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calc\n",
    "    loss = y_true * K.log(y_pred) * weights\n",
    "    loss = -K.sum(loss, -1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Weighted loss function to tackle class imbalance in the dataset\n",
    "\n",
    "weights = np.array([float(len(y_train)) / float(list(y_train).count(0)), float(len(y_train)) / float(list(y_train).count(1)), float(len(y_train)) / float(list(y_train).count(2))])\n",
    "w_cat_crossentropy = partial(weighted_categorical_crossentropy, weights = weights)\n",
    "w_cat_crossentropy.__name__ = 'weighted_categorical_crossentropy'\n",
    "        \n",
    "balanced_accuracy = BalancedAccuracy(train_data = (encoder_X_train, y_train), validation_data = (encoder_X_test, y_test))\n",
    "CALLBACKS = [balanced_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18347 samples, validate on 1393 samples\n",
      "Epoch 1/500\n",
      " - 1s - loss: 5.3066 - acc: 0.3233 - val_loss: 3.8643 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.617 \t val: 0.463\n",
      "Epoch 2/500\n",
      " - 1s - loss: 5.1663 - acc: 0.3289 - val_loss: 3.8070 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.636 \t val: 0.463\n",
      "Epoch 3/500\n",
      " - 1s - loss: 5.1241 - acc: 0.3284 - val_loss: 3.7616 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.654 \t val: 0.463\n",
      "Epoch 4/500\n",
      " - 1s - loss: 5.0470 - acc: 0.3329 - val_loss: 3.7232 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.463\n",
      "Epoch 5/500\n",
      " - 1s - loss: 4.9985 - acc: 0.3255 - val_loss: 3.6961 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.684 \t val: 0.463\n",
      "Epoch 6/500\n",
      " - 0s - loss: 4.9172 - acc: 0.3307 - val_loss: 3.6757 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.696 \t val: 0.463\n",
      "Epoch 7/500\n",
      " - 1s - loss: 4.9532 - acc: 0.3236 - val_loss: 3.6570 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.711 \t val: 0.463\n",
      "Epoch 8/500\n",
      " - 1s - loss: 4.8953 - acc: 0.3329 - val_loss: 3.6456 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.717 \t val: 0.463\n",
      "Epoch 9/500\n",
      " - 0s - loss: 4.8354 - acc: 0.3275 - val_loss: 3.6383 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.732 \t val: 0.463\n",
      "Epoch 10/500\n",
      " - 0s - loss: 4.7903 - acc: 0.3353 - val_loss: 3.6333 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.741 \t val: 0.463\n",
      "Epoch 11/500\n",
      " - 0s - loss: 4.8032 - acc: 0.3324 - val_loss: 3.6344 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.751 \t val: 0.463\n",
      "Epoch 12/500\n",
      " - 1s - loss: 4.7488 - acc: 0.3293 - val_loss: 3.6376 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.759 \t val: 0.463\n",
      "Epoch 13/500\n",
      " - 1s - loss: 4.7103 - acc: 0.3318 - val_loss: 3.6410 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.770 \t val: 0.463\n",
      "Epoch 14/500\n",
      " - 0s - loss: 4.7611 - acc: 0.3258 - val_loss: 3.6409 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.780 \t val: 0.463\n",
      "Epoch 15/500\n",
      " - 0s - loss: 4.6968 - acc: 0.3351 - val_loss: 3.6489 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.785 \t val: 0.463\n",
      "Epoch 16/500\n",
      " - 0s - loss: 4.6390 - acc: 0.3366 - val_loss: 3.6549 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.793 \t val: 0.463\n",
      "Epoch 17/500\n",
      " - 0s - loss: 4.6371 - acc: 0.3352 - val_loss: 3.6597 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.802 \t val: 0.463\n",
      "Epoch 18/500\n",
      " - 1s - loss: 4.6070 - acc: 0.3331 - val_loss: 3.6582 - val_acc: 0.4774\n",
      "\n",
      "Balanced Accuracy - train: 0.809 \t val: 0.463\n",
      "Epoch 19/500\n",
      " - 1s - loss: 4.6039 - acc: 0.3362 - val_loss: 3.6596 - val_acc: 0.4838\n",
      "\n",
      "Balanced Accuracy - train: 0.818 \t val: 0.463\n",
      "Epoch 20/500\n",
      " - 1s - loss: 4.5686 - acc: 0.3334 - val_loss: 3.6625 - val_acc: 0.4838\n",
      "\n",
      "Balanced Accuracy - train: 0.820 \t val: 0.463\n",
      "Epoch 21/500\n",
      " - 0s - loss: 4.5568 - acc: 0.3404 - val_loss: 3.6691 - val_acc: 0.4831\n",
      "\n",
      "Balanced Accuracy - train: 0.824 \t val: 0.463\n",
      "Epoch 22/500\n",
      " - 1s - loss: 4.5710 - acc: 0.3346 - val_loss: 3.6773 - val_acc: 0.4788\n",
      "\n",
      "Balanced Accuracy - train: 0.835 \t val: 0.463\n",
      "Epoch 23/500\n",
      " - 1s - loss: 4.5505 - acc: 0.3367 - val_loss: 3.6884 - val_acc: 0.4781\n",
      "\n",
      "Balanced Accuracy - train: 0.836 \t val: 0.463\n",
      "Epoch 24/500\n",
      " - 0s - loss: 4.5583 - acc: 0.3348 - val_loss: 3.6921 - val_acc: 0.4781\n",
      "\n",
      "Balanced Accuracy - train: 0.842 \t val: 0.463\n",
      "Epoch 25/500\n",
      " - 0s - loss: 4.5096 - acc: 0.3431 - val_loss: 3.6961 - val_acc: 0.4752\n",
      "\n",
      "Balanced Accuracy - train: 0.848 \t val: 0.463\n",
      "Epoch 26/500\n",
      " - 1s - loss: 4.4869 - acc: 0.3403 - val_loss: 3.6978 - val_acc: 0.4831\n",
      "\n",
      "Balanced Accuracy - train: 0.852 \t val: 0.463\n",
      "Epoch 27/500\n",
      " - 1s - loss: 4.5238 - acc: 0.3310 - val_loss: 3.7043 - val_acc: 0.4860\n",
      "\n",
      "Balanced Accuracy - train: 0.857 \t val: 0.463\n",
      "Epoch 28/500\n",
      " - 1s - loss: 4.4667 - acc: 0.3400 - val_loss: 3.7076 - val_acc: 0.4838\n",
      "\n",
      "Balanced Accuracy - train: 0.860 \t val: 0.463\n",
      "Epoch 29/500\n",
      " - 1s - loss: 4.4588 - acc: 0.3359 - val_loss: 3.7153 - val_acc: 0.4874\n",
      "\n",
      "Balanced Accuracy - train: 0.867 \t val: 0.463\n",
      "Epoch 30/500\n",
      " - 0s - loss: 4.4632 - acc: 0.3413 - val_loss: 3.7200 - val_acc: 0.4853\n",
      "\n",
      "Balanced Accuracy - train: 0.865 \t val: 0.463\n",
      "Epoch 31/500\n",
      " - 1s - loss: 4.4044 - acc: 0.3354 - val_loss: 3.7215 - val_acc: 0.4867\n",
      "\n",
      "Balanced Accuracy - train: 0.871 \t val: 0.463\n",
      "Epoch 32/500\n",
      " - 1s - loss: 4.4288 - acc: 0.3415 - val_loss: 3.7333 - val_acc: 0.4846\n",
      "\n",
      "Balanced Accuracy - train: 0.876 \t val: 0.463\n",
      "Epoch 33/500\n",
      " - 0s - loss: 4.4209 - acc: 0.3355 - val_loss: 3.7373 - val_acc: 0.4831\n",
      "\n",
      "Balanced Accuracy - train: 0.878 \t val: 0.463\n",
      "Epoch 34/500\n",
      " - 1s - loss: 4.4609 - acc: 0.3329 - val_loss: 3.7407 - val_acc: 0.4824\n",
      "\n",
      "Balanced Accuracy - train: 0.882 \t val: 0.463\n",
      "Epoch 35/500\n",
      " - 1s - loss: 4.3720 - acc: 0.3412 - val_loss: 3.7398 - val_acc: 0.4824\n",
      "\n",
      "Balanced Accuracy - train: 0.885 \t val: 0.463\n",
      "Epoch 36/500\n",
      " - 0s - loss: 4.3357 - acc: 0.3392 - val_loss: 3.7393 - val_acc: 0.4896\n",
      "\n",
      "Balanced Accuracy - train: 0.890 \t val: 0.463\n",
      "Epoch 37/500\n",
      " - 1s - loss: 4.3921 - acc: 0.3417 - val_loss: 3.7481 - val_acc: 0.4831\n",
      "\n",
      "Balanced Accuracy - train: 0.891 \t val: 0.463\n",
      "Epoch 38/500\n",
      " - 1s - loss: 4.3296 - acc: 0.3409 - val_loss: 3.7547 - val_acc: 0.4831\n",
      "\n",
      "Balanced Accuracy - train: 0.894 \t val: 0.463\n",
      "Epoch 39/500\n",
      " - 0s - loss: 4.3495 - acc: 0.3411 - val_loss: 3.7500 - val_acc: 0.4838\n",
      "\n",
      "Balanced Accuracy - train: 0.895 \t val: 0.463\n",
      "Epoch 40/500\n",
      " - 0s - loss: 4.3496 - acc: 0.3344 - val_loss: 3.7562 - val_acc: 0.4846\n",
      "\n",
      "Balanced Accuracy - train: 0.900 \t val: 0.463\n",
      "Epoch 41/500\n",
      " - 0s - loss: 4.3377 - acc: 0.3372 - val_loss: 3.7601 - val_acc: 0.4838\n",
      "\n",
      "Balanced Accuracy - train: 0.902 \t val: 0.463\n",
      "Epoch 42/500\n",
      " - 0s - loss: 4.3334 - acc: 0.3347 - val_loss: 3.7676 - val_acc: 0.4817\n",
      "\n",
      "Balanced Accuracy - train: 0.903 \t val: 0.463\n",
      "Epoch 43/500\n",
      " - 1s - loss: 4.3201 - acc: 0.3372 - val_loss: 3.7670 - val_acc: 0.4824\n",
      "\n",
      "Balanced Accuracy - train: 0.906 \t val: 0.463\n",
      "Epoch 44/500\n",
      " - 1s - loss: 4.3080 - acc: 0.3357 - val_loss: 3.7701 - val_acc: 0.4838\n",
      "\n",
      "Balanced Accuracy - train: 0.909 \t val: 0.463\n",
      "Epoch 45/500\n",
      " - 0s - loss: 4.3278 - acc: 0.3383 - val_loss: 3.7734 - val_acc: 0.4838\n",
      "\n",
      "Balanced Accuracy - train: 0.911 \t val: 0.463\n",
      "Epoch 46/500\n",
      " - 0s - loss: 4.2526 - acc: 0.3404 - val_loss: 3.7759 - val_acc: 0.4831\n",
      "\n",
      "Balanced Accuracy - train: 0.913 \t val: 0.463\n",
      "Epoch 47/500\n",
      " - 0s - loss: 4.2698 - acc: 0.3435 - val_loss: 3.7772 - val_acc: 0.4810\n",
      "\n",
      "Balanced Accuracy - train: 0.913 \t val: 0.463\n",
      "Epoch 48/500\n",
      " - 0s - loss: 4.2282 - acc: 0.3419 - val_loss: 3.7780 - val_acc: 0.4817\n",
      "\n",
      "Balanced Accuracy - train: 0.915 \t val: 0.463\n",
      "Epoch 49/500\n",
      " - 0s - loss: 4.2562 - acc: 0.3415 - val_loss: 3.7806 - val_acc: 0.4817\n",
      "\n",
      "Balanced Accuracy - train: 0.918 \t val: 0.463\n",
      "Epoch 50/500\n",
      " - 0s - loss: 4.2260 - acc: 0.3431 - val_loss: 3.7866 - val_acc: 0.4846\n",
      "\n",
      "Balanced Accuracy - train: 0.920 \t val: 0.463\n",
      "Epoch 51/500\n",
      " - 0s - loss: 4.2345 - acc: 0.3391 - val_loss: 3.7882 - val_acc: 0.4817\n",
      "\n",
      "Balanced Accuracy - train: 0.919 \t val: 0.463\n",
      "Epoch 52/500\n",
      " - 1s - loss: 4.1771 - acc: 0.3457 - val_loss: 3.7919 - val_acc: 0.4817\n",
      "\n",
      "Balanced Accuracy - train: 0.922 \t val: 0.463\n",
      "Epoch 53/500\n",
      " - 1s - loss: 4.1981 - acc: 0.3414 - val_loss: 3.7949 - val_acc: 0.4774\n",
      "\n",
      "Balanced Accuracy - train: 0.924 \t val: 0.463\n",
      "Epoch 54/500\n",
      " - 1s - loss: 4.2209 - acc: 0.3369 - val_loss: 3.7903 - val_acc: 0.4882\n",
      "\n",
      "Balanced Accuracy - train: 0.929 \t val: 0.463\n",
      "Epoch 55/500\n",
      " - 1s - loss: 4.1485 - acc: 0.3396 - val_loss: 3.7900 - val_acc: 0.4874\n",
      "\n",
      "Balanced Accuracy - train: 0.930 \t val: 0.463\n",
      "Epoch 56/500\n",
      " - 1s - loss: 4.1664 - acc: 0.3389 - val_loss: 3.7932 - val_acc: 0.4882\n",
      "\n",
      "Balanced Accuracy - train: 0.930 \t val: 0.463\n",
      "Epoch 57/500\n",
      " - 0s - loss: 4.1757 - acc: 0.3403 - val_loss: 3.7945 - val_acc: 0.4932\n",
      "\n",
      "Balanced Accuracy - train: 0.931 \t val: 0.463\n",
      "Epoch 58/500\n",
      " - 0s - loss: 4.1724 - acc: 0.3403 - val_loss: 3.8007 - val_acc: 0.4882\n",
      "\n",
      "Balanced Accuracy - train: 0.932 \t val: 0.463\n",
      "Epoch 59/500\n",
      " - 0s - loss: 4.1770 - acc: 0.3432 - val_loss: 3.7978 - val_acc: 0.4817\n",
      "\n",
      "Balanced Accuracy - train: 0.934 \t val: 0.463\n",
      "Epoch 60/500\n",
      " - 0s - loss: 4.1553 - acc: 0.3379 - val_loss: 3.7991 - val_acc: 0.4903\n",
      "\n",
      "Balanced Accuracy - train: 0.934 \t val: 0.463\n",
      "Epoch 61/500\n",
      " - 0s - loss: 4.1432 - acc: 0.3431 - val_loss: 3.8005 - val_acc: 0.4889\n",
      "\n",
      "Balanced Accuracy - train: 0.936 \t val: 0.463\n",
      "Epoch 62/500\n",
      " - 0s - loss: 4.1218 - acc: 0.3437 - val_loss: 3.7965 - val_acc: 0.4910\n",
      "\n",
      "Balanced Accuracy - train: 0.936 \t val: 0.463\n",
      "Epoch 63/500\n",
      " - 1s - loss: 4.1514 - acc: 0.3367 - val_loss: 3.7958 - val_acc: 0.4910\n",
      "\n",
      "Balanced Accuracy - train: 0.939 \t val: 0.463\n",
      "Epoch 64/500\n",
      " - 1s - loss: 4.1285 - acc: 0.3356 - val_loss: 3.8047 - val_acc: 0.4803\n",
      "\n",
      "Balanced Accuracy - train: 0.940 \t val: 0.463\n",
      "Epoch 65/500\n",
      " - 1s - loss: 4.0863 - acc: 0.3401 - val_loss: 3.8129 - val_acc: 0.4731\n",
      "\n",
      "Balanced Accuracy - train: 0.941 \t val: 0.463\n",
      "Epoch 66/500\n",
      " - 1s - loss: 4.0900 - acc: 0.3407 - val_loss: 3.8086 - val_acc: 0.4867\n",
      "\n",
      "Balanced Accuracy - train: 0.944 \t val: 0.463\n",
      "Epoch 67/500\n",
      " - 1s - loss: 4.0976 - acc: 0.3420 - val_loss: 3.8180 - val_acc: 0.4731\n",
      "\n",
      "Balanced Accuracy - train: 0.946 \t val: 0.463\n",
      "Epoch 68/500\n",
      " - 1s - loss: 4.0906 - acc: 0.3444 - val_loss: 3.8142 - val_acc: 0.4831\n",
      "\n",
      "Balanced Accuracy - train: 0.944 \t val: 0.463\n",
      "Epoch 69/500\n",
      " - 0s - loss: 4.1039 - acc: 0.3407 - val_loss: 3.8224 - val_acc: 0.4745\n",
      "\n",
      "Balanced Accuracy - train: 0.948 \t val: 0.463\n",
      "Epoch 70/500\n",
      " - 0s - loss: 4.0854 - acc: 0.3448 - val_loss: 3.8229 - val_acc: 0.4702\n",
      "\n",
      "Balanced Accuracy - train: 0.949 \t val: 0.463\n",
      "Epoch 71/500\n",
      " - 0s - loss: 4.0600 - acc: 0.3430 - val_loss: 3.8203 - val_acc: 0.4724\n",
      "\n",
      "Balanced Accuracy - train: 0.949 \t val: 0.463\n",
      "Epoch 72/500\n",
      " - 1s - loss: 4.0662 - acc: 0.3410 - val_loss: 3.8196 - val_acc: 0.4795\n",
      "\n",
      "Balanced Accuracy - train: 0.949 \t val: 0.463\n",
      "Epoch 73/500\n",
      " - 0s - loss: 4.0673 - acc: 0.3413 - val_loss: 3.8204 - val_acc: 0.4795\n",
      "\n",
      "Balanced Accuracy - train: 0.952 \t val: 0.463\n",
      "Epoch 74/500\n",
      " - 0s - loss: 4.0321 - acc: 0.3401 - val_loss: 3.8208 - val_acc: 0.4824\n",
      "\n",
      "Balanced Accuracy - train: 0.953 \t val: 0.463\n",
      "Epoch 75/500\n",
      " - 1s - loss: 4.0780 - acc: 0.3374 - val_loss: 3.8284 - val_acc: 0.4760\n",
      "\n",
      "Balanced Accuracy - train: 0.953 \t val: 0.463\n",
      "Epoch 76/500\n",
      " - 0s - loss: 4.0352 - acc: 0.3394 - val_loss: 3.8366 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.954 \t val: 0.463\n",
      "Epoch 77/500\n",
      " - 1s - loss: 4.0335 - acc: 0.3398 - val_loss: 3.8346 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.956 \t val: 0.463\n",
      "Epoch 78/500\n",
      " - 0s - loss: 3.9956 - acc: 0.3440 - val_loss: 3.8369 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.957 \t val: 0.463\n",
      "Epoch 79/500\n",
      " - 0s - loss: 4.0436 - acc: 0.3408 - val_loss: 3.8376 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.958 \t val: 0.463\n",
      "Epoch 80/500\n",
      " - 0s - loss: 3.9939 - acc: 0.3414 - val_loss: 3.8374 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.959 \t val: 0.463\n",
      "Epoch 81/500\n",
      " - 0s - loss: 4.0034 - acc: 0.3407 - val_loss: 3.8407 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.960 \t val: 0.463\n",
      "Epoch 82/500\n",
      " - 0s - loss: 3.9888 - acc: 0.3379 - val_loss: 3.8422 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.958 \t val: 0.463\n",
      "Epoch 83/500\n",
      " - 0s - loss: 3.9886 - acc: 0.3379 - val_loss: 3.8398 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.959 \t val: 0.463\n",
      "Epoch 84/500\n",
      " - 0s - loss: 3.9576 - acc: 0.3360 - val_loss: 3.8389 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.962 \t val: 0.463\n",
      "Epoch 85/500\n",
      " - 1s - loss: 3.9998 - acc: 0.3424 - val_loss: 3.8378 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.960 \t val: 0.463\n",
      "Epoch 86/500\n",
      " - 0s - loss: 3.9603 - acc: 0.3465 - val_loss: 3.8341 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.962 \t val: 0.463\n",
      "Epoch 87/500\n",
      " - 1s - loss: 3.9781 - acc: 0.3372 - val_loss: 3.8372 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.962 \t val: 0.463\n",
      "Epoch 88/500\n",
      " - 1s - loss: 3.9665 - acc: 0.3362 - val_loss: 3.8367 - val_acc: 0.4709\n",
      "\n",
      "Balanced Accuracy - train: 0.963 \t val: 0.463\n",
      "Epoch 89/500\n",
      " - 0s - loss: 3.9239 - acc: 0.3445 - val_loss: 3.8358 - val_acc: 0.4702\n",
      "\n",
      "Balanced Accuracy - train: 0.964 \t val: 0.463\n",
      "Epoch 90/500\n",
      " - 1s - loss: 3.9700 - acc: 0.3380 - val_loss: 3.8376 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.963 \t val: 0.463\n",
      "Epoch 91/500\n",
      " - 1s - loss: 3.9466 - acc: 0.3382 - val_loss: 3.8411 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.965 \t val: 0.463\n",
      "Epoch 92/500\n",
      " - 1s - loss: 3.9424 - acc: 0.3432 - val_loss: 3.8426 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.965 \t val: 0.463\n",
      "Epoch 93/500\n",
      " - 1s - loss: 3.9149 - acc: 0.3390 - val_loss: 3.8404 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.967 \t val: 0.463\n",
      "Epoch 94/500\n",
      " - 1s - loss: 3.8871 - acc: 0.3464 - val_loss: 3.8424 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.966 \t val: 0.463\n",
      "Epoch 95/500\n",
      " - 0s - loss: 3.8871 - acc: 0.3399 - val_loss: 3.8459 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.966 \t val: 0.463\n",
      "Epoch 96/500\n",
      " - 0s - loss: 3.9201 - acc: 0.3436 - val_loss: 3.8436 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.967 \t val: 0.463\n",
      "Epoch 97/500\n",
      " - 0s - loss: 3.9096 - acc: 0.3384 - val_loss: 3.8467 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.968 \t val: 0.463\n",
      "Epoch 98/500\n",
      " - 0s - loss: 3.8977 - acc: 0.3411 - val_loss: 3.8478 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.969 \t val: 0.463\n",
      "Epoch 99/500\n",
      " - 1s - loss: 3.8838 - acc: 0.3422 - val_loss: 3.8489 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.969 \t val: 0.463\n",
      "Epoch 100/500\n",
      " - 1s - loss: 3.8622 - acc: 0.3463 - val_loss: 3.8484 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.969 \t val: 0.463\n",
      "Epoch 101/500\n",
      " - 1s - loss: 3.8367 - acc: 0.3435 - val_loss: 3.8483 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.969 \t val: 0.463\n",
      "Epoch 102/500\n",
      " - 1s - loss: 3.8752 - acc: 0.3421 - val_loss: 3.8519 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.970 \t val: 0.463\n",
      "Epoch 103/500\n",
      " - 1s - loss: 3.8770 - acc: 0.3407 - val_loss: 3.8550 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.971 \t val: 0.463\n",
      "Epoch 104/500\n",
      " - 1s - loss: 3.8372 - acc: 0.3452 - val_loss: 3.8521 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.972 \t val: 0.463\n",
      "Epoch 105/500\n",
      " - 1s - loss: 3.8434 - acc: 0.3419 - val_loss: 3.8495 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.973 \t val: 0.463\n",
      "Epoch 106/500\n",
      " - 1s - loss: 3.8328 - acc: 0.3429 - val_loss: 3.8504 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.973 \t val: 0.463\n",
      "Epoch 107/500\n",
      " - 1s - loss: 3.8203 - acc: 0.3421 - val_loss: 3.8491 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.973 \t val: 0.463\n",
      "Epoch 108/500\n",
      " - 1s - loss: 3.8213 - acc: 0.3422 - val_loss: 3.8507 - val_acc: 0.4709\n",
      "\n",
      "Balanced Accuracy - train: 0.974 \t val: 0.463\n",
      "Epoch 109/500\n",
      " - 1s - loss: 3.8038 - acc: 0.3461 - val_loss: 3.8493 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.975 \t val: 0.463\n",
      "Epoch 110/500\n",
      " - 1s - loss: 3.8436 - acc: 0.3372 - val_loss: 3.8493 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.975 \t val: 0.463\n",
      "Epoch 111/500\n",
      " - 1s - loss: 3.8026 - acc: 0.3411 - val_loss: 3.8513 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.975 \t val: 0.463\n",
      "Epoch 112/500\n",
      " - 1s - loss: 3.8106 - acc: 0.3466 - val_loss: 3.8525 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.977 \t val: 0.463\n",
      "Epoch 113/500\n",
      " - 1s - loss: 3.7988 - acc: 0.3446 - val_loss: 3.8602 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.976 \t val: 0.463\n",
      "Epoch 114/500\n",
      " - 1s - loss: 3.8108 - acc: 0.3393 - val_loss: 3.8564 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.976 \t val: 0.463\n",
      "Epoch 115/500\n",
      " - 1s - loss: 3.7815 - acc: 0.3468 - val_loss: 3.8558 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.977 \t val: 0.463\n",
      "Epoch 116/500\n",
      " - 1s - loss: 3.7672 - acc: 0.3471 - val_loss: 3.8580 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.977 \t val: 0.463\n",
      "Epoch 117/500\n",
      " - 1s - loss: 3.7945 - acc: 0.3436 - val_loss: 3.8593 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.977 \t val: 0.463\n",
      "Epoch 118/500\n",
      " - 1s - loss: 3.7800 - acc: 0.3445 - val_loss: 3.8583 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.976 \t val: 0.463\n",
      "Epoch 119/500\n",
      " - 1s - loss: 3.7781 - acc: 0.3398 - val_loss: 3.8571 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.977 \t val: 0.463\n",
      "Epoch 120/500\n",
      " - 1s - loss: 3.7665 - acc: 0.3436 - val_loss: 3.8521 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.977 \t val: 0.463\n",
      "Epoch 121/500\n",
      " - 1s - loss: 3.7908 - acc: 0.3423 - val_loss: 3.8534 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.977 \t val: 0.463\n",
      "Epoch 122/500\n",
      " - 1s - loss: 3.7487 - acc: 0.3465 - val_loss: 3.8553 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.977 \t val: 0.463\n",
      "Epoch 123/500\n",
      " - 0s - loss: 3.7494 - acc: 0.3427 - val_loss: 3.8531 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.979 \t val: 0.463\n",
      "Epoch 124/500\n",
      " - 0s - loss: 3.7371 - acc: 0.3441 - val_loss: 3.8515 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.979 \t val: 0.463\n",
      "Epoch 125/500\n",
      " - 1s - loss: 3.7558 - acc: 0.3443 - val_loss: 3.8471 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.979 \t val: 0.463\n",
      "Epoch 126/500\n",
      " - 1s - loss: 3.7222 - acc: 0.3416 - val_loss: 3.8450 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.979 \t val: 0.463\n",
      "Epoch 127/500\n",
      " - 1s - loss: 3.7476 - acc: 0.3431 - val_loss: 3.8468 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.978 \t val: 0.463\n",
      "Epoch 128/500\n",
      " - 0s - loss: 3.7155 - acc: 0.3475 - val_loss: 3.8491 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.979 \t val: 0.463\n",
      "Epoch 129/500\n",
      " - 1s - loss: 3.7428 - acc: 0.3442 - val_loss: 3.8538 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.979 \t val: 0.463\n",
      "Epoch 130/500\n",
      " - 0s - loss: 3.7066 - acc: 0.3452 - val_loss: 3.8560 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.981 \t val: 0.463\n",
      "Epoch 131/500\n",
      " - 1s - loss: 3.7088 - acc: 0.3480 - val_loss: 3.8567 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.980 \t val: 0.463\n",
      "Epoch 132/500\n",
      " - 1s - loss: 3.7166 - acc: 0.3466 - val_loss: 3.8563 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.981 \t val: 0.463\n",
      "Epoch 133/500\n",
      " - 1s - loss: 3.7073 - acc: 0.3474 - val_loss: 3.8565 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.981 \t val: 0.463\n",
      "Epoch 134/500\n",
      " - 1s - loss: 3.7050 - acc: 0.3459 - val_loss: 3.8602 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.981 \t val: 0.463\n",
      "Epoch 135/500\n",
      " - 1s - loss: 3.6929 - acc: 0.3534 - val_loss: 3.8591 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.980 \t val: 0.463\n",
      "Epoch 136/500\n",
      " - 1s - loss: 3.6967 - acc: 0.3399 - val_loss: 3.8611 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.981 \t val: 0.463\n",
      "Epoch 137/500\n",
      " - 1s - loss: 3.7098 - acc: 0.3383 - val_loss: 3.8583 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.982 \t val: 0.463\n",
      "Epoch 138/500\n",
      " - 1s - loss: 3.6464 - acc: 0.3540 - val_loss: 3.8608 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.982 \t val: 0.463\n",
      "Epoch 139/500\n",
      " - 1s - loss: 3.6655 - acc: 0.3422 - val_loss: 3.8609 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.983 \t val: 0.463\n",
      "Epoch 140/500\n",
      " - 1s - loss: 3.6723 - acc: 0.3454 - val_loss: 3.8611 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.982 \t val: 0.463\n",
      "Epoch 141/500\n",
      " - 0s - loss: 3.6714 - acc: 0.3452 - val_loss: 3.8599 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.982 \t val: 0.463\n",
      "Epoch 142/500\n",
      " - 1s - loss: 3.6529 - acc: 0.3492 - val_loss: 3.8649 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.983 \t val: 0.463\n",
      "Epoch 143/500\n",
      " - 1s - loss: 3.6486 - acc: 0.3461 - val_loss: 3.8618 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.983 \t val: 0.463\n",
      "Epoch 144/500\n",
      " - 1s - loss: 3.6663 - acc: 0.3417 - val_loss: 3.8596 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.984 \t val: 0.463\n",
      "Epoch 145/500\n",
      " - 1s - loss: 3.6533 - acc: 0.3462 - val_loss: 3.8596 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.984 \t val: 0.463\n",
      "Epoch 146/500\n",
      " - 1s - loss: 3.6525 - acc: 0.3385 - val_loss: 3.8651 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.984 \t val: 0.463\n",
      "Epoch 147/500\n",
      " - 0s - loss: 3.6643 - acc: 0.3433 - val_loss: 3.8601 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.984 \t val: 0.463\n",
      "Epoch 148/500\n",
      " - 0s - loss: 3.6426 - acc: 0.3503 - val_loss: 3.8675 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.984 \t val: 0.463\n",
      "Epoch 149/500\n",
      " - 0s - loss: 3.6390 - acc: 0.3476 - val_loss: 3.8665 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.985 \t val: 0.463\n",
      "Epoch 150/500\n",
      " - 1s - loss: 3.6220 - acc: 0.3458 - val_loss: 3.8624 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.985 \t val: 0.463\n",
      "Epoch 151/500\n",
      " - 1s - loss: 3.6344 - acc: 0.3399 - val_loss: 3.8648 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.985 \t val: 0.463\n",
      "Epoch 152/500\n",
      " - 1s - loss: 3.6260 - acc: 0.3436 - val_loss: 3.8687 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.986 \t val: 0.463\n",
      "Epoch 153/500\n",
      " - 0s - loss: 3.6275 - acc: 0.3492 - val_loss: 3.8686 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.985 \t val: 0.463\n",
      "Epoch 154/500\n",
      " - 1s - loss: 3.6184 - acc: 0.3453 - val_loss: 3.8666 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.986 \t val: 0.463\n",
      "Epoch 155/500\n",
      " - 1s - loss: 3.6118 - acc: 0.3438 - val_loss: 3.8652 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.986 \t val: 0.463\n",
      "Epoch 156/500\n",
      " - 1s - loss: 3.6116 - acc: 0.3485 - val_loss: 3.8663 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.986 \t val: 0.463\n",
      "Epoch 157/500\n",
      " - 0s - loss: 3.5997 - acc: 0.3446 - val_loss: 3.8650 - val_acc: 0.4709\n",
      "\n",
      "Balanced Accuracy - train: 0.987 \t val: 0.463\n",
      "Epoch 158/500\n",
      " - 0s - loss: 3.6211 - acc: 0.3444 - val_loss: 3.8704 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.987 \t val: 0.463\n",
      "Epoch 159/500\n",
      " - 1s - loss: 3.6041 - acc: 0.3435 - val_loss: 3.8709 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.986 \t val: 0.463\n",
      "Epoch 160/500\n",
      " - 1s - loss: 3.5854 - acc: 0.3438 - val_loss: 3.8710 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.987 \t val: 0.463\n",
      "Epoch 161/500\n",
      " - 1s - loss: 3.5934 - acc: 0.3447 - val_loss: 3.8726 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.988 \t val: 0.463\n",
      "Epoch 162/500\n",
      " - 1s - loss: 3.5783 - acc: 0.3487 - val_loss: 3.8707 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.988 \t val: 0.463\n",
      "Epoch 163/500\n",
      " - 1s - loss: 3.5967 - acc: 0.3391 - val_loss: 3.8705 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.987 \t val: 0.463\n",
      "Epoch 164/500\n",
      " - 1s - loss: 3.6095 - acc: 0.3448 - val_loss: 3.8692 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.987 \t val: 0.463\n",
      "Epoch 165/500\n",
      " - 1s - loss: 3.5945 - acc: 0.3462 - val_loss: 3.8681 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.987 \t val: 0.463\n",
      "Epoch 166/500\n",
      " - 0s - loss: 3.5735 - acc: 0.3490 - val_loss: 3.8694 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.987 \t val: 0.463\n",
      "Epoch 167/500\n",
      " - 1s - loss: 3.5745 - acc: 0.3468 - val_loss: 3.8681 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.988 \t val: 0.463\n",
      "Epoch 168/500\n",
      " - 1s - loss: 3.5804 - acc: 0.3465 - val_loss: 3.8719 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.988 \t val: 0.463\n",
      "Epoch 169/500\n",
      " - 0s - loss: 3.5770 - acc: 0.3435 - val_loss: 3.8732 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.988 \t val: 0.463\n",
      "Epoch 170/500\n",
      " - 1s - loss: 3.5754 - acc: 0.3410 - val_loss: 3.8721 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.988 \t val: 0.463\n",
      "Epoch 171/500\n",
      " - 1s - loss: 3.5741 - acc: 0.3474 - val_loss: 3.8755 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.988 \t val: 0.463\n",
      "Epoch 172/500\n",
      " - 1s - loss: 3.5737 - acc: 0.3436 - val_loss: 3.8719 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.988 \t val: 0.463\n",
      "Epoch 173/500\n",
      " - 1s - loss: 3.5505 - acc: 0.3473 - val_loss: 3.8746 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.989 \t val: 0.463\n",
      "Epoch 174/500\n",
      " - 1s - loss: 3.5378 - acc: 0.3452 - val_loss: 3.8733 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.989 \t val: 0.463\n",
      "Epoch 175/500\n",
      " - 1s - loss: 3.5280 - acc: 0.3498 - val_loss: 3.8749 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.989 \t val: 0.463\n",
      "Epoch 176/500\n",
      " - 1s - loss: 3.5557 - acc: 0.3440 - val_loss: 3.8746 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.989 \t val: 0.463\n",
      "Epoch 177/500\n",
      " - 1s - loss: 3.5531 - acc: 0.3502 - val_loss: 3.8784 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.989 \t val: 0.463\n",
      "Epoch 178/500\n",
      " - 1s - loss: 3.5344 - acc: 0.3532 - val_loss: 3.8756 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.989 \t val: 0.463\n",
      "Epoch 179/500\n",
      " - 1s - loss: 3.5356 - acc: 0.3465 - val_loss: 3.8780 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.990 \t val: 0.463\n",
      "Epoch 180/500\n",
      " - 0s - loss: 3.5625 - acc: 0.3426 - val_loss: 3.8781 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.990 \t val: 0.463\n",
      "Epoch 181/500\n",
      " - 1s - loss: 3.5440 - acc: 0.3493 - val_loss: 3.8814 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.990 \t val: 0.463\n",
      "Epoch 182/500\n",
      " - 1s - loss: 3.5324 - acc: 0.3446 - val_loss: 3.8817 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.990 \t val: 0.463\n",
      "Epoch 183/500\n",
      " - 0s - loss: 3.5227 - acc: 0.3480 - val_loss: 3.8775 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.990 \t val: 0.463\n",
      "Epoch 184/500\n",
      " - 0s - loss: 3.5322 - acc: 0.3471 - val_loss: 3.8755 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.990 \t val: 0.463\n",
      "Epoch 185/500\n",
      " - 0s - loss: 3.5432 - acc: 0.3439 - val_loss: 3.8755 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.990 \t val: 0.463\n",
      "Epoch 186/500\n",
      " - 1s - loss: 3.5333 - acc: 0.3481 - val_loss: 3.8768 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 187/500\n",
      " - 0s - loss: 3.5241 - acc: 0.3467 - val_loss: 3.8773 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 188/500\n",
      " - 0s - loss: 3.5342 - acc: 0.3443 - val_loss: 3.8771 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 189/500\n",
      " - 1s - loss: 3.5135 - acc: 0.3455 - val_loss: 3.8794 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 190/500\n",
      " - 1s - loss: 3.4972 - acc: 0.3458 - val_loss: 3.8799 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 191/500\n",
      " - 1s - loss: 3.5060 - acc: 0.3468 - val_loss: 3.8789 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 192/500\n",
      " - 1s - loss: 3.4855 - acc: 0.3515 - val_loss: 3.8791 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 193/500\n",
      " - 1s - loss: 3.4910 - acc: 0.3471 - val_loss: 3.8817 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 194/500\n",
      " - 1s - loss: 3.4962 - acc: 0.3468 - val_loss: 3.8787 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 195/500\n",
      " - 1s - loss: 3.5074 - acc: 0.3462 - val_loss: 3.8792 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 196/500\n",
      " - 1s - loss: 3.4961 - acc: 0.3484 - val_loss: 3.8835 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 197/500\n",
      " - 1s - loss: 3.4901 - acc: 0.3445 - val_loss: 3.8856 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 198/500\n",
      " - 0s - loss: 3.4804 - acc: 0.3510 - val_loss: 3.8882 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 199/500\n",
      " - 1s - loss: 3.4998 - acc: 0.3451 - val_loss: 3.8878 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.991 \t val: 0.463\n",
      "Epoch 200/500\n",
      " - 1s - loss: 3.4994 - acc: 0.3460 - val_loss: 3.8878 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 201/500\n",
      " - 1s - loss: 3.4952 - acc: 0.3475 - val_loss: 3.8860 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 202/500\n",
      " - 1s - loss: 3.4886 - acc: 0.3484 - val_loss: 3.8854 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 203/500\n",
      " - 0s - loss: 3.4844 - acc: 0.3487 - val_loss: 3.8871 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 204/500\n",
      " - 1s - loss: 3.4725 - acc: 0.3481 - val_loss: 3.8888 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 205/500\n",
      " - 1s - loss: 3.4843 - acc: 0.3428 - val_loss: 3.8871 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 206/500\n",
      " - 1s - loss: 3.4621 - acc: 0.3488 - val_loss: 3.8876 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 207/500\n",
      " - 1s - loss: 3.4674 - acc: 0.3462 - val_loss: 3.8884 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 208/500\n",
      " - 0s - loss: 3.4634 - acc: 0.3459 - val_loss: 3.8920 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 209/500\n",
      " - 0s - loss: 3.4741 - acc: 0.3462 - val_loss: 3.8913 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 210/500\n",
      " - 0s - loss: 3.4667 - acc: 0.3468 - val_loss: 3.8942 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 211/500\n",
      " - 0s - loss: 3.4502 - acc: 0.3465 - val_loss: 3.8951 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 212/500\n",
      " - 0s - loss: 3.4612 - acc: 0.3486 - val_loss: 3.8961 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 213/500\n",
      " - 0s - loss: 3.4578 - acc: 0.3473 - val_loss: 3.8978 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 214/500\n",
      " - 0s - loss: 3.4510 - acc: 0.3502 - val_loss: 3.8959 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.992 \t val: 0.463\n",
      "Epoch 215/500\n",
      " - 1s - loss: 3.4522 - acc: 0.3478 - val_loss: 3.8952 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 216/500\n",
      " - 1s - loss: 3.4540 - acc: 0.3441 - val_loss: 3.8949 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 217/500\n",
      " - 1s - loss: 3.4570 - acc: 0.3450 - val_loss: 3.8959 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 218/500\n",
      " - 1s - loss: 3.4370 - acc: 0.3530 - val_loss: 3.8950 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 219/500\n",
      " - 1s - loss: 3.4474 - acc: 0.3477 - val_loss: 3.8984 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 220/500\n",
      " - 1s - loss: 3.4464 - acc: 0.3483 - val_loss: 3.8985 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 221/500\n",
      " - 1s - loss: 3.4496 - acc: 0.3467 - val_loss: 3.8995 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 222/500\n",
      " - 1s - loss: 3.4327 - acc: 0.3502 - val_loss: 3.8982 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 223/500\n",
      " - 1s - loss: 3.4367 - acc: 0.3463 - val_loss: 3.8981 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 224/500\n",
      " - 0s - loss: 3.4481 - acc: 0.3417 - val_loss: 3.9008 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 225/500\n",
      " - 1s - loss: 3.4309 - acc: 0.3496 - val_loss: 3.9020 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 226/500\n",
      " - 1s - loss: 3.4390 - acc: 0.3527 - val_loss: 3.9040 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 227/500\n",
      " - 0s - loss: 3.4248 - acc: 0.3561 - val_loss: 3.9000 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 228/500\n",
      " - 1s - loss: 3.4128 - acc: 0.3495 - val_loss: 3.8981 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 229/500\n",
      " - 1s - loss: 3.4236 - acc: 0.3484 - val_loss: 3.9000 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 230/500\n",
      " - 1s - loss: 3.4365 - acc: 0.3441 - val_loss: 3.9024 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 231/500\n",
      " - 1s - loss: 3.4325 - acc: 0.3464 - val_loss: 3.9017 - val_acc: 0.4630\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 232/500\n",
      " - 0s - loss: 3.4173 - acc: 0.3460 - val_loss: 3.9002 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 233/500\n",
      " - 1s - loss: 3.4116 - acc: 0.3499 - val_loss: 3.8983 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 234/500\n",
      " - 1s - loss: 3.4300 - acc: 0.3479 - val_loss: 3.8991 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 235/500\n",
      " - 1s - loss: 3.4183 - acc: 0.3511 - val_loss: 3.9008 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 236/500\n",
      " - 1s - loss: 3.4039 - acc: 0.3508 - val_loss: 3.9028 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 237/500\n",
      " - 1s - loss: 3.4009 - acc: 0.3519 - val_loss: 3.9040 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 238/500\n",
      " - 0s - loss: 3.4173 - acc: 0.3423 - val_loss: 3.9043 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 239/500\n",
      " - 0s - loss: 3.4163 - acc: 0.3480 - val_loss: 3.9021 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 240/500\n",
      " - 0s - loss: 3.4154 - acc: 0.3487 - val_loss: 3.9009 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 241/500\n",
      " - 0s - loss: 3.4038 - acc: 0.3490 - val_loss: 3.9015 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.993 \t val: 0.463\n",
      "Epoch 242/500\n",
      " - 0s - loss: 3.4032 - acc: 0.3460 - val_loss: 3.8992 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 243/500\n",
      " - 1s - loss: 3.4017 - acc: 0.3476 - val_loss: 3.8994 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 244/500\n",
      " - 1s - loss: 3.4032 - acc: 0.3488 - val_loss: 3.8984 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 245/500\n",
      " - 0s - loss: 3.4041 - acc: 0.3474 - val_loss: 3.8993 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 246/500\n",
      " - 1s - loss: 3.4036 - acc: 0.3441 - val_loss: 3.8983 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 247/500\n",
      " - 0s - loss: 3.4009 - acc: 0.3503 - val_loss: 3.9000 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 248/500\n",
      " - 1s - loss: 3.3925 - acc: 0.3522 - val_loss: 3.9000 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 249/500\n",
      " - 1s - loss: 3.3950 - acc: 0.3455 - val_loss: 3.8972 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 250/500\n",
      " - 0s - loss: 3.3893 - acc: 0.3501 - val_loss: 3.8998 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 251/500\n",
      " - 0s - loss: 3.3971 - acc: 0.3469 - val_loss: 3.8995 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 252/500\n",
      " - 0s - loss: 3.3916 - acc: 0.3477 - val_loss: 3.9005 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 253/500\n",
      " - 0s - loss: 3.4020 - acc: 0.3462 - val_loss: 3.9029 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 254/500\n",
      " - 0s - loss: 3.3828 - acc: 0.3447 - val_loss: 3.9011 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 255/500\n",
      " - 0s - loss: 3.3799 - acc: 0.3524 - val_loss: 3.9018 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 256/500\n",
      " - 0s - loss: 3.3777 - acc: 0.3484 - val_loss: 3.9034 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 257/500\n",
      " - 0s - loss: 3.3854 - acc: 0.3518 - val_loss: 3.9049 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 258/500\n",
      " - 1s - loss: 3.3671 - acc: 0.3482 - val_loss: 3.9069 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 259/500\n",
      " - 1s - loss: 3.3756 - acc: 0.3490 - val_loss: 3.9081 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 260/500\n",
      " - 1s - loss: 3.3768 - acc: 0.3514 - val_loss: 3.9067 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.994 \t val: 0.463\n",
      "Epoch 261/500\n",
      " - 0s - loss: 3.3705 - acc: 0.3514 - val_loss: 3.9062 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 262/500\n",
      " - 0s - loss: 3.3748 - acc: 0.3556 - val_loss: 3.9077 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 263/500\n",
      " - 0s - loss: 3.3757 - acc: 0.3507 - val_loss: 3.9048 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 264/500\n",
      " - 0s - loss: 3.3759 - acc: 0.3445 - val_loss: 3.9046 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 265/500\n",
      " - 1s - loss: 3.3651 - acc: 0.3523 - val_loss: 3.9026 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 266/500\n",
      " - 1s - loss: 3.3784 - acc: 0.3463 - val_loss: 3.9043 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 267/500\n",
      " - 0s - loss: 3.3703 - acc: 0.3483 - val_loss: 3.9037 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 268/500\n",
      " - 0s - loss: 3.3703 - acc: 0.3471 - val_loss: 3.9039 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 269/500\n",
      " - 0s - loss: 3.3676 - acc: 0.3456 - val_loss: 3.9024 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 270/500\n",
      " - 0s - loss: 3.3554 - acc: 0.3529 - val_loss: 3.9013 - val_acc: 0.4702\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 271/500\n",
      " - 0s - loss: 3.3658 - acc: 0.3510 - val_loss: 3.9018 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 272/500\n",
      " - 0s - loss: 3.3644 - acc: 0.3469 - val_loss: 3.9036 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 273/500\n",
      " - 0s - loss: 3.3542 - acc: 0.3492 - val_loss: 3.9056 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 274/500\n",
      " - 0s - loss: 3.3666 - acc: 0.3512 - val_loss: 3.9074 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 275/500\n",
      " - 1s - loss: 3.3647 - acc: 0.3481 - val_loss: 3.9078 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 276/500\n",
      " - 0s - loss: 3.3566 - acc: 0.3507 - val_loss: 3.9061 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 277/500\n",
      " - 1s - loss: 3.3593 - acc: 0.3476 - val_loss: 3.9062 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 278/500\n",
      " - 0s - loss: 3.3597 - acc: 0.3540 - val_loss: 3.9046 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 279/500\n",
      " - 0s - loss: 3.3699 - acc: 0.3430 - val_loss: 3.9038 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 280/500\n",
      " - 0s - loss: 3.3483 - acc: 0.3555 - val_loss: 3.9037 - val_acc: 0.4702\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 281/500\n",
      " - 1s - loss: 3.3560 - acc: 0.3519 - val_loss: 3.9059 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 282/500\n",
      " - 1s - loss: 3.3584 - acc: 0.3457 - val_loss: 3.9058 - val_acc: 0.4702\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 283/500\n",
      " - 0s - loss: 3.3623 - acc: 0.3441 - val_loss: 3.9065 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 284/500\n",
      " - 0s - loss: 3.3556 - acc: 0.3454 - val_loss: 3.9064 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 285/500\n",
      " - 0s - loss: 3.3495 - acc: 0.3540 - val_loss: 3.9075 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 286/500\n",
      " - 0s - loss: 3.3487 - acc: 0.3525 - val_loss: 3.9062 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 287/500\n",
      " - 0s - loss: 3.3508 - acc: 0.3538 - val_loss: 3.9066 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 288/500\n",
      " - 0s - loss: 3.3431 - acc: 0.3506 - val_loss: 3.9112 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 289/500\n",
      " - 0s - loss: 3.3447 - acc: 0.3498 - val_loss: 3.9100 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.995 \t val: 0.463\n",
      "Epoch 290/500\n",
      " - 0s - loss: 3.3480 - acc: 0.3445 - val_loss: 3.9100 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 291/500\n",
      " - 0s - loss: 3.3605 - acc: 0.3441 - val_loss: 3.9089 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 292/500\n",
      " - 1s - loss: 3.3589 - acc: 0.3441 - val_loss: 3.9107 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 293/500\n",
      " - 1s - loss: 3.3310 - acc: 0.3525 - val_loss: 3.9098 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 294/500\n",
      " - 0s - loss: 3.3336 - acc: 0.3538 - val_loss: 3.9092 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 295/500\n",
      " - 0s - loss: 3.3359 - acc: 0.3525 - val_loss: 3.9110 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 296/500\n",
      " - 0s - loss: 3.3552 - acc: 0.3445 - val_loss: 3.9122 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 297/500\n",
      " - 0s - loss: 3.3350 - acc: 0.3560 - val_loss: 3.9129 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 298/500\n",
      " - 0s - loss: 3.3339 - acc: 0.3524 - val_loss: 3.9128 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 299/500\n",
      " - 0s - loss: 3.3543 - acc: 0.3456 - val_loss: 3.9109 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 300/500\n",
      " - 1s - loss: 3.3394 - acc: 0.3462 - val_loss: 3.9112 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 301/500\n",
      " - 0s - loss: 3.3403 - acc: 0.3494 - val_loss: 3.9120 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 302/500\n",
      " - 0s - loss: 3.3307 - acc: 0.3500 - val_loss: 3.9121 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 303/500\n",
      " - 0s - loss: 3.3430 - acc: 0.3471 - val_loss: 3.9139 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 304/500\n",
      " - 0s - loss: 3.3321 - acc: 0.3536 - val_loss: 3.9130 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 305/500\n",
      " - 0s - loss: 3.3350 - acc: 0.3453 - val_loss: 3.9163 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 306/500\n",
      " - 0s - loss: 3.3368 - acc: 0.3446 - val_loss: 3.9148 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 307/500\n",
      " - 0s - loss: 3.3443 - acc: 0.3470 - val_loss: 3.9136 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 308/500\n",
      " - 0s - loss: 3.3356 - acc: 0.3500 - val_loss: 3.9140 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 309/500\n",
      " - 1s - loss: 3.3249 - acc: 0.3507 - val_loss: 3.9150 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 310/500\n",
      " - 1s - loss: 3.3259 - acc: 0.3576 - val_loss: 3.9154 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 311/500\n",
      " - 0s - loss: 3.3272 - acc: 0.3528 - val_loss: 3.9152 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 312/500\n",
      " - 0s - loss: 3.3267 - acc: 0.3551 - val_loss: 3.9144 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 313/500\n",
      " - 0s - loss: 3.3286 - acc: 0.3481 - val_loss: 3.9145 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 314/500\n",
      " - 0s - loss: 3.3208 - acc: 0.3492 - val_loss: 3.9144 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 315/500\n",
      " - 1s - loss: 3.3302 - acc: 0.3516 - val_loss: 3.9150 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 316/500\n",
      " - 1s - loss: 3.3215 - acc: 0.3541 - val_loss: 3.9154 - val_acc: 0.4637\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 317/500\n",
      " - 1s - loss: 3.3226 - acc: 0.3496 - val_loss: 3.9155 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 318/500\n",
      " - 0s - loss: 3.3200 - acc: 0.3482 - val_loss: 3.9161 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 319/500\n",
      " - 0s - loss: 3.3169 - acc: 0.3526 - val_loss: 3.9180 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 320/500\n",
      " - 0s - loss: 3.3345 - acc: 0.3497 - val_loss: 3.9160 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 321/500\n",
      " - 0s - loss: 3.3199 - acc: 0.3502 - val_loss: 3.9157 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 322/500\n",
      " - 0s - loss: 3.3278 - acc: 0.3491 - val_loss: 3.9130 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 323/500\n",
      " - 0s - loss: 3.3213 - acc: 0.3471 - val_loss: 3.9135 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 324/500\n",
      " - 0s - loss: 3.3286 - acc: 0.3461 - val_loss: 3.9135 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 325/500\n",
      " - 0s - loss: 3.3168 - acc: 0.3531 - val_loss: 3.9129 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 326/500\n",
      " - 1s - loss: 3.3228 - acc: 0.3513 - val_loss: 3.9107 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 327/500\n",
      " - 0s - loss: 3.3174 - acc: 0.3471 - val_loss: 3.9110 - val_acc: 0.4716\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 328/500\n",
      " - 1s - loss: 3.3140 - acc: 0.3494 - val_loss: 3.9084 - val_acc: 0.4724\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 329/500\n",
      " - 1s - loss: 3.3146 - acc: 0.3485 - val_loss: 3.9076 - val_acc: 0.4738\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 330/500\n",
      " - 1s - loss: 3.3173 - acc: 0.3472 - val_loss: 3.9107 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 331/500\n",
      " - 1s - loss: 3.3148 - acc: 0.3546 - val_loss: 3.9129 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 332/500\n",
      " - 1s - loss: 3.3110 - acc: 0.3478 - val_loss: 3.9130 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 333/500\n",
      " - 0s - loss: 3.3116 - acc: 0.3471 - val_loss: 3.9136 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 334/500\n",
      " - 0s - loss: 3.3050 - acc: 0.3532 - val_loss: 3.9131 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 335/500\n",
      " - 0s - loss: 3.3134 - acc: 0.3458 - val_loss: 3.9132 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 336/500\n",
      " - 0s - loss: 3.3062 - acc: 0.3489 - val_loss: 3.9138 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 337/500\n",
      " - 0s - loss: 3.3135 - acc: 0.3484 - val_loss: 3.9132 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 338/500\n",
      " - 0s - loss: 3.3105 - acc: 0.3467 - val_loss: 3.9153 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 339/500\n",
      " - 1s - loss: 3.3093 - acc: 0.3488 - val_loss: 3.9141 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 340/500\n",
      " - 0s - loss: 3.3045 - acc: 0.3525 - val_loss: 3.9141 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 341/500\n",
      " - 1s - loss: 3.3141 - acc: 0.3484 - val_loss: 3.9164 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 342/500\n",
      " - 1s - loss: 3.3119 - acc: 0.3524 - val_loss: 3.9164 - val_acc: 0.4695\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 343/500\n",
      " - 1s - loss: 3.3094 - acc: 0.3495 - val_loss: 3.9172 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 344/500\n",
      " - 1s - loss: 3.3127 - acc: 0.3464 - val_loss: 3.9176 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 345/500\n",
      " - 1s - loss: 3.3086 - acc: 0.3517 - val_loss: 3.9192 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 346/500\n",
      " - 1s - loss: 3.3082 - acc: 0.3479 - val_loss: 3.9204 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 347/500\n",
      " - 1s - loss: 3.3047 - acc: 0.3483 - val_loss: 3.9198 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 348/500\n",
      " - 1s - loss: 3.3136 - acc: 0.3493 - val_loss: 3.9194 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 349/500\n",
      " - 0s - loss: 3.2952 - acc: 0.3522 - val_loss: 3.9181 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 350/500\n",
      " - 0s - loss: 3.3033 - acc: 0.3498 - val_loss: 3.9172 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 351/500\n",
      " - 0s - loss: 3.3099 - acc: 0.3520 - val_loss: 3.9162 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 352/500\n",
      " - 0s - loss: 3.3048 - acc: 0.3555 - val_loss: 3.9183 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 353/500\n",
      " - 0s - loss: 3.3054 - acc: 0.3541 - val_loss: 3.9181 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 354/500\n",
      " - 1s - loss: 3.3031 - acc: 0.3463 - val_loss: 3.9202 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 355/500\n",
      " - 1s - loss: 3.3024 - acc: 0.3524 - val_loss: 3.9207 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 356/500\n",
      " - 0s - loss: 3.3045 - acc: 0.3498 - val_loss: 3.9230 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 357/500\n",
      " - 1s - loss: 3.3042 - acc: 0.3544 - val_loss: 3.9200 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 358/500\n",
      " - 1s - loss: 3.3037 - acc: 0.3510 - val_loss: 3.9218 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 359/500\n",
      " - 0s - loss: 3.3006 - acc: 0.3529 - val_loss: 3.9225 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 360/500\n",
      " - 1s - loss: 3.2992 - acc: 0.3531 - val_loss: 3.9233 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 361/500\n",
      " - 0s - loss: 3.2974 - acc: 0.3497 - val_loss: 3.9219 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 362/500\n",
      " - 1s - loss: 3.2964 - acc: 0.3555 - val_loss: 3.9213 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 363/500\n",
      " - 1s - loss: 3.3040 - acc: 0.3492 - val_loss: 3.9218 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 364/500\n",
      " - 1s - loss: 3.2914 - acc: 0.3542 - val_loss: 3.9220 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 365/500\n",
      " - 0s - loss: 3.3061 - acc: 0.3503 - val_loss: 3.9214 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 366/500\n",
      " - 0s - loss: 3.2945 - acc: 0.3490 - val_loss: 3.9200 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 367/500\n",
      " - 0s - loss: 3.3061 - acc: 0.3517 - val_loss: 3.9195 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 368/500\n",
      " - 0s - loss: 3.3042 - acc: 0.3496 - val_loss: 3.9177 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 369/500\n",
      " - 0s - loss: 3.2974 - acc: 0.3490 - val_loss: 3.9192 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 370/500\n",
      " - 1s - loss: 3.3010 - acc: 0.3573 - val_loss: 3.9213 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 371/500\n",
      " - 0s - loss: 3.2922 - acc: 0.3522 - val_loss: 3.9200 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 372/500\n",
      " - 0s - loss: 3.2979 - acc: 0.3547 - val_loss: 3.9211 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 373/500\n",
      " - 1s - loss: 3.2950 - acc: 0.3528 - val_loss: 3.9225 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 374/500\n",
      " - 1s - loss: 3.2989 - acc: 0.3518 - val_loss: 3.9225 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 375/500\n",
      " - 0s - loss: 3.2920 - acc: 0.3528 - val_loss: 3.9228 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 376/500\n",
      " - 1s - loss: 3.2945 - acc: 0.3500 - val_loss: 3.9216 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 377/500\n",
      " - 1s - loss: 3.2904 - acc: 0.3562 - val_loss: 3.9227 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 378/500\n",
      " - 1s - loss: 3.2976 - acc: 0.3522 - val_loss: 3.9245 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 379/500\n",
      " - 0s - loss: 3.2926 - acc: 0.3501 - val_loss: 3.9238 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 380/500\n",
      " - 0s - loss: 3.2877 - acc: 0.3546 - val_loss: 3.9234 - val_acc: 0.4645\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 381/500\n",
      " - 0s - loss: 3.2982 - acc: 0.3469 - val_loss: 3.9234 - val_acc: 0.4652\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 382/500\n",
      " - 0s - loss: 3.2850 - acc: 0.3550 - val_loss: 3.9215 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 383/500\n",
      " - 0s - loss: 3.2949 - acc: 0.3495 - val_loss: 3.9203 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 384/500\n",
      " - 0s - loss: 3.2931 - acc: 0.3447 - val_loss: 3.9211 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 385/500\n",
      " - 0s - loss: 3.2906 - acc: 0.3560 - val_loss: 3.9221 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 386/500\n",
      " - 0s - loss: 3.2878 - acc: 0.3553 - val_loss: 3.9211 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 387/500\n",
      " - 0s - loss: 3.2869 - acc: 0.3550 - val_loss: 3.9218 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 388/500\n",
      " - 1s - loss: 3.2923 - acc: 0.3554 - val_loss: 3.9222 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 389/500\n",
      " - 0s - loss: 3.2909 - acc: 0.3518 - val_loss: 3.9222 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 390/500\n",
      " - 0s - loss: 3.2869 - acc: 0.3513 - val_loss: 3.9221 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 391/500\n",
      " - 0s - loss: 3.2920 - acc: 0.3477 - val_loss: 3.9220 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 392/500\n",
      " - 0s - loss: 3.2893 - acc: 0.3561 - val_loss: 3.9209 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 393/500\n",
      " - 1s - loss: 3.2860 - acc: 0.3518 - val_loss: 3.9222 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 394/500\n",
      " - 1s - loss: 3.2958 - acc: 0.3500 - val_loss: 3.9216 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 395/500\n",
      " - 0s - loss: 3.2912 - acc: 0.3534 - val_loss: 3.9215 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 396/500\n",
      " - 0s - loss: 3.2949 - acc: 0.3483 - val_loss: 3.9230 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 397/500\n",
      " - 0s - loss: 3.2902 - acc: 0.3552 - val_loss: 3.9253 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 398/500\n",
      " - 0s - loss: 3.2930 - acc: 0.3534 - val_loss: 3.9252 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 399/500\n",
      " - 0s - loss: 3.2940 - acc: 0.3498 - val_loss: 3.9243 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 400/500\n",
      " - 0s - loss: 3.2870 - acc: 0.3528 - val_loss: 3.9246 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 401/500\n",
      " - 0s - loss: 3.2967 - acc: 0.3481 - val_loss: 3.9260 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 402/500\n",
      " - 0s - loss: 3.2908 - acc: 0.3532 - val_loss: 3.9235 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 403/500\n",
      " - 0s - loss: 3.2972 - acc: 0.3540 - val_loss: 3.9214 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 404/500\n",
      " - 0s - loss: 3.2946 - acc: 0.3477 - val_loss: 3.9236 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 405/500\n",
      " - 0s - loss: 3.2888 - acc: 0.3557 - val_loss: 3.9238 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 406/500\n",
      " - 1s - loss: 3.2922 - acc: 0.3477 - val_loss: 3.9226 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 407/500\n",
      " - 1s - loss: 3.2874 - acc: 0.3543 - val_loss: 3.9229 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 408/500\n",
      " - 1s - loss: 3.2897 - acc: 0.3560 - val_loss: 3.9240 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 409/500\n",
      " - 1s - loss: 3.2913 - acc: 0.3486 - val_loss: 3.9248 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 410/500\n",
      " - 0s - loss: 3.2828 - acc: 0.3567 - val_loss: 3.9243 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 411/500\n",
      " - 1s - loss: 3.2833 - acc: 0.3548 - val_loss: 3.9250 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 412/500\n",
      " - 1s - loss: 3.2927 - acc: 0.3554 - val_loss: 3.9258 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 413/500\n",
      " - 1s - loss: 3.2900 - acc: 0.3510 - val_loss: 3.9220 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 414/500\n",
      " - 1s - loss: 3.2808 - acc: 0.3543 - val_loss: 3.9234 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 415/500\n",
      " - 0s - loss: 3.2810 - acc: 0.3533 - val_loss: 3.9236 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 416/500\n",
      " - 0s - loss: 3.2956 - acc: 0.3506 - val_loss: 3.9250 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 417/500\n",
      " - 0s - loss: 3.2892 - acc: 0.3508 - val_loss: 3.9231 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 418/500\n",
      " - 1s - loss: 3.2824 - acc: 0.3595 - val_loss: 3.9209 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 419/500\n",
      " - 0s - loss: 3.2879 - acc: 0.3495 - val_loss: 3.9206 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 420/500\n",
      " - 1s - loss: 3.2812 - acc: 0.3532 - val_loss: 3.9213 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 421/500\n",
      " - 0s - loss: 3.2762 - acc: 0.3528 - val_loss: 3.9216 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 422/500\n",
      " - 0s - loss: 3.2877 - acc: 0.3475 - val_loss: 3.9202 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 423/500\n",
      " - 0s - loss: 3.2773 - acc: 0.3541 - val_loss: 3.9214 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 424/500\n",
      " - 1s - loss: 3.2812 - acc: 0.3568 - val_loss: 3.9217 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 425/500\n",
      " - 0s - loss: 3.2805 - acc: 0.3528 - val_loss: 3.9216 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 426/500\n",
      " - 0s - loss: 3.2806 - acc: 0.3516 - val_loss: 3.9227 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 427/500\n",
      " - 0s - loss: 3.2906 - acc: 0.3526 - val_loss: 3.9235 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 428/500\n",
      " - 0s - loss: 3.2765 - acc: 0.3577 - val_loss: 3.9245 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 429/500\n",
      " - 0s - loss: 3.2813 - acc: 0.3548 - val_loss: 3.9247 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 430/500\n",
      " - 0s - loss: 3.2817 - acc: 0.3602 - val_loss: 3.9256 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 431/500\n",
      " - 0s - loss: 3.2839 - acc: 0.3535 - val_loss: 3.9245 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 432/500\n",
      " - 0s - loss: 3.2749 - acc: 0.3608 - val_loss: 3.9272 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 433/500\n",
      " - 0s - loss: 3.2907 - acc: 0.3489 - val_loss: 3.9278 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 434/500\n",
      " - 0s - loss: 3.2821 - acc: 0.3511 - val_loss: 3.9270 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 435/500\n",
      " - 1s - loss: 3.2880 - acc: 0.3541 - val_loss: 3.9280 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 436/500\n",
      " - 1s - loss: 3.2794 - acc: 0.3522 - val_loss: 3.9255 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 437/500\n",
      " - 1s - loss: 3.2849 - acc: 0.3537 - val_loss: 3.9274 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 438/500\n",
      " - 1s - loss: 3.2785 - acc: 0.3581 - val_loss: 3.9275 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 439/500\n",
      " - 1s - loss: 3.2802 - acc: 0.3591 - val_loss: 3.9272 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 440/500\n",
      " - 0s - loss: 3.2774 - acc: 0.3610 - val_loss: 3.9279 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 441/500\n",
      " - 0s - loss: 3.2733 - acc: 0.3561 - val_loss: 3.9289 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 442/500\n",
      " - 0s - loss: 3.2777 - acc: 0.3532 - val_loss: 3.9281 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 443/500\n",
      " - 1s - loss: 3.2872 - acc: 0.3516 - val_loss: 3.9280 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 444/500\n",
      " - 1s - loss: 3.2819 - acc: 0.3536 - val_loss: 3.9301 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 445/500\n",
      " - 1s - loss: 3.2781 - acc: 0.3568 - val_loss: 3.9308 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 446/500\n",
      " - 1s - loss: 3.2804 - acc: 0.3502 - val_loss: 3.9297 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 447/500\n",
      " - 0s - loss: 3.2782 - acc: 0.3580 - val_loss: 3.9298 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 448/500\n",
      " - 0s - loss: 3.2766 - acc: 0.3556 - val_loss: 3.9287 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 449/500\n",
      " - 0s - loss: 3.2803 - acc: 0.3503 - val_loss: 3.9296 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 450/500\n",
      " - 0s - loss: 3.2728 - acc: 0.3577 - val_loss: 3.9298 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 451/500\n",
      " - 0s - loss: 3.2717 - acc: 0.3505 - val_loss: 3.9295 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 452/500\n",
      " - 0s - loss: 3.2764 - acc: 0.3576 - val_loss: 3.9291 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 453/500\n",
      " - 0s - loss: 3.2706 - acc: 0.3552 - val_loss: 3.9299 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 454/500\n",
      " - 0s - loss: 3.2771 - acc: 0.3536 - val_loss: 3.9270 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 455/500\n",
      " - 0s - loss: 3.2792 - acc: 0.3572 - val_loss: 3.9299 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 456/500\n",
      " - 1s - loss: 3.2787 - acc: 0.3532 - val_loss: 3.9319 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 457/500\n",
      " - 1s - loss: 3.2769 - acc: 0.3553 - val_loss: 3.9307 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 458/500\n",
      " - 0s - loss: 3.2731 - acc: 0.3599 - val_loss: 3.9310 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 459/500\n",
      " - 0s - loss: 3.2772 - acc: 0.3539 - val_loss: 3.9297 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 460/500\n",
      " - 1s - loss: 3.2757 - acc: 0.3565 - val_loss: 3.9289 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 461/500\n",
      " - 1s - loss: 3.2725 - acc: 0.3579 - val_loss: 3.9292 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 462/500\n",
      " - 1s - loss: 3.2742 - acc: 0.3561 - val_loss: 3.9282 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 463/500\n",
      " - 0s - loss: 3.2760 - acc: 0.3590 - val_loss: 3.9293 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 464/500\n",
      " - 0s - loss: 3.2797 - acc: 0.3549 - val_loss: 3.9294 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 465/500\n",
      " - 1s - loss: 3.2791 - acc: 0.3530 - val_loss: 3.9304 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 466/500\n",
      " - 1s - loss: 3.2786 - acc: 0.3559 - val_loss: 3.9314 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 467/500\n",
      " - 0s - loss: 3.2759 - acc: 0.3528 - val_loss: 3.9315 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 468/500\n",
      " - 0s - loss: 3.2746 - acc: 0.3578 - val_loss: 3.9297 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 469/500\n",
      " - 0s - loss: 3.2751 - acc: 0.3565 - val_loss: 3.9288 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 470/500\n",
      " - 0s - loss: 3.2754 - acc: 0.3552 - val_loss: 3.9282 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 471/500\n",
      " - 1s - loss: 3.2754 - acc: 0.3532 - val_loss: 3.9279 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 472/500\n",
      " - 1s - loss: 3.2744 - acc: 0.3558 - val_loss: 3.9281 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 473/500\n",
      " - 0s - loss: 3.2686 - acc: 0.3590 - val_loss: 3.9280 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 474/500\n",
      " - 1s - loss: 3.2745 - acc: 0.3534 - val_loss: 3.9263 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 475/500\n",
      " - 1s - loss: 3.2730 - acc: 0.3518 - val_loss: 3.9257 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 476/500\n",
      " - 0s - loss: 3.2731 - acc: 0.3542 - val_loss: 3.9270 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 477/500\n",
      " - 0s - loss: 3.2799 - acc: 0.3536 - val_loss: 3.9275 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 478/500\n",
      " - 0s - loss: 3.2722 - acc: 0.3530 - val_loss: 3.9300 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 479/500\n",
      " - 0s - loss: 3.2752 - acc: 0.3510 - val_loss: 3.9279 - val_acc: 0.4659\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 480/500\n",
      " - 1s - loss: 3.2713 - acc: 0.3556 - val_loss: 3.9289 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 481/500\n",
      " - 1s - loss: 3.2664 - acc: 0.3561 - val_loss: 3.9287 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 482/500\n",
      " - 0s - loss: 3.2732 - acc: 0.3564 - val_loss: 3.9291 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 483/500\n",
      " - 0s - loss: 3.2800 - acc: 0.3513 - val_loss: 3.9306 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 484/500\n",
      " - 1s - loss: 3.2789 - acc: 0.3524 - val_loss: 3.9306 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 485/500\n",
      " - 1s - loss: 3.2729 - acc: 0.3531 - val_loss: 3.9283 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 486/500\n",
      " - 1s - loss: 3.2723 - acc: 0.3524 - val_loss: 3.9294 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 487/500\n",
      " - 0s - loss: 3.2720 - acc: 0.3592 - val_loss: 3.9283 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 488/500\n",
      " - 1s - loss: 3.2708 - acc: 0.3571 - val_loss: 3.9275 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 489/500\n",
      " - 0s - loss: 3.2725 - acc: 0.3581 - val_loss: 3.9276 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 490/500\n",
      " - 0s - loss: 3.2753 - acc: 0.3531 - val_loss: 3.9273 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 491/500\n",
      " - 1s - loss: 3.2747 - acc: 0.3530 - val_loss: 3.9269 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 492/500\n",
      " - 1s - loss: 3.2756 - acc: 0.3609 - val_loss: 3.9282 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 493/500\n",
      " - 1s - loss: 3.2703 - acc: 0.3561 - val_loss: 3.9285 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 494/500\n",
      " - 1s - loss: 3.2737 - acc: 0.3596 - val_loss: 3.9308 - val_acc: 0.4681\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 495/500\n",
      " - 1s - loss: 3.2759 - acc: 0.3515 - val_loss: 3.9300 - val_acc: 0.4688\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 496/500\n",
      " - 0s - loss: 3.2671 - acc: 0.3546 - val_loss: 3.9304 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 497/500\n",
      " - 0s - loss: 3.2656 - acc: 0.3607 - val_loss: 3.9313 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 498/500\n",
      " - 1s - loss: 3.2701 - acc: 0.3574 - val_loss: 3.9301 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n",
      "Epoch 499/500\n",
      " - 1s - loss: 3.2692 - acc: 0.3518 - val_loss: 3.9309 - val_acc: 0.4666\n",
      "\n",
      "Balanced Accuracy - train: 0.996 \t val: 0.463\n",
      "Epoch 500/500\n",
      " - 1s - loss: 3.2710 - acc: 0.3539 - val_loss: 3.9290 - val_acc: 0.4673\n",
      "\n",
      "Balanced Accuracy - train: 0.997 \t val: 0.463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7898967be0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, kernel_initializer=\"uniform\", input_shape = (32,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "adam = Adam(lr = 0.00001)\n",
    "y_train_one_hot = one_hot(y_train, 3)\n",
    "y_test_one_hot = one_hot(y_test, 3)\n",
    "\n",
    "model.compile(loss=w_cat_crossentropy, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(encoder_X_train, y_train_one_hot, epochs=500, batch_size=128, validation_data=(encoder_X_test, y_test_one_hot), \n",
    "          verbose=2, shuffle=True, callbacks=CALLBACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[   0    0    0]\n",
      " [   0    0    0]\n",
      " [1368    3   22]]\n",
      "Normalized confusion matrix\n",
      "[[  nan   nan   nan]\n",
      " [  nan   nan   nan]\n",
      " [ 0.98  0.    0.02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:12: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEYCAYAAAA3cc++AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f788d53d128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEYCAYAAAApuP8NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f788d3e6a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = model.predict(encoder_X_test)\n",
    "\n",
    "# decode one-hot to single labels\n",
    "preds = [ np.argmax(pred, axis = 0) for pred in preds ]\n",
    "labels = [ np.argmax(label, axis = 0) for label in y_test_one_hot ]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(labels, preds)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"non-frail\", \"pre-frail\", \"frail\"],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"non-frail\", \"pre-frail\", \"frail\"], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
