{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nikos/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, UpSampling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the random seeds\n",
    "random.seed(1)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "\n",
    "# Description: This function is responsible for loading our data.\n",
    "# Args in: filepath - the path of the .mat file containing the data\n",
    "# Returns: inputs - the data, labels - the labels corresponding to the data, \n",
    "#            patients - the patients corresponding to the data\n",
    "\n",
    "    mat = scipy.io.loadmat(filepath)\n",
    "    inputs = mat['Xrec'][:]\n",
    "    labels = mat['Y']\n",
    "    patients = mat['patientID']\n",
    "\n",
    "    labels = np.einsum('ij->ji', labels)\n",
    "    labels = [label for sublist in labels for label in sublist]\n",
    "    patients = np.einsum('ij->ji', patients)\n",
    "    patients = [patient for sublist in patients for patient in sublist]\n",
    "\n",
    "    return inputs, labels, patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_non_frail(inputs, labels, patients):\n",
    "\n",
    "#Description: This function allows us to access specifically the data corresponding to the patients\n",
    "#that are labeled as being not frail.\n",
    "#Args in & Returns are self-explanatory.\n",
    "    \n",
    "    first_pre_frail_index = labels.index(1)\n",
    "    last_pre_frail_index = len(labels) - 1 - labels[::-1].index(1)\n",
    "    \n",
    "    inputs = np.asarray(list(inputs[:first_pre_frail_index]))\n",
    "    labels = np.asarray(list(labels[:first_pre_frail_index]))\n",
    "    patients = np.asarray(list(patients[:first_pre_frail_index]))\n",
    "    \n",
    "    assert(np.unique(labels) == [0])\n",
    "    \n",
    "    return inputs, labels, patients\n",
    "\n",
    "def get_pre_frail(inputs, labels, patients):\n",
    "\n",
    "# Description: Symmetric to the function above.\n",
    "# Args in & Returns are self-explanatory.\n",
    "    \n",
    "    first_pre_frail_index = labels.index(1)\n",
    "    last_pre_frail_index = len(labels) - 1 - labels[::-1].index(1)\n",
    "    \n",
    "    inputs = np.asarray(list(inputs[first_pre_frail_index+1:last_pre_frail_index]))\n",
    "    labels = np.asarray(list(labels[first_pre_frail_index+1:last_pre_frail_index]))\n",
    "    patients = np.asarray(list(patients[first_pre_frail_index+1:last_pre_frail_index]))\n",
    "    \n",
    "    assert(np.unique(labels) == [1])\n",
    "    \n",
    "    return inputs, labels, patients\n",
    "\n",
    "def get_frail(inputs, labels, patients):\n",
    " \n",
    "# Description: Symmetric to the function above.\n",
    "# Args in & Returns are self-explanatory.\n",
    "    \n",
    "    first_pre_frail_index = labels.index(1)\n",
    "    last_pre_frail_index = len(labels) - 1 - labels[::-1].index(1)\n",
    "    \n",
    "    inputs = np.asarray(list(inputs[last_pre_frail_index+1:]))\n",
    "    labels = np.asarray(list(labels[last_pre_frail_index+1:]))\n",
    "    patients = np.asarray(list(patients[last_pre_frail_index+1:]))\n",
    "    \n",
    "    assert(np.unique(labels) == [2])\n",
    "    \n",
    "    return inputs, labels, patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_duplicates(duplicates_list, patients):\n",
    "\n",
    "# Description: This function is responsible for filtering out\n",
    "# some patients that are found to be present in more than one classes.\n",
    "# Args in & Returns are self-explanatory.\n",
    "    \n",
    "    patients = list(patients)\n",
    "    \n",
    "    for duplicate in duplicates_list:\n",
    "        patients = list(filter(lambda a: a != duplicate, patients))\n",
    "    \n",
    "    return np.asarray(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, labels, patients = load_data('/home/nikos/Desktop/Zacharaki/PARAFAC missing values 0_90/ReconstructedTensorAndFeatures90Missing_StrSGD.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patients = filter_duplicates([1002, 1104], patients)\n",
    "non_frail_inputs, non_frail_labels, non_frail_patients = get_non_frail(inputs, labels, patients)\n",
    "pre_frail_inputs, pre_frail_labels, pre_frail_patients = get_pre_frail(inputs, labels, patients)\n",
    "frail_inputs, frail_labels, frail_patients = get_frail(inputs, labels, patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we hereby start by loading the data and break them in non_frail, pre_frail and frail containers for future use. We are going to need the separation in order to build three autoencoder models, one corresponding to each of the classes in our data. We will then split our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test(inputs, targets, patients, patients_for_val):\n",
    "\n",
    "# Description: This function is responsible for splitting our data into training and test data\n",
    "# Args in: patients_for_val - list containing the patients for our test data\n",
    "# Returns: the data and labels of our training and test data\n",
    "\n",
    "    X_train_size = X_val_size = 0\n",
    "\n",
    "    patients_for_train = [item for item in list(np.unique(patients)) if item not in patients_for_val]\n",
    "    idpatients_val = []\n",
    "    for patient in patients_for_val:\n",
    "        idpatient = [i for i, x in enumerate(patients) if x == patient]\n",
    "        idpatients_val.append(idpatient)\n",
    "    idpatients_val = [item for sublist in idpatients_val for item in sublist]\n",
    "    X_val = [inputs[i] for i in idpatients_val]\n",
    "    y_val = [targets[i] for i in idpatients_val]\n",
    "\n",
    "    idpatients_train = list(set([i for i in range(inputs.shape[0])]) - set(idpatients_val))\n",
    "    inputs = [inputs[i] for i in idpatients_train]\n",
    "    targets = [targets[i] for i in idpatients_train]\n",
    "\n",
    "    return np.asarray(inputs), np.asarray(targets), np.asarray(X_val), np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(645, 1500, 7) (13, 1500, 7) (399, 1500, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train_non_frail, y_train_non_frail, X_test_non_frail, y_test_non_frail = split_train_test(non_frail_inputs, \n",
    "                                                                                            non_frail_labels, \n",
    "                                                                                            non_frail_patients, \n",
    "                                                                                            [1106])\n",
    "X_train_pre_frail, y_train_pre_frail, X_test_pre_frail, y_test_pre_frail = split_train_test(pre_frail_inputs, \n",
    "                                                                                            pre_frail_labels, \n",
    "                                                                                            pre_frail_patients, \n",
    "                                                                                            [1107])\n",
    "X_train_frail, y_train_frail, X_test_frail, y_test_frail = split_train_test(frail_inputs, \n",
    "                                                                            frail_labels, \n",
    "                                                                            frail_patients, \n",
    "                                                                            [2097])\n",
    "\n",
    "print(X_test_non_frail.shape, X_test_pre_frail.shape, X_test_frail.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "\n",
    "# Description: This function preprocesses our data. \n",
    "# We want to ensure that our training data have zero mean and unit variance. \n",
    "# We also use subtract the same mean from the test data and then devide them by\n",
    "# the same standard deviation. We do that to ensure that no information about the\n",
    "# test set distribution is known ahead of time.\n",
    "\n",
    "# Args in & Returns are self-explanatory.\n",
    "\n",
    "    X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis = 0)\n",
    "    \n",
    "    X_train, X_val, train_ground, valid_ground = train_test_split(X_train, X_train, test_size = 0.1, random_state = 13)\n",
    "\n",
    "    X_test = (X_test -  np.mean(X_train, axis=0)) / np.std(X_train, axis = 0)\n",
    "    \n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1] * X_val.shape[2]))\n",
    "    train_ground = np.reshape(train_ground, (train_ground.shape[0], train_ground.shape[1] * train_ground.shape[2]))\n",
    "    valid_ground = np.reshape(valid_ground, (valid_ground.shape[0], valid_ground.shape[1] * valid_ground.shape[2]))\n",
    "    \n",
    "    return X_train, X_val, train_ground, valid_ground, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_non_frail, X_val_non_frail, train_ground_non_frail, valid_ground_non_frail, X_test_non_frail = preprocess_data(X_train_non_frail, X_test_non_frail)\n",
    "X_train_pre_frail, X_val_pre_frail, train_ground_pre_frail, valid_ground_pre_frail, X_test_pre_frail = preprocess_data(X_train_pre_frail, X_test_pre_frail)\n",
    "X_train_frail, X_val_frail, train_ground_frail, valid_ground_frail, X_test_frail = preprocess_data(X_train_frail, X_test_frail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define our models. Each model will be an autoencoder trained with the training instances of a single class, so that we can end up with three autoencoders, each one building a (hopefully) useful representation of the class, the instances of which it was trained with. Since we want to use these models for feature extraction and, at the same time, dimensionality reduction we're going to use the intermediate layer (the encoder) to get features vectors that are way smaller in size than the raw vectors we started with. Hopefully, the compressed information these feature vectors contain will be (almost) equally valuable to the information encapsulated in the original data.\n",
    "\n",
    "Our autoencoder model will be the simplest possible, with just one hidden layer containing $32$ neurons acting as the encoder. This means that the model is going to learn 2 things: a function that is able to map our input of shape $1500$ (number-of-time-steps) * $7$ (number-of-channels) = $10500$ to a latent space of shape 32 and then a function which maps this latent space into the original space again, thus trying to recreate the original data. The best way to understand how the autoencoder works is thinking of a bottleneck which enforces our model to try and keep just the essential parts of the data needed for their recreation.\n",
    "\n",
    "We're going to train each model using the Adam optimizer with a learning rate of $0.0001$ for $30$ epochs, using a batch size of $128$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10500)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                336032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10500)             346500    \n",
      "=================================================================\n",
      "Total params: 682,532\n",
      "Trainable params: 682,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6106 samples, validate on 679 samples\n",
      "Epoch 1/30\n",
      " - 2s - loss: 0.8537 - mean_squared_error: 0.8537 - val_loss: 0.7344 - val_mean_squared_error: 0.7344\n",
      "Epoch 2/30\n",
      " - 1s - loss: 0.7066 - mean_squared_error: 0.7066 - val_loss: 0.6993 - val_mean_squared_error: 0.6993\n",
      "Epoch 3/30\n",
      " - 1s - loss: 0.6877 - mean_squared_error: 0.6877 - val_loss: 0.6902 - val_mean_squared_error: 0.6902\n",
      "Epoch 4/30\n",
      " - 1s - loss: 0.6819 - mean_squared_error: 0.6819 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 5/30\n",
      " - 1s - loss: 0.6800 - mean_squared_error: 0.6800 - val_loss: 0.6847 - val_mean_squared_error: 0.6847\n",
      "Epoch 6/30\n",
      " - 1s - loss: 0.6777 - mean_squared_error: 0.6777 - val_loss: 0.6832 - val_mean_squared_error: 0.6832\n",
      "Epoch 7/30\n",
      " - 1s - loss: 0.6765 - mean_squared_error: 0.6765 - val_loss: 0.6826 - val_mean_squared_error: 0.6826\n",
      "Epoch 8/30\n",
      " - 1s - loss: 0.6758 - mean_squared_error: 0.6758 - val_loss: 0.6818 - val_mean_squared_error: 0.6818\n",
      "Epoch 9/30\n",
      " - 1s - loss: 0.6752 - mean_squared_error: 0.6752 - val_loss: 0.6808 - val_mean_squared_error: 0.6808\n",
      "Epoch 10/30\n",
      " - 1s - loss: 0.6748 - mean_squared_error: 0.6748 - val_loss: 0.6809 - val_mean_squared_error: 0.6809\n",
      "Epoch 11/30\n",
      " - 1s - loss: 0.6747 - mean_squared_error: 0.6747 - val_loss: 0.6811 - val_mean_squared_error: 0.6811\n",
      "Epoch 12/30\n",
      " - 1s - loss: 0.6741 - mean_squared_error: 0.6741 - val_loss: 0.6810 - val_mean_squared_error: 0.6810\n",
      "Epoch 13/30\n",
      " - 1s - loss: 0.6730 - mean_squared_error: 0.6730 - val_loss: 0.6790 - val_mean_squared_error: 0.6790\n",
      "Epoch 14/30\n",
      " - 1s - loss: 0.6721 - mean_squared_error: 0.6721 - val_loss: 0.6787 - val_mean_squared_error: 0.6787\n",
      "Epoch 15/30\n",
      " - 1s - loss: 0.6716 - mean_squared_error: 0.6716 - val_loss: 0.6782 - val_mean_squared_error: 0.6782\n",
      "Epoch 16/30\n",
      " - 1s - loss: 0.6714 - mean_squared_error: 0.6714 - val_loss: 0.6775 - val_mean_squared_error: 0.6775\n",
      "Epoch 17/30\n",
      " - 1s - loss: 0.6709 - mean_squared_error: 0.6709 - val_loss: 0.6782 - val_mean_squared_error: 0.6782\n",
      "Epoch 18/30\n",
      " - 1s - loss: 0.6706 - mean_squared_error: 0.6706 - val_loss: 0.6767 - val_mean_squared_error: 0.6767\n",
      "Epoch 19/30\n",
      " - 1s - loss: 0.6700 - mean_squared_error: 0.6700 - val_loss: 0.6773 - val_mean_squared_error: 0.6773\n",
      "Epoch 20/30\n",
      " - 1s - loss: 0.6698 - mean_squared_error: 0.6698 - val_loss: 0.6764 - val_mean_squared_error: 0.6764\n",
      "Epoch 21/30\n",
      " - 1s - loss: 0.6690 - mean_squared_error: 0.6690 - val_loss: 0.6760 - val_mean_squared_error: 0.6760\n",
      "Epoch 22/30\n",
      " - 1s - loss: 0.6687 - mean_squared_error: 0.6687 - val_loss: 0.6750 - val_mean_squared_error: 0.6750\n",
      "Epoch 23/30\n",
      " - 1s - loss: 0.6680 - mean_squared_error: 0.6680 - val_loss: 0.6754 - val_mean_squared_error: 0.6754\n",
      "Epoch 24/30\n",
      " - 1s - loss: 0.6677 - mean_squared_error: 0.6677 - val_loss: 0.6735 - val_mean_squared_error: 0.6735\n",
      "Epoch 25/30\n",
      " - 1s - loss: 0.6669 - mean_squared_error: 0.6669 - val_loss: 0.6737 - val_mean_squared_error: 0.6737\n",
      "Epoch 26/30\n",
      " - 1s - loss: 0.6670 - mean_squared_error: 0.6670 - val_loss: 0.6742 - val_mean_squared_error: 0.6742\n",
      "Epoch 27/30\n",
      " - 1s - loss: 0.6668 - mean_squared_error: 0.6668 - val_loss: 0.6729 - val_mean_squared_error: 0.6729\n",
      "Epoch 28/30\n",
      " - 1s - loss: 0.6659 - mean_squared_error: 0.6659 - val_loss: 0.6725 - val_mean_squared_error: 0.6725\n",
      "Epoch 29/30\n",
      " - 1s - loss: 0.6657 - mean_squared_error: 0.6657 - val_loss: 0.6720 - val_mean_squared_error: 0.6720\n",
      "Epoch 30/30\n",
      " - 1s - loss: 0.6656 - mean_squared_error: 0.6656 - val_loss: 0.6733 - val_mean_squared_error: 0.6733\n"
     ]
    }
   ],
   "source": [
    "input_signal = Input(shape = (X_train_non_frail.shape[1],))\n",
    "encoded_non_frail = Dense(32, activation = 'relu')(input_signal)\n",
    "\n",
    "# decoder\n",
    "decoded_non_frail = Dense(1500*7, activation = 'sigmoid')(encoded_non_frail)\n",
    "\n",
    "autoencoder_non_frail = Model(input = input_signal, output = decoded_non_frail)\n",
    "\n",
    "non_frail_encoder = Model(input = input_signal, output = encoded_non_frail)\n",
    "non_frail_encoded_input = Input(shape=(32,))\n",
    "non_frail_decoder_layer = autoencoder_non_frail.layers[-1]\n",
    "non_frail_decoder = Model(non_frail_encoded_input, non_frail_decoder_layer(non_frail_encoded_input))\n",
    "\n",
    "autoencoder_non_frail.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=['mse'])\n",
    "print(autoencoder_non_frail.summary())\n",
    "history = autoencoder_non_frail.fit(X_train_non_frail, train_ground_non_frail, epochs=30, batch_size=128, \n",
    "                                    validation_data=(X_val_non_frail, valid_ground_non_frail), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we're just uniting the feature vectors for the training set, extracted by just our encoder layer.\n",
    "encoder_non_frail_X_train = [non_frail_encoder.predict(X_train_non_frail), non_frail_encoder.predict(X_val_non_frail),\n",
    "                             non_frail_encoder.predict(X_train_pre_frail), non_frail_encoder.predict(X_val_pre_frail),\n",
    "                             non_frail_encoder.predict(X_train_frail), non_frail_encoder.predict(X_val_frail),\n",
    "                            ]\n",
    "encoder_non_frail_X_train = [i for sublist in encoder_non_frail_X_train for i in sublist]\n",
    "encoder_non_frail_X_test = [non_frail_encoder.predict(X_test_non_frail),\n",
    "                            non_frail_encoder.predict(X_test_pre_frail),\n",
    "                            non_frail_encoder.predict(X_test_frail)\n",
    "                           ]\n",
    "encoder_non_frail_X_test = [i for sublist in encoder_non_frail_X_test for i in sublist]\n",
    "\n",
    "encoder_non_frail_X_train = np.asarray(encoder_non_frail_X_train)\n",
    "encoder_non_frail_X_test = np.asarray(encoder_non_frail_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10500)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                336032    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10500)             346500    \n",
      "=================================================================\n",
      "Total params: 682,532\n",
      "Trainable params: 682,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8071 samples, validate on 897 samples\n",
      "Epoch 1/30\n",
      " - 1s - loss: 0.7236 - mean_squared_error: 0.7236 - val_loss: 0.6719 - val_mean_squared_error: 0.6719\n",
      "Epoch 2/30\n",
      " - 1s - loss: 0.6618 - mean_squared_error: 0.6618 - val_loss: 0.6600 - val_mean_squared_error: 0.6600\n",
      "Epoch 3/30\n",
      " - 1s - loss: 0.6548 - mean_squared_error: 0.6548 - val_loss: 0.6565 - val_mean_squared_error: 0.6565\n",
      "Epoch 4/30\n",
      " - 1s - loss: 0.6502 - mean_squared_error: 0.6502 - val_loss: 0.6522 - val_mean_squared_error: 0.6522\n",
      "Epoch 5/30\n",
      " - 1s - loss: 0.6449 - mean_squared_error: 0.6449 - val_loss: 0.6469 - val_mean_squared_error: 0.6469\n",
      "Epoch 6/30\n",
      " - 1s - loss: 0.6416 - mean_squared_error: 0.6416 - val_loss: 0.6433 - val_mean_squared_error: 0.6433\n",
      "Epoch 7/30\n",
      " - 1s - loss: 0.6355 - mean_squared_error: 0.6355 - val_loss: 0.6403 - val_mean_squared_error: 0.6403\n",
      "Epoch 8/30\n",
      " - 1s - loss: 0.6336 - mean_squared_error: 0.6336 - val_loss: 0.6370 - val_mean_squared_error: 0.6370\n",
      "Epoch 9/30\n",
      " - 1s - loss: 0.6311 - mean_squared_error: 0.6311 - val_loss: 0.6341 - val_mean_squared_error: 0.6341\n",
      "Epoch 10/30\n",
      " - 1s - loss: 0.6293 - mean_squared_error: 0.6293 - val_loss: 0.6339 - val_mean_squared_error: 0.6339\n",
      "Epoch 11/30\n",
      " - 1s - loss: 0.6278 - mean_squared_error: 0.6278 - val_loss: 0.6322 - val_mean_squared_error: 0.6322\n",
      "Epoch 12/30\n",
      " - 1s - loss: 0.6274 - mean_squared_error: 0.6274 - val_loss: 0.6316 - val_mean_squared_error: 0.6316\n",
      "Epoch 13/30\n",
      " - 1s - loss: 0.6265 - mean_squared_error: 0.6265 - val_loss: 0.6309 - val_mean_squared_error: 0.6309\n",
      "Epoch 14/30\n",
      " - 1s - loss: 0.6241 - mean_squared_error: 0.6241 - val_loss: 0.6304 - val_mean_squared_error: 0.6304\n",
      "Epoch 15/30\n",
      " - 1s - loss: 0.6236 - mean_squared_error: 0.6236 - val_loss: 0.6299 - val_mean_squared_error: 0.6299\n",
      "Epoch 16/30\n",
      " - 1s - loss: 0.6235 - mean_squared_error: 0.6235 - val_loss: 0.6295 - val_mean_squared_error: 0.6295\n",
      "Epoch 17/30\n",
      " - 1s - loss: 0.6223 - mean_squared_error: 0.6223 - val_loss: 0.6297 - val_mean_squared_error: 0.6297\n",
      "Epoch 18/30\n",
      " - 1s - loss: 0.6234 - mean_squared_error: 0.6234 - val_loss: 0.6271 - val_mean_squared_error: 0.6271\n",
      "Epoch 19/30\n",
      " - 1s - loss: 0.6208 - mean_squared_error: 0.6208 - val_loss: 0.6292 - val_mean_squared_error: 0.6292\n",
      "Epoch 20/30\n",
      " - 1s - loss: 0.6259 - mean_squared_error: 0.6259 - val_loss: 0.6275 - val_mean_squared_error: 0.6275\n",
      "Epoch 21/30\n",
      " - 1s - loss: 0.6208 - mean_squared_error: 0.6208 - val_loss: 0.6274 - val_mean_squared_error: 0.6274\n",
      "Epoch 22/30\n",
      " - 1s - loss: 0.6200 - mean_squared_error: 0.6200 - val_loss: 0.6253 - val_mean_squared_error: 0.6253\n",
      "Epoch 23/30\n",
      " - 1s - loss: 0.6193 - mean_squared_error: 0.6193 - val_loss: 0.6254 - val_mean_squared_error: 0.6254\n",
      "Epoch 24/30\n",
      " - 1s - loss: 0.6192 - mean_squared_error: 0.6192 - val_loss: 0.6250 - val_mean_squared_error: 0.6250\n",
      "Epoch 25/30\n",
      " - 1s - loss: 0.6186 - mean_squared_error: 0.6186 - val_loss: 0.6248 - val_mean_squared_error: 0.6248\n",
      "Epoch 26/30\n",
      " - 1s - loss: 0.6185 - mean_squared_error: 0.6185 - val_loss: 0.6243 - val_mean_squared_error: 0.6243\n",
      "Epoch 27/30\n",
      " - 1s - loss: 0.6183 - mean_squared_error: 0.6183 - val_loss: 0.6241 - val_mean_squared_error: 0.6241\n",
      "Epoch 28/30\n",
      " - 1s - loss: 0.6177 - mean_squared_error: 0.6177 - val_loss: 0.6238 - val_mean_squared_error: 0.6238\n",
      "Epoch 29/30\n",
      " - 1s - loss: 0.6175 - mean_squared_error: 0.6175 - val_loss: 0.6233 - val_mean_squared_error: 0.6233\n",
      "Epoch 30/30\n",
      " - 1s - loss: 0.6170 - mean_squared_error: 0.6170 - val_loss: 0.6237 - val_mean_squared_error: 0.6237\n"
     ]
    }
   ],
   "source": [
    "input_signal = Input(shape = (X_train_pre_frail.shape[1],))\n",
    "encoded_pre_frail = Dense(32, activation = 'relu')(input_signal)\n",
    "\n",
    "# decoder\n",
    "decoded_pre_frail = Dense(1500*7, activation = 'sigmoid')(encoded_pre_frail)\n",
    "\n",
    "autoencoder_pre_frail = Model(input = input_signal, output = decoded_pre_frail)\n",
    "\n",
    "pre_frail_encoder = Model(input = input_signal, output = encoded_pre_frail)\n",
    "pre_frail_encoded_input = Input(shape=(32,))\n",
    "pre_frail_decoder_layer = autoencoder_pre_frail.layers[-1]\n",
    "pre_frail_decoder = Model(pre_frail_encoded_input, pre_frail_decoder_layer(pre_frail_encoded_input))\n",
    "\n",
    "autoencoder_pre_frail.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=['mse'])\n",
    "print(autoencoder_pre_frail.summary())\n",
    "history = autoencoder_pre_frail.fit(X_train_pre_frail, train_ground_pre_frail, epochs=30, batch_size=128, \n",
    "                                    validation_data=(X_val_pre_frail, valid_ground_pre_frail), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_pre_frail_X_train = [pre_frail_encoder.predict(X_train_non_frail), pre_frail_encoder.predict(X_val_non_frail),\n",
    "                             pre_frail_encoder.predict(X_train_pre_frail), pre_frail_encoder.predict(X_val_pre_frail),\n",
    "                             pre_frail_encoder.predict(X_train_frail), pre_frail_encoder.predict(X_val_frail),\n",
    "                            ]\n",
    "encoder_pre_frail_X_train = [i for sublist in encoder_pre_frail_X_train for i in sublist]\n",
    "encoder_pre_frail_X_test = [pre_frail_encoder.predict(X_test_non_frail),\n",
    "                            pre_frail_encoder.predict(X_test_pre_frail),\n",
    "                            pre_frail_encoder.predict(X_test_frail)\n",
    "                           ]\n",
    "encoder_pre_frail_X_test = [i for sublist in encoder_pre_frail_X_test for i in sublist]\n",
    "\n",
    "encoder_pre_frail_X_train = np.asarray(encoder_pre_frail_X_train)\n",
    "encoder_pre_frail_X_test = np.asarray(encoder_pre_frail_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/home/nikos/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 10500)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                336032    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10500)             346500    \n",
      "=================================================================\n",
      "Total params: 682,532\n",
      "Trainable params: 682,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2635 samples, validate on 293 samples\n",
      "Epoch 1/30\n",
      " - 0s - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 0.6099 - val_mean_squared_error: 0.6099\n",
      "Epoch 2/30\n",
      " - 0s - loss: 0.7269 - mean_squared_error: 0.7269 - val_loss: 0.5918 - val_mean_squared_error: 0.5918\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.7138 - mean_squared_error: 0.7138 - val_loss: 0.5829 - val_mean_squared_error: 0.5829\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.7065 - mean_squared_error: 0.7065 - val_loss: 0.5795 - val_mean_squared_error: 0.5795\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.7024 - mean_squared_error: 0.7024 - val_loss: 0.5759 - val_mean_squared_error: 0.5759\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.6998 - mean_squared_error: 0.6998 - val_loss: 0.5739 - val_mean_squared_error: 0.5739\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.6976 - mean_squared_error: 0.6976 - val_loss: 0.5731 - val_mean_squared_error: 0.5731\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.6961 - mean_squared_error: 0.6961 - val_loss: 0.5711 - val_mean_squared_error: 0.5711\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.6949 - mean_squared_error: 0.6949 - val_loss: 0.5693 - val_mean_squared_error: 0.5693\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.6931 - mean_squared_error: 0.6931 - val_loss: 0.5686 - val_mean_squared_error: 0.5686\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.6914 - mean_squared_error: 0.6914 - val_loss: 0.5663 - val_mean_squared_error: 0.5663\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.6894 - mean_squared_error: 0.6894 - val_loss: 0.5650 - val_mean_squared_error: 0.5650\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.6884 - mean_squared_error: 0.6884 - val_loss: 0.5630 - val_mean_squared_error: 0.5630\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.6865 - mean_squared_error: 0.6865 - val_loss: 0.5627 - val_mean_squared_error: 0.5627\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.6850 - mean_squared_error: 0.6850 - val_loss: 0.5618 - val_mean_squared_error: 0.5618\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.6838 - mean_squared_error: 0.6838 - val_loss: 0.5599 - val_mean_squared_error: 0.5599\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.6831 - mean_squared_error: 0.6831 - val_loss: 0.5582 - val_mean_squared_error: 0.5582\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.6820 - mean_squared_error: 0.6820 - val_loss: 0.5570 - val_mean_squared_error: 0.5570\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.6821 - mean_squared_error: 0.6821 - val_loss: 0.5571 - val_mean_squared_error: 0.5571\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.6817 - mean_squared_error: 0.6817 - val_loss: 0.5574 - val_mean_squared_error: 0.5574\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.6806 - mean_squared_error: 0.6806 - val_loss: 0.5562 - val_mean_squared_error: 0.5562\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.6794 - mean_squared_error: 0.6794 - val_loss: 0.5566 - val_mean_squared_error: 0.5566\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.6788 - mean_squared_error: 0.6788 - val_loss: 0.5557 - val_mean_squared_error: 0.5557\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.6780 - mean_squared_error: 0.6780 - val_loss: 0.5535 - val_mean_squared_error: 0.5535\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.6769 - mean_squared_error: 0.6769 - val_loss: 0.5531 - val_mean_squared_error: 0.5531\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.6760 - mean_squared_error: 0.6760 - val_loss: 0.5523 - val_mean_squared_error: 0.5523\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.6758 - mean_squared_error: 0.6758 - val_loss: 0.5518 - val_mean_squared_error: 0.5518\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.6752 - mean_squared_error: 0.6752 - val_loss: 0.5518 - val_mean_squared_error: 0.5518\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.6751 - mean_squared_error: 0.6751 - val_loss: 0.5510 - val_mean_squared_error: 0.5510\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.6750 - mean_squared_error: 0.6750 - val_loss: 0.5502 - val_mean_squared_error: 0.5502\n"
     ]
    }
   ],
   "source": [
    "input_signal = Input(shape = (X_train_frail.shape[1],))\n",
    "encoded_frail = Dense(32, activation = 'relu')(input_signal)\n",
    "\n",
    "# decoder\n",
    "decoded_frail = Dense(1500*7, activation = 'sigmoid')(encoded_frail)\n",
    "\n",
    "autoencoder_frail = Model(input = input_signal, output = decoded_frail)\n",
    "\n",
    "frail_encoder = Model(input = input_signal, output = encoded_frail)\n",
    "frail_encoded_input = Input(shape=(32,))\n",
    "frail_decoder_layer = autoencoder_frail.layers[-1]\n",
    "frail_decoder = Model(frail_encoded_input, frail_decoder_layer(frail_encoded_input))\n",
    "\n",
    "autoencoder_frail.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=['mse'])\n",
    "print(autoencoder_frail.summary())\n",
    "history = autoencoder_frail.fit(X_train_frail, train_ground_frail, epochs=30, batch_size=128, \n",
    "                                    validation_data=(X_val_frail, valid_ground_frail), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_frail_X_train = [frail_encoder.predict(X_train_non_frail), frail_encoder.predict(X_val_non_frail),\n",
    "                             frail_encoder.predict(X_train_pre_frail), frail_encoder.predict(X_val_pre_frail),\n",
    "                             frail_encoder.predict(X_train_frail), frail_encoder.predict(X_val_frail),\n",
    "                            ]\n",
    "encoder_frail_X_train = [i for sublist in encoder_frail_X_train for i in sublist]\n",
    "encoder_frail_X_test = [frail_encoder.predict(X_test_non_frail),\n",
    "                            frail_encoder.predict(X_test_pre_frail),\n",
    "                            frail_encoder.predict(X_test_frail)\n",
    "                           ]\n",
    "encoder_frail_X_test = [i for sublist in encoder_frail_X_test for i in sublist]\n",
    "\n",
    "encoder_frail_X_train = np.asarray(encoder_frail_X_train)\n",
    "encoder_frail_X_test = np.asarray(encoder_frail_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're just building the training and test set consisting of the features extracted by our encoder models in order to use them for classification purposes, using some simple machine learning classifier like kNN or SVM. Each of our features will consist of the concatenated feature vectors of each encoder, thus producing a feature vector consisting of $32$ + $32$ + $32$ = $96$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18681, 96)\n",
      "(1057, 96)\n",
      "(18681, 96) (1057, 96)\n"
     ]
    }
   ],
   "source": [
    "final_X_train = np.concatenate((encoder_non_frail_X_train, encoder_pre_frail_X_train), axis = 1)\n",
    "final_X_train = np.concatenate((final_X_train, encoder_frail_X_train), axis = 1)\n",
    "print(final_X_train.shape)\n",
    "final_X_test = np.concatenate((encoder_non_frail_X_test , encoder_pre_frail_X_test ), axis = 1)\n",
    "final_X_test = np.concatenate((final_X_test, encoder_frail_X_test), axis = 1)\n",
    "print(final_X_test.shape)\n",
    "\n",
    "\n",
    "encoder_y_train = [y_train_non_frail, y_train_pre_frail, y_train_frail]\n",
    "encoder_y_test = [y_test_non_frail, y_test_pre_frail, y_test_frail]\n",
    "\n",
    "encoder_y_train = [i for sublist in encoder_y_train for i in sublist]\n",
    "encoder_y_test = [i for sublist in encoder_y_test for i in sublist]\n",
    "encoder_y_train = np.asarray(encoder_y_train)\n",
    "encoder_y_test = np.asarray(encoder_y_test)\n",
    "\n",
    "c = list(zip(final_X_train, encoder_y_train))\n",
    "random.shuffle(c)\n",
    "final_X_train, final_y_train = zip(*c)\n",
    "\n",
    "c = list(zip(final_X_test, encoder_y_test))\n",
    "random.shuffle(c)\n",
    "final_X_test, final_y_test = zip(*c)\n",
    "\n",
    "final_X_train = np.asarray(final_X_train)\n",
    "final_X_test = np.asarray(final_X_test)\n",
    "print(final_X_train.shape, final_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run an initial experiment on the data we created by simply using a kNN classifier with the default parameters. The results of this experiment are good but not the best. Maybe with some hyperparameter tuning we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(final_X_train, final_y_train)\n",
    "preds = neigh.predict(final_X_test)\n",
    "print(\"Accuracy score %.2f\"%(accuracy_score(preds, final_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.662 (+/-0.023) for {'n_neighbors': 2}\n",
      "0.646 (+/-0.010) for {'n_neighbors': 3}\n",
      "0.645 (+/-0.013) for {'n_neighbors': 4}\n",
      "0.665 (+/-0.010) for {'n_neighbors': 5}\n",
      "0.666 (+/-0.010) for {'n_neighbors': 6}\n",
      "0.664 (+/-0.011) for {'n_neighbors': 7}\n",
      "0.670 (+/-0.014) for {'n_neighbors': 8}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      1.00      0.76       645\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00       399\n",
      "\n",
      "avg / total       0.37      0.61      0.46      1057\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 7}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.552 (+/-0.011) for {'n_neighbors': 2}\n",
      "0.596 (+/-0.005) for {'n_neighbors': 3}\n",
      "0.587 (+/-0.011) for {'n_neighbors': 4}\n",
      "0.594 (+/-0.007) for {'n_neighbors': 5}\n",
      "0.600 (+/-0.011) for {'n_neighbors': 6}\n",
      "0.601 (+/-0.006) for {'n_neighbors': 7}\n",
      "0.597 (+/-0.008) for {'n_neighbors': 8}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      1.00      0.76       645\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00       399\n",
      "\n",
      "avg / total       0.37      0.61      0.46      1057\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_neighbors': [2, 3, 4, 5, 6, 7, 8]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(KNeighborsClassifier(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(final_X_train, final_y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = final_y_test, clf.predict(final_X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the above results a promising setting is to use the kNN classifier with the n_neighbors parameter set to 5. Indeed the results are way much better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.61\n",
      "Confusion matrix, without normalization\n",
      "[[645   0   0]\n",
      " [ 13   0   0]\n",
      " [399   0   0]]\n",
      "Normalized confusion matrix\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEYCAYAAADLZOR0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKTrnXML/A0qD6VIvMONzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea0196fb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEYCAYAAAApuP8NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea0fc246a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=7)\n",
    "neigh.fit(final_X_train, final_y_train)\n",
    "preds = neigh.predict(final_X_test)\n",
    "print(\"Accuracy score %.2f\"%(accuracy_score(preds, final_y_test)))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(final_y_test, preds)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"non-frail\", \"pre-frail\", \"frail\"],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"non-frail\", \"pre-frail\", \"frail\"], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(labels, n_class = 6):\n",
    "    \"\"\" One-hot encoding \"\"\"\n",
    "    expansion = np.eye(n_class)\n",
    "    y = []\n",
    "    for i in labels:\n",
    "        y.append(expansion[int(i)])\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import keras.backend as K\n",
    "\n",
    "class BalancedAccuracy(Callback):\n",
    "    def __init__(self, train_data, validation_data):\n",
    "        super(BalancedAccuracy, self).__init__()\n",
    "        self.acas = []\n",
    "        self.validation_data = validation_data\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        X_val = self.validation_data[0]\n",
    "        y_val = self.validation_data[1]\n",
    "\n",
    "        X_train = self.train_data[0]\n",
    "        y_train = self.train_data[1]\n",
    "\n",
    "        y_val_pred = self.model.predict(X_val)\n",
    "        y_train_pred = self.model.predict(X_train)\n",
    "\n",
    "        val_score = self.eval_avg_class_acc(y_val, y_val_pred)\n",
    "        train_score = self.eval_avg_class_acc(y_train, y_train_pred)\n",
    "\n",
    "        self.acas.append([val_score])\n",
    "        self.acas.append([train_score])\n",
    "\n",
    "        print(\"\\nBalanced Accuracy - train: %.3f \\t val: %.3f\"%(train_score, val_score))        \n",
    "        \n",
    "    def eval_avg_class_acc(self, y_true, y_pred):\n",
    "\n",
    "        # decode one-hot to single labels\n",
    "        y_pred = y_pred.round()\n",
    "        y_pred = [ np.argmax(pred, axis = 0) for pred in y_pred ]\n",
    "        y_true = [ np.argmax(label, axis = 0) for label in y_true ]\n",
    "\n",
    "        cf = confusion_matrix(y_true, y_pred)\n",
    "        if np.unique(y_true).shape[0] == 2:\n",
    "            sensitivity = float(cf[1][1]) / float((cf[1][1] + cf[1][0]))\n",
    "            specificity = float(cf[0][0]) / float((cf[0][1] + cf[0][0]))\n",
    "\n",
    "            balanced_acc = (sensitivity + specificity) / 2\n",
    "        else:\n",
    "            balanced_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        return balanced_acc\n",
    "\n",
    "def weighted_categorical_crossentropy(y_true, y_pred, weights):\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    # scale predictions so that the class probas of each sample sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calc\n",
    "    loss = y_true * K.log(y_pred) * weights\n",
    "    loss = -K.sum(loss, -1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Weighted loss function to tackle class imbalance in the dataset\n",
    "\n",
    "weights = np.array([float(len(final_y_train)) / float(list(final_y_train).count(0)), float(len(final_y_train)) / \n",
    "                    float(list(final_y_train).count(1)), float(len(final_y_train)) / float(list(final_y_train).count(2))])\n",
    "w_cat_crossentropy = partial(weighted_categorical_crossentropy, weights = weights)\n",
    "w_cat_crossentropy.__name__ = 'weighted_categorical_crossentropy'\n",
    "        \n",
    "balanced_accuracy = BalancedAccuracy(train_data = (final_X_train, final_y_train), validation_data = (final_X_test, final_y_test))\n",
    "CALLBACKS = [balanced_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 3)\n",
      "Train on 18681 samples, validate on 1057 samples\n",
      "Epoch 1/300\n",
      " - 2s - loss: 7.9196 - acc: 0.3437 - val_loss: 4.3394 - val_acc: 0.6102\n",
      "\n",
      "Balanced Accuracy - train: 0.340 \t val: 0.610\n",
      "Epoch 2/300\n",
      " - 1s - loss: 7.5894 - acc: 0.3477 - val_loss: 4.1884 - val_acc: 0.6112\n",
      "\n",
      "Balanced Accuracy - train: 0.345 \t val: 0.610\n",
      "Epoch 3/300\n",
      " - 1s - loss: 7.3820 - acc: 0.3484 - val_loss: 4.1141 - val_acc: 0.6140\n",
      "\n",
      "Balanced Accuracy - train: 0.358 \t val: 0.610\n",
      "Epoch 4/300\n",
      " - 1s - loss: 7.2784 - acc: 0.3516 - val_loss: 4.0635 - val_acc: 0.6452\n",
      "\n",
      "Balanced Accuracy - train: 0.368 \t val: 0.610\n",
      "Epoch 5/300\n",
      " - 1s - loss: 7.0263 - acc: 0.3687 - val_loss: 4.0551 - val_acc: 0.3548\n",
      "\n",
      "Balanced Accuracy - train: 0.379 \t val: 0.610\n",
      "Epoch 6/300\n",
      " - 1s - loss: 6.8761 - acc: 0.3694 - val_loss: 4.0697 - val_acc: 0.3699\n",
      "\n",
      "Balanced Accuracy - train: 0.388 \t val: 0.610\n",
      "Epoch 7/300\n",
      " - 1s - loss: 6.7860 - acc: 0.3702 - val_loss: 4.0587 - val_acc: 0.3765\n",
      "\n",
      "Balanced Accuracy - train: 0.397 \t val: 0.610\n",
      "Epoch 8/300\n",
      " - 1s - loss: 6.6246 - acc: 0.3715 - val_loss: 4.0451 - val_acc: 0.3765\n",
      "\n",
      "Balanced Accuracy - train: 0.404 \t val: 0.610\n",
      "Epoch 9/300\n",
      " - 1s - loss: 6.5318 - acc: 0.3786 - val_loss: 4.1008 - val_acc: 0.3756\n",
      "\n",
      "Balanced Accuracy - train: 0.416 \t val: 0.610\n",
      "Epoch 10/300\n",
      " - 1s - loss: 6.4990 - acc: 0.3808 - val_loss: 4.0769 - val_acc: 0.3756\n",
      "\n",
      "Balanced Accuracy - train: 0.423 \t val: 0.610\n",
      "Epoch 11/300\n",
      " - 1s - loss: 6.4048 - acc: 0.3897 - val_loss: 4.1045 - val_acc: 0.3765\n",
      "\n",
      "Balanced Accuracy - train: 0.428 \t val: 0.610\n",
      "Epoch 12/300\n",
      " - 1s - loss: 6.3270 - acc: 0.3892 - val_loss: 4.1221 - val_acc: 0.3765\n",
      "\n",
      "Balanced Accuracy - train: 0.438 \t val: 0.610\n",
      "Epoch 13/300\n",
      " - 1s - loss: 6.2720 - acc: 0.3878 - val_loss: 4.1709 - val_acc: 0.3680\n",
      "\n",
      "Balanced Accuracy - train: 0.446 \t val: 0.610\n",
      "Epoch 14/300\n",
      " - 1s - loss: 6.2252 - acc: 0.3915 - val_loss: 4.1757 - val_acc: 0.3680\n",
      "\n",
      "Balanced Accuracy - train: 0.449 \t val: 0.610\n",
      "Epoch 15/300\n",
      " - 1s - loss: 6.1382 - acc: 0.3939 - val_loss: 4.1996 - val_acc: 0.3623\n",
      "\n",
      "Balanced Accuracy - train: 0.458 \t val: 0.610\n",
      "Epoch 16/300\n",
      " - 1s - loss: 6.0414 - acc: 0.4034 - val_loss: 4.2297 - val_acc: 0.3633\n",
      "\n",
      "Balanced Accuracy - train: 0.464 \t val: 0.610\n",
      "Epoch 17/300\n",
      " - 1s - loss: 5.9480 - acc: 0.4065 - val_loss: 4.2570 - val_acc: 0.3595\n",
      "\n",
      "Balanced Accuracy - train: 0.469 \t val: 0.610\n",
      "Epoch 18/300\n",
      " - 1s - loss: 5.8727 - acc: 0.4068 - val_loss: 4.2459 - val_acc: 0.3652\n",
      "\n",
      "Balanced Accuracy - train: 0.471 \t val: 0.610\n",
      "Epoch 19/300\n",
      " - 1s - loss: 5.8984 - acc: 0.4081 - val_loss: 4.2881 - val_acc: 0.3472\n",
      "\n",
      "Balanced Accuracy - train: 0.478 \t val: 0.610\n",
      "Epoch 20/300\n",
      " - 1s - loss: 5.8384 - acc: 0.4066 - val_loss: 4.3096 - val_acc: 0.3434\n",
      "\n",
      "Balanced Accuracy - train: 0.483 \t val: 0.610\n",
      "Epoch 21/300\n",
      " - 1s - loss: 5.8426 - acc: 0.4052 - val_loss: 4.3171 - val_acc: 0.3415\n",
      "\n",
      "Balanced Accuracy - train: 0.486 \t val: 0.610\n",
      "Epoch 22/300\n",
      " - 1s - loss: 5.7446 - acc: 0.4165 - val_loss: 4.3437 - val_acc: 0.3368\n",
      "\n",
      "Balanced Accuracy - train: 0.489 \t val: 0.610\n",
      "Epoch 23/300\n",
      " - 1s - loss: 5.6769 - acc: 0.4173 - val_loss: 4.3471 - val_acc: 0.3396\n",
      "\n",
      "Balanced Accuracy - train: 0.495 \t val: 0.610\n",
      "Epoch 24/300\n",
      " - 1s - loss: 5.6485 - acc: 0.4183 - val_loss: 4.3628 - val_acc: 0.3359\n",
      "\n",
      "Balanced Accuracy - train: 0.498 \t val: 0.610\n",
      "Epoch 25/300\n",
      " - 1s - loss: 5.6219 - acc: 0.4214 - val_loss: 4.3821 - val_acc: 0.3368\n",
      "\n",
      "Balanced Accuracy - train: 0.499 \t val: 0.610\n",
      "Epoch 26/300\n",
      " - 1s - loss: 5.4748 - acc: 0.4317 - val_loss: 4.4088 - val_acc: 0.3869\n",
      "\n",
      "Balanced Accuracy - train: 0.506 \t val: 0.610\n",
      "Epoch 27/300\n",
      " - 1s - loss: 5.4917 - acc: 0.4263 - val_loss: 4.4216 - val_acc: 0.3586\n",
      "\n",
      "Balanced Accuracy - train: 0.510 \t val: 0.610\n",
      "Epoch 28/300\n",
      " - 1s - loss: 5.5063 - acc: 0.4341 - val_loss: 4.4358 - val_acc: 0.3576\n",
      "\n",
      "Balanced Accuracy - train: 0.513 \t val: 0.610\n",
      "Epoch 29/300\n",
      " - 1s - loss: 5.4218 - acc: 0.4271 - val_loss: 4.4262 - val_acc: 0.3841\n",
      "\n",
      "Balanced Accuracy - train: 0.516 \t val: 0.610\n",
      "Epoch 30/300\n",
      " - 1s - loss: 5.3789 - acc: 0.4309 - val_loss: 4.4545 - val_acc: 0.4144\n",
      "\n",
      "Balanced Accuracy - train: 0.519 \t val: 0.610\n",
      "Epoch 31/300\n",
      " - 1s - loss: 5.4258 - acc: 0.4284 - val_loss: 4.4612 - val_acc: 0.4144\n",
      "\n",
      "Balanced Accuracy - train: 0.524 \t val: 0.610\n",
      "Epoch 32/300\n",
      " - 1s - loss: 5.3404 - acc: 0.4340 - val_loss: 4.4710 - val_acc: 0.3983\n",
      "\n",
      "Balanced Accuracy - train: 0.526 \t val: 0.610\n",
      "Epoch 33/300\n",
      " - 1s - loss: 5.2828 - acc: 0.4391 - val_loss: 4.4906 - val_acc: 0.4428\n",
      "\n",
      "Balanced Accuracy - train: 0.529 \t val: 0.610\n",
      "Epoch 34/300\n",
      " - 1s - loss: 5.2589 - acc: 0.4393 - val_loss: 4.5130 - val_acc: 0.4002\n",
      "\n",
      "Balanced Accuracy - train: 0.534 \t val: 0.610\n",
      "Epoch 35/300\n",
      " - 1s - loss: 5.2785 - acc: 0.4432 - val_loss: 4.5206 - val_acc: 0.4361\n",
      "\n",
      "Balanced Accuracy - train: 0.534 \t val: 0.610\n",
      "Epoch 36/300\n",
      " - 1s - loss: 5.1490 - acc: 0.4527 - val_loss: 4.5288 - val_acc: 0.4324\n",
      "\n",
      "Balanced Accuracy - train: 0.538 \t val: 0.610\n",
      "Epoch 37/300\n",
      " - 1s - loss: 5.1113 - acc: 0.4517 - val_loss: 4.5300 - val_acc: 0.4418\n",
      "\n",
      "Balanced Accuracy - train: 0.540 \t val: 0.610\n",
      "Epoch 38/300\n",
      " - 1s - loss: 5.1405 - acc: 0.4487 - val_loss: 4.5544 - val_acc: 0.4588\n",
      "\n",
      "Balanced Accuracy - train: 0.544 \t val: 0.610\n",
      "Epoch 39/300\n",
      " - 1s - loss: 5.1319 - acc: 0.4492 - val_loss: 4.5791 - val_acc: 0.4787\n",
      "\n",
      "Balanced Accuracy - train: 0.546 \t val: 0.610\n",
      "Epoch 40/300\n",
      " - 1s - loss: 5.0845 - acc: 0.4560 - val_loss: 4.5897 - val_acc: 0.4229\n",
      "\n",
      "Balanced Accuracy - train: 0.548 \t val: 0.610\n",
      "Epoch 41/300\n",
      " - 1s - loss: 5.0696 - acc: 0.4497 - val_loss: 4.6210 - val_acc: 0.4257\n",
      "\n",
      "Balanced Accuracy - train: 0.552 \t val: 0.610\n",
      "Epoch 42/300\n",
      " - 1s - loss: 4.9832 - acc: 0.4605 - val_loss: 4.6362 - val_acc: 0.5109\n",
      "\n",
      "Balanced Accuracy - train: 0.554 \t val: 0.610\n",
      "Epoch 43/300\n",
      " - 1s - loss: 4.9803 - acc: 0.4603 - val_loss: 4.6509 - val_acc: 0.5222\n",
      "\n",
      "Balanced Accuracy - train: 0.556 \t val: 0.610\n",
      "Epoch 44/300\n",
      " - 1s - loss: 5.0065 - acc: 0.4556 - val_loss: 4.6711 - val_acc: 0.4730\n",
      "\n",
      "Balanced Accuracy - train: 0.559 \t val: 0.610\n",
      "Epoch 45/300\n",
      " - 1s - loss: 4.9301 - acc: 0.4614 - val_loss: 4.6878 - val_acc: 0.4182\n",
      "\n",
      "Balanced Accuracy - train: 0.561 \t val: 0.610\n",
      "Epoch 46/300\n",
      " - 1s - loss: 4.9139 - acc: 0.4588 - val_loss: 4.6795 - val_acc: 0.3680\n",
      "\n",
      "Balanced Accuracy - train: 0.563 \t val: 0.610\n",
      "Epoch 47/300\n",
      " - 1s - loss: 4.8467 - acc: 0.4725 - val_loss: 4.6874 - val_acc: 0.4522\n",
      "\n",
      "Balanced Accuracy - train: 0.565 \t val: 0.610\n",
      "Epoch 48/300\n",
      " - 1s - loss: 4.8357 - acc: 0.4700 - val_loss: 4.7023 - val_acc: 0.5071\n",
      "\n",
      "Balanced Accuracy - train: 0.567 \t val: 0.610\n",
      "Epoch 49/300\n",
      " - 1s - loss: 4.8154 - acc: 0.4720 - val_loss: 4.7348 - val_acc: 0.3983\n",
      "\n",
      "Balanced Accuracy - train: 0.568 \t val: 0.610\n",
      "Epoch 50/300\n",
      " - 1s - loss: 4.8322 - acc: 0.4705 - val_loss: 4.7175 - val_acc: 0.2602\n",
      "\n",
      "Balanced Accuracy - train: 0.571 \t val: 0.610\n",
      "Epoch 51/300\n",
      " - 1s - loss: 4.7874 - acc: 0.4757 - val_loss: 4.7425 - val_acc: 0.1958\n",
      "\n",
      "Balanced Accuracy - train: 0.573 \t val: 0.610\n",
      "Epoch 52/300\n",
      " - 1s - loss: 4.7979 - acc: 0.4749 - val_loss: 4.7507 - val_acc: 0.0965\n",
      "\n",
      "Balanced Accuracy - train: 0.577 \t val: 0.610\n",
      "Epoch 53/300\n",
      " - 1s - loss: 4.7124 - acc: 0.4780 - val_loss: 4.7549 - val_acc: 0.0956\n",
      "\n",
      "Balanced Accuracy - train: 0.578 \t val: 0.610\n",
      "Epoch 54/300\n",
      " - 1s - loss: 4.7135 - acc: 0.4819 - val_loss: 4.7811 - val_acc: 0.0785\n",
      "\n",
      "Balanced Accuracy - train: 0.581 \t val: 0.610\n",
      "Epoch 55/300\n",
      " - 1s - loss: 4.6739 - acc: 0.4871 - val_loss: 4.8151 - val_acc: 0.0482\n",
      "\n",
      "Balanced Accuracy - train: 0.583 \t val: 0.610\n",
      "Epoch 56/300\n",
      " - 1s - loss: 4.6781 - acc: 0.4833 - val_loss: 4.8187 - val_acc: 0.0511\n",
      "\n",
      "Balanced Accuracy - train: 0.585 \t val: 0.610\n",
      "Epoch 57/300\n",
      " - 1s - loss: 4.6888 - acc: 0.4805 - val_loss: 4.8331 - val_acc: 0.0426\n",
      "\n",
      "Balanced Accuracy - train: 0.588 \t val: 0.610\n",
      "Epoch 58/300\n",
      " - 1s - loss: 4.5867 - acc: 0.4869 - val_loss: 4.8717 - val_acc: 0.0322\n",
      "\n",
      "Balanced Accuracy - train: 0.588 \t val: 0.610\n",
      "Epoch 59/300\n",
      " - 1s - loss: 4.5788 - acc: 0.4889 - val_loss: 4.8751 - val_acc: 0.0350\n",
      "\n",
      "Balanced Accuracy - train: 0.591 \t val: 0.610\n",
      "Epoch 60/300\n",
      " - 1s - loss: 4.5995 - acc: 0.4841 - val_loss: 4.9087 - val_acc: 0.0218\n",
      "\n",
      "Balanced Accuracy - train: 0.592 \t val: 0.610\n",
      "Epoch 61/300\n",
      " - 1s - loss: 4.6231 - acc: 0.4815 - val_loss: 4.9197 - val_acc: 0.0208\n",
      "\n",
      "Balanced Accuracy - train: 0.595 \t val: 0.610\n",
      "Epoch 62/300\n",
      " - 1s - loss: 4.5113 - acc: 0.4983 - val_loss: 4.9344 - val_acc: 0.0189\n",
      "\n",
      "Balanced Accuracy - train: 0.597 \t val: 0.610\n",
      "Epoch 63/300\n",
      " - 1s - loss: 4.5781 - acc: 0.4948 - val_loss: 4.9241 - val_acc: 0.0208\n",
      "\n",
      "Balanced Accuracy - train: 0.599 \t val: 0.610\n",
      "Epoch 64/300\n",
      " - 1s - loss: 4.5360 - acc: 0.4923 - val_loss: 4.9456 - val_acc: 0.0180\n",
      "\n",
      "Balanced Accuracy - train: 0.601 \t val: 0.610\n",
      "Epoch 65/300\n",
      " - 1s - loss: 4.5020 - acc: 0.4964 - val_loss: 4.9397 - val_acc: 0.0180\n",
      "\n",
      "Balanced Accuracy - train: 0.602 \t val: 0.610\n",
      "Epoch 66/300\n",
      " - 1s - loss: 4.4866 - acc: 0.4980 - val_loss: 4.9587 - val_acc: 0.0180\n",
      "\n",
      "Balanced Accuracy - train: 0.603 \t val: 0.610\n",
      "Epoch 67/300\n",
      " - 1s - loss: 4.4110 - acc: 0.5026 - val_loss: 4.9861 - val_acc: 0.0170\n",
      "\n",
      "Balanced Accuracy - train: 0.604 \t val: 0.610\n",
      "Epoch 68/300\n",
      " - 1s - loss: 4.4240 - acc: 0.5043 - val_loss: 4.9872 - val_acc: 0.0170\n",
      "\n",
      "Balanced Accuracy - train: 0.607 \t val: 0.610\n",
      "Epoch 69/300\n",
      " - 1s - loss: 4.4248 - acc: 0.4991 - val_loss: 5.0205 - val_acc: 0.0161\n",
      "\n",
      "Balanced Accuracy - train: 0.606 \t val: 0.610\n",
      "Epoch 70/300\n",
      " - 1s - loss: 4.3823 - acc: 0.5083 - val_loss: 4.9950 - val_acc: 0.0161\n",
      "\n",
      "Balanced Accuracy - train: 0.608 \t val: 0.610\n",
      "Epoch 71/300\n",
      " - 1s - loss: 4.4282 - acc: 0.5010 - val_loss: 4.9907 - val_acc: 0.0170\n",
      "\n",
      "Balanced Accuracy - train: 0.611 \t val: 0.610\n",
      "Epoch 72/300\n",
      " - 1s - loss: 4.3687 - acc: 0.5049 - val_loss: 5.0539 - val_acc: 0.0142\n",
      "\n",
      "Balanced Accuracy - train: 0.610 \t val: 0.610\n",
      "Epoch 73/300\n",
      " - 1s - loss: 4.3210 - acc: 0.5130 - val_loss: 5.0707 - val_acc: 0.0142\n",
      "\n",
      "Balanced Accuracy - train: 0.613 \t val: 0.610\n",
      "Epoch 74/300\n",
      " - 1s - loss: 4.3395 - acc: 0.5090 - val_loss: 5.0433 - val_acc: 0.0161\n",
      "\n",
      "Balanced Accuracy - train: 0.614 \t val: 0.610\n",
      "Epoch 75/300\n",
      " - 1s - loss: 4.2929 - acc: 0.5136 - val_loss: 5.0649 - val_acc: 0.0142\n",
      "\n",
      "Balanced Accuracy - train: 0.616 \t val: 0.610\n",
      "Epoch 76/300\n",
      " - 1s - loss: 4.3179 - acc: 0.5124 - val_loss: 5.0998 - val_acc: 0.0132\n",
      "\n",
      "Balanced Accuracy - train: 0.616 \t val: 0.610\n",
      "Epoch 77/300\n",
      " - 1s - loss: 4.3417 - acc: 0.5100 - val_loss: 5.0705 - val_acc: 0.0142\n",
      "\n",
      "Balanced Accuracy - train: 0.617 \t val: 0.610\n",
      "Epoch 78/300\n",
      " - 1s - loss: 4.3136 - acc: 0.5103 - val_loss: 5.1134 - val_acc: 0.0132\n",
      "\n",
      "Balanced Accuracy - train: 0.618 \t val: 0.610\n",
      "Epoch 79/300\n",
      " - 1s - loss: 4.2572 - acc: 0.5191 - val_loss: 5.1284 - val_acc: 0.0132\n",
      "\n",
      "Balanced Accuracy - train: 0.619 \t val: 0.610\n",
      "Epoch 80/300\n",
      " - 1s - loss: 4.2462 - acc: 0.5168 - val_loss: 5.1275 - val_acc: 0.0132\n",
      "\n",
      "Balanced Accuracy - train: 0.621 \t val: 0.610\n",
      "Epoch 81/300\n",
      " - 1s - loss: 4.2936 - acc: 0.5174 - val_loss: 5.1269 - val_acc: 0.0132\n",
      "\n",
      "Balanced Accuracy - train: 0.622 \t val: 0.610\n",
      "Epoch 82/300\n",
      " - 1s - loss: 4.2634 - acc: 0.5163 - val_loss: 5.1785 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.623 \t val: 0.610\n",
      "Epoch 83/300\n",
      " - 1s - loss: 4.2283 - acc: 0.5193 - val_loss: 5.2494 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.624 \t val: 0.610\n",
      "Epoch 84/300\n",
      " - 1s - loss: 4.2295 - acc: 0.5181 - val_loss: 5.2409 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.625 \t val: 0.610\n",
      "Epoch 85/300\n",
      " - 1s - loss: 4.2145 - acc: 0.5234 - val_loss: 5.2365 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.626 \t val: 0.610\n",
      "Epoch 86/300\n",
      " - 1s - loss: 4.1865 - acc: 0.5170 - val_loss: 5.2544 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.626 \t val: 0.610\n",
      "Epoch 87/300\n",
      " - 1s - loss: 4.1548 - acc: 0.5210 - val_loss: 5.2627 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.626 \t val: 0.610\n",
      "Epoch 88/300\n",
      " - 1s - loss: 4.1622 - acc: 0.5229 - val_loss: 5.2702 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.628 \t val: 0.610\n",
      "Epoch 89/300\n",
      " - 1s - loss: 4.1338 - acc: 0.5274 - val_loss: 5.2736 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.629 \t val: 0.610\n",
      "Epoch 90/300\n",
      " - 1s - loss: 4.1555 - acc: 0.5276 - val_loss: 5.3179 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.630 \t val: 0.610\n",
      "Epoch 91/300\n",
      " - 1s - loss: 4.1067 - acc: 0.5288 - val_loss: 5.3682 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.632 \t val: 0.610\n",
      "Epoch 92/300\n",
      " - 1s - loss: 4.0794 - acc: 0.5325 - val_loss: 5.3594 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.632 \t val: 0.610\n",
      "Epoch 93/300\n",
      " - 1s - loss: 4.0561 - acc: 0.5289 - val_loss: 5.3517 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.633 \t val: 0.610\n",
      "Epoch 94/300\n",
      " - 1s - loss: 4.1214 - acc: 0.5309 - val_loss: 5.3890 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.634 \t val: 0.610\n",
      "Epoch 95/300\n",
      " - 1s - loss: 4.1079 - acc: 0.5313 - val_loss: 5.4033 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.634 \t val: 0.610\n",
      "Epoch 96/300\n",
      " - 1s - loss: 4.0408 - acc: 0.5377 - val_loss: 5.4296 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.635 \t val: 0.610\n",
      "Epoch 97/300\n",
      " - 1s - loss: 4.0325 - acc: 0.5334 - val_loss: 5.4311 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.636 \t val: 0.610\n",
      "Epoch 98/300\n",
      " - 1s - loss: 4.0286 - acc: 0.5384 - val_loss: 5.5005 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.638 \t val: 0.610\n",
      "Epoch 99/300\n",
      " - 1s - loss: 4.0367 - acc: 0.5386 - val_loss: 5.5068 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.637 \t val: 0.610\n",
      "Epoch 100/300\n",
      " - 1s - loss: 3.9963 - acc: 0.5403 - val_loss: 5.5266 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.638 \t val: 0.610\n",
      "Epoch 101/300\n",
      " - 1s - loss: 4.0144 - acc: 0.5408 - val_loss: 5.5173 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.638 \t val: 0.610\n",
      "Epoch 102/300\n",
      " - 1s - loss: 3.9881 - acc: 0.5417 - val_loss: 5.5520 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.639 \t val: 0.610\n",
      "Epoch 103/300\n",
      " - 1s - loss: 3.9830 - acc: 0.5455 - val_loss: 5.5647 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.641 \t val: 0.610\n",
      "Epoch 104/300\n",
      " - 1s - loss: 3.9910 - acc: 0.5355 - val_loss: 5.6067 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.641 \t val: 0.610\n",
      "Epoch 105/300\n",
      " - 1s - loss: 3.9229 - acc: 0.5458 - val_loss: 5.6312 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.642 \t val: 0.610\n",
      "Epoch 106/300\n",
      " - 1s - loss: 3.9244 - acc: 0.5464 - val_loss: 5.6035 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.642 \t val: 0.610\n",
      "Epoch 107/300\n",
      " - 1s - loss: 3.9211 - acc: 0.5478 - val_loss: 5.6356 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.643 \t val: 0.610\n",
      "Epoch 108/300\n",
      " - 1s - loss: 3.9024 - acc: 0.5506 - val_loss: 5.6873 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.642 \t val: 0.610\n",
      "Epoch 109/300\n",
      " - 1s - loss: 3.8757 - acc: 0.5476 - val_loss: 5.6933 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.643 \t val: 0.610\n",
      "Epoch 110/300\n",
      " - 1s - loss: 3.8359 - acc: 0.5549 - val_loss: 5.6671 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.643 \t val: 0.610\n",
      "Epoch 111/300\n",
      " - 1s - loss: 3.8761 - acc: 0.5526 - val_loss: 5.6687 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.644 \t val: 0.610\n",
      "Epoch 112/300\n",
      " - 1s - loss: 3.9219 - acc: 0.5428 - val_loss: 5.7279 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.645 \t val: 0.610\n",
      "Epoch 113/300\n",
      " - 1s - loss: 3.8833 - acc: 0.5490 - val_loss: 5.7291 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.645 \t val: 0.610\n",
      "Epoch 114/300\n",
      " - 1s - loss: 3.8542 - acc: 0.5506 - val_loss: 5.7354 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.645 \t val: 0.610\n",
      "Epoch 115/300\n",
      " - 1s - loss: 3.8523 - acc: 0.5551 - val_loss: 5.7696 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.646 \t val: 0.610\n",
      "Epoch 116/300\n",
      " - 1s - loss: 3.8545 - acc: 0.5531 - val_loss: 5.8001 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.647 \t val: 0.611\n",
      "Epoch 117/300\n",
      " - 1s - loss: 3.8130 - acc: 0.5548 - val_loss: 5.8300 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.646 \t val: 0.612\n",
      "Epoch 118/300\n",
      " - 1s - loss: 3.8100 - acc: 0.5575 - val_loss: 5.7982 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.647 \t val: 0.612\n",
      "Epoch 119/300\n",
      " - 1s - loss: 3.8238 - acc: 0.5550 - val_loss: 5.8236 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.649 \t val: 0.612\n",
      "Epoch 120/300\n",
      " - 1s - loss: 3.7886 - acc: 0.5605 - val_loss: 5.8782 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.649 \t val: 0.614\n",
      "Epoch 121/300\n",
      " - 1s - loss: 3.8029 - acc: 0.5561 - val_loss: 5.8823 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.650 \t val: 0.614\n",
      "Epoch 122/300\n",
      " - 1s - loss: 3.7946 - acc: 0.5547 - val_loss: 5.8919 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.650 \t val: 0.614\n",
      "Epoch 123/300\n",
      " - 1s - loss: 3.8134 - acc: 0.5552 - val_loss: 5.8872 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.650 \t val: 0.614\n",
      "Epoch 124/300\n",
      " - 1s - loss: 3.7808 - acc: 0.5609 - val_loss: 5.9729 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.649 \t val: 0.613\n",
      "Epoch 125/300\n",
      " - 1s - loss: 3.7590 - acc: 0.5612 - val_loss: 6.0386 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.650 \t val: 0.616\n",
      "Epoch 126/300\n",
      " - 1s - loss: 3.7336 - acc: 0.5599 - val_loss: 5.9936 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.651 \t val: 0.616\n",
      "Epoch 127/300\n",
      " - 1s - loss: 3.7380 - acc: 0.5662 - val_loss: 5.9507 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.652 \t val: 0.613\n",
      "Epoch 128/300\n",
      " - 1s - loss: 3.7343 - acc: 0.5632 - val_loss: 6.0461 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.652 \t val: 0.615\n",
      "Epoch 129/300\n",
      " - 1s - loss: 3.6972 - acc: 0.5691 - val_loss: 5.9985 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.653 \t val: 0.616\n",
      "Epoch 130/300\n",
      " - 1s - loss: 3.7096 - acc: 0.5670 - val_loss: 5.9933 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.654 \t val: 0.614\n",
      "Epoch 131/300\n",
      " - 1s - loss: 3.6664 - acc: 0.5705 - val_loss: 6.0127 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.654 \t val: 0.615\n",
      "Epoch 132/300\n",
      " - 1s - loss: 3.6893 - acc: 0.5676 - val_loss: 6.0881 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.655 \t val: 0.615\n",
      "Epoch 133/300\n",
      " - 1s - loss: 3.7065 - acc: 0.5614 - val_loss: 6.0536 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.654 \t val: 0.616\n",
      "Epoch 134/300\n",
      " - 1s - loss: 3.6728 - acc: 0.5707 - val_loss: 6.1041 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.655 \t val: 0.616\n",
      "Epoch 135/300\n",
      " - 1s - loss: 3.6643 - acc: 0.5703 - val_loss: 6.0945 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.656 \t val: 0.610\n",
      "Epoch 136/300\n",
      " - 1s - loss: 3.6569 - acc: 0.5713 - val_loss: 6.1210 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.656 \t val: 0.595\n",
      "Epoch 137/300\n",
      " - 1s - loss: 3.6743 - acc: 0.5735 - val_loss: 6.1563 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.657 \t val: 0.585\n",
      "Epoch 138/300\n",
      " - 1s - loss: 3.6879 - acc: 0.5715 - val_loss: 6.1546 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.657 \t val: 0.596\n",
      "Epoch 139/300\n",
      " - 1s - loss: 3.6480 - acc: 0.5709 - val_loss: 6.1976 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.658 \t val: 0.553\n",
      "Epoch 140/300\n",
      " - 1s - loss: 3.6419 - acc: 0.5718 - val_loss: 6.2214 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.657 \t val: 0.516\n",
      "Epoch 141/300\n",
      " - 1s - loss: 3.6024 - acc: 0.5765 - val_loss: 6.2461 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.660 \t val: 0.507\n",
      "Epoch 142/300\n",
      " - 1s - loss: 3.6236 - acc: 0.5767 - val_loss: 6.2512 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.660 \t val: 0.509\n",
      "Epoch 143/300\n",
      " - 1s - loss: 3.5676 - acc: 0.5812 - val_loss: 6.3099 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.661 \t val: 0.412\n",
      "Epoch 144/300\n",
      " - 1s - loss: 3.6143 - acc: 0.5761 - val_loss: 6.3393 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.661 \t val: 0.317\n",
      "Epoch 145/300\n",
      " - 1s - loss: 3.6189 - acc: 0.5780 - val_loss: 6.3014 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.662 \t val: 0.469\n",
      "Epoch 146/300\n",
      " - 1s - loss: 3.5716 - acc: 0.5813 - val_loss: 6.3501 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.662 \t val: 0.382\n",
      "Epoch 147/300\n",
      " - 1s - loss: 3.5642 - acc: 0.5800 - val_loss: 6.3521 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.662 \t val: 0.386\n",
      "Epoch 148/300\n",
      " - 1s - loss: 3.5576 - acc: 0.5798 - val_loss: 6.4232 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.663 \t val: 0.219\n",
      "Epoch 149/300\n",
      " - 1s - loss: 3.5688 - acc: 0.5773 - val_loss: 6.4817 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.663 \t val: 0.169\n",
      "Epoch 150/300\n",
      " - 1s - loss: 3.5597 - acc: 0.5811 - val_loss: 6.4848 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.663 \t val: 0.149\n",
      "Epoch 151/300\n",
      " - 1s - loss: 3.5427 - acc: 0.5851 - val_loss: 6.4822 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.665 \t val: 0.210\n",
      "Epoch 152/300\n",
      " - 1s - loss: 3.5166 - acc: 0.5797 - val_loss: 6.5429 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.664 \t val: 0.114\n",
      "Epoch 153/300\n",
      " - 1s - loss: 3.5553 - acc: 0.5811 - val_loss: 6.5153 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.664 \t val: 0.112\n",
      "Epoch 154/300\n",
      " - 1s - loss: 3.5259 - acc: 0.5835 - val_loss: 6.5193 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.665 \t val: 0.126\n",
      "Epoch 155/300\n",
      " - 1s - loss: 3.5019 - acc: 0.5874 - val_loss: 6.5055 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.666 \t val: 0.141\n",
      "Epoch 156/300\n",
      " - 1s - loss: 3.5158 - acc: 0.5858 - val_loss: 6.5414 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.666 \t val: 0.143\n",
      "Epoch 157/300\n",
      " - 1s - loss: 3.5484 - acc: 0.5848 - val_loss: 6.5825 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.666 \t val: 0.072\n",
      "Epoch 158/300\n",
      " - 1s - loss: 3.5002 - acc: 0.5882 - val_loss: 6.5909 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.667 \t val: 0.060\n",
      "Epoch 159/300\n",
      " - 1s - loss: 3.5034 - acc: 0.5908 - val_loss: 6.5938 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.666 \t val: 0.061\n",
      "Epoch 160/300\n",
      " - 1s - loss: 3.4800 - acc: 0.5884 - val_loss: 6.6381 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.667 \t val: 0.057\n",
      "Epoch 161/300\n",
      " - 1s - loss: 3.5164 - acc: 0.5876 - val_loss: 6.6677 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.666 \t val: 0.043\n",
      "Epoch 162/300\n",
      " - 1s - loss: 3.4671 - acc: 0.5880 - val_loss: 6.7212 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.668 \t val: 0.026\n",
      "Epoch 163/300\n",
      " - 1s - loss: 3.4651 - acc: 0.5919 - val_loss: 6.6472 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.668 \t val: 0.048\n",
      "Epoch 164/300\n",
      " - 1s - loss: 3.4512 - acc: 0.5904 - val_loss: 6.6423 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.669 \t val: 0.083\n",
      "Epoch 165/300\n",
      " - 1s - loss: 3.4545 - acc: 0.5928 - val_loss: 6.6402 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.671 \t val: 0.084\n",
      "Epoch 166/300\n",
      " - 1s - loss: 3.4497 - acc: 0.5959 - val_loss: 6.7480 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.022\n",
      "Epoch 167/300\n",
      " - 1s - loss: 3.4294 - acc: 0.5969 - val_loss: 6.7579 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.669 \t val: 0.020\n",
      "Epoch 168/300\n",
      " - 1s - loss: 3.4361 - acc: 0.5946 - val_loss: 6.7896 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.669 \t val: 0.020\n",
      "Epoch 169/300\n",
      " - 1s - loss: 3.4637 - acc: 0.5853 - val_loss: 6.7266 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.020\n",
      "Epoch 170/300\n",
      " - 1s - loss: 3.4372 - acc: 0.5910 - val_loss: 6.7511 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.670 \t val: 0.020\n",
      "Epoch 171/300\n",
      " - 1s - loss: 3.4172 - acc: 0.5971 - val_loss: 6.7659 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.671 \t val: 0.020\n",
      "Epoch 172/300\n",
      " - 1s - loss: 3.4442 - acc: 0.5925 - val_loss: 6.7505 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.672 \t val: 0.022\n",
      "Epoch 173/300\n",
      " - 1s - loss: 3.3874 - acc: 0.6000 - val_loss: 6.8158 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.672 \t val: 0.017\n",
      "Epoch 174/300\n",
      " - 1s - loss: 3.3719 - acc: 0.6015 - val_loss: 6.7616 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.673 \t val: 0.020\n",
      "Epoch 175/300\n",
      " - 1s - loss: 3.3803 - acc: 0.5968 - val_loss: 6.7764 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.673 \t val: 0.020\n",
      "Epoch 176/300\n",
      " - 1s - loss: 3.3917 - acc: 0.5970 - val_loss: 6.7447 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.674 \t val: 0.025\n",
      "Epoch 177/300\n",
      " - 1s - loss: 3.3757 - acc: 0.6006 - val_loss: 6.7893 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.674 \t val: 0.020\n",
      "Epoch 178/300\n",
      " - 1s - loss: 3.3503 - acc: 0.6048 - val_loss: 6.7668 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.675 \t val: 0.023\n",
      "Epoch 179/300\n",
      " - 1s - loss: 3.3453 - acc: 0.6018 - val_loss: 6.8434 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.674 \t val: 0.020\n",
      "Epoch 180/300\n",
      " - 1s - loss: 3.3195 - acc: 0.6067 - val_loss: 6.8406 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.675 \t val: 0.017\n",
      "Epoch 181/300\n",
      " - 1s - loss: 3.3309 - acc: 0.6056 - val_loss: 6.8606 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.676 \t val: 0.017\n",
      "Epoch 182/300\n",
      " - 1s - loss: 3.3163 - acc: 0.6082 - val_loss: 6.8807 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.675 \t val: 0.015\n",
      "Epoch 183/300\n",
      " - 1s - loss: 3.3195 - acc: 0.6029 - val_loss: 6.8773 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.676 \t val: 0.015\n",
      "Epoch 184/300\n",
      " - 1s - loss: 3.3393 - acc: 0.6051 - val_loss: 6.8789 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.676 \t val: 0.016\n",
      "Epoch 185/300\n",
      " - 1s - loss: 3.3366 - acc: 0.6059 - val_loss: 6.8791 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.677 \t val: 0.015\n",
      "Epoch 186/300\n",
      " - 1s - loss: 3.3441 - acc: 0.6028 - val_loss: 6.9053 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.677 \t val: 0.015\n",
      "Epoch 187/300\n",
      " - 1s - loss: 3.3114 - acc: 0.6057 - val_loss: 6.9686 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.677 \t val: 0.014\n",
      "Epoch 188/300\n",
      " - 1s - loss: 3.2847 - acc: 0.6055 - val_loss: 7.0222 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.677 \t val: 0.013\n",
      "Epoch 189/300\n",
      " - 1s - loss: 3.3141 - acc: 0.6049 - val_loss: 7.0864 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.678 \t val: 0.012\n",
      "Epoch 190/300\n",
      " - 1s - loss: 3.2879 - acc: 0.6106 - val_loss: 6.9991 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.679 \t val: 0.013\n",
      "Epoch 191/300\n",
      " - 1s - loss: 3.3160 - acc: 0.6032 - val_loss: 7.0535 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.679 \t val: 0.012\n",
      "Epoch 192/300\n",
      " - 1s - loss: 3.3074 - acc: 0.6030 - val_loss: 7.0107 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.680 \t val: 0.013\n",
      "Epoch 193/300\n",
      " - 1s - loss: 3.3000 - acc: 0.6066 - val_loss: 7.0504 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.680 \t val: 0.012\n",
      "Epoch 194/300\n",
      " - 1s - loss: 3.2530 - acc: 0.6142 - val_loss: 7.0347 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.680 \t val: 0.012\n",
      "Epoch 195/300\n",
      " - 1s - loss: 3.2560 - acc: 0.6094 - val_loss: 7.0802 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.680 \t val: 0.012\n",
      "Epoch 196/300\n",
      " - 1s - loss: 3.2638 - acc: 0.6115 - val_loss: 7.0699 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.681 \t val: 0.013\n",
      "Epoch 197/300\n",
      " - 1s - loss: 3.2568 - acc: 0.6102 - val_loss: 7.0847 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.681 \t val: 0.012\n",
      "Epoch 198/300\n",
      " - 1s - loss: 3.2926 - acc: 0.6074 - val_loss: 7.1020 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.681 \t val: 0.012\n",
      "Epoch 199/300\n",
      " - 1s - loss: 3.2539 - acc: 0.6119 - val_loss: 7.0937 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.681 \t val: 0.012\n",
      "Epoch 200/300\n",
      " - 1s - loss: 3.2429 - acc: 0.6124 - val_loss: 7.0612 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.682 \t val: 0.014\n",
      "Epoch 201/300\n",
      " - 1s - loss: 3.2454 - acc: 0.6117 - val_loss: 7.1173 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.683 \t val: 0.012\n",
      "Epoch 202/300\n",
      " - 1s - loss: 3.2302 - acc: 0.6172 - val_loss: 7.0650 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.683 \t val: 0.013\n",
      "Epoch 203/300\n",
      " - 1s - loss: 3.2117 - acc: 0.6181 - val_loss: 7.0979 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.683 \t val: 0.012\n",
      "Epoch 204/300\n",
      " - 1s - loss: 3.2341 - acc: 0.6159 - val_loss: 7.1337 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.684 \t val: 0.012\n",
      "Epoch 205/300\n",
      " - 1s - loss: 3.2057 - acc: 0.6134 - val_loss: 7.1569 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.685 \t val: 0.012\n",
      "Epoch 206/300\n",
      " - 1s - loss: 3.2135 - acc: 0.6153 - val_loss: 7.1671 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.684 \t val: 0.012\n",
      "Epoch 207/300\n",
      " - 1s - loss: 3.2362 - acc: 0.6137 - val_loss: 7.1644 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.685 \t val: 0.012\n",
      "Epoch 208/300\n",
      " - 1s - loss: 3.1928 - acc: 0.6154 - val_loss: 7.2181 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.684 \t val: 0.012\n",
      "Epoch 209/300\n",
      " - 1s - loss: 3.2082 - acc: 0.6159 - val_loss: 7.1681 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.685 \t val: 0.012\n",
      "Epoch 210/300\n",
      " - 1s - loss: 3.1994 - acc: 0.6170 - val_loss: 7.1506 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.686 \t val: 0.012\n",
      "Epoch 211/300\n",
      " - 1s - loss: 3.2108 - acc: 0.6195 - val_loss: 7.2725 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.684 \t val: 0.012\n",
      "Epoch 212/300\n",
      " - 1s - loss: 3.1894 - acc: 0.6177 - val_loss: 7.2668 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.685 \t val: 0.012\n",
      "Epoch 213/300\n",
      " - 1s - loss: 3.1957 - acc: 0.6163 - val_loss: 7.2285 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.687 \t val: 0.012\n",
      "Epoch 214/300\n",
      " - 1s - loss: 3.1705 - acc: 0.6209 - val_loss: 7.2942 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.687 \t val: 0.012\n",
      "Epoch 215/300\n",
      " - 1s - loss: 3.1849 - acc: 0.6197 - val_loss: 7.3174 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.687 \t val: 0.012\n",
      "Epoch 216/300\n",
      " - 1s - loss: 3.1742 - acc: 0.6179 - val_loss: 7.2846 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.688 \t val: 0.012\n",
      "Epoch 217/300\n",
      " - 1s - loss: 3.1691 - acc: 0.6216 - val_loss: 7.3164 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.687 \t val: 0.012\n",
      "Epoch 218/300\n",
      " - 1s - loss: 3.1760 - acc: 0.6196 - val_loss: 7.3503 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.687 \t val: 0.012\n",
      "Epoch 219/300\n",
      " - 1s - loss: 3.1911 - acc: 0.6221 - val_loss: 7.3442 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.688 \t val: 0.012\n",
      "Epoch 220/300\n",
      " - 1s - loss: 3.1601 - acc: 0.6252 - val_loss: 7.3369 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.689 \t val: 0.012\n",
      "Epoch 221/300\n",
      " - 1s - loss: 3.1599 - acc: 0.6227 - val_loss: 7.3448 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.689 \t val: 0.012\n",
      "Epoch 222/300\n",
      " - 1s - loss: 3.1549 - acc: 0.6239 - val_loss: 7.3695 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.689 \t val: 0.012\n",
      "Epoch 223/300\n",
      " - 1s - loss: 3.1453 - acc: 0.6271 - val_loss: 7.4342 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.688 \t val: 0.012\n",
      "Epoch 224/300\n",
      " - 1s - loss: 3.1444 - acc: 0.6209 - val_loss: 7.3318 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.689 \t val: 0.012\n",
      "Epoch 225/300\n",
      " - 1s - loss: 3.1350 - acc: 0.6269 - val_loss: 7.2903 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.691 \t val: 0.012\n",
      "Epoch 226/300\n",
      " - 1s - loss: 3.1398 - acc: 0.6248 - val_loss: 7.3387 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.690 \t val: 0.012\n",
      "Epoch 227/300\n",
      " - 1s - loss: 3.1539 - acc: 0.6211 - val_loss: 7.3375 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.690 \t val: 0.012\n",
      "Epoch 228/300\n",
      " - 1s - loss: 3.1109 - acc: 0.6284 - val_loss: 7.3604 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.690 \t val: 0.012\n",
      "Epoch 229/300\n",
      " - 1s - loss: 3.1473 - acc: 0.6215 - val_loss: 7.3207 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.691 \t val: 0.012\n",
      "Epoch 230/300\n",
      " - 1s - loss: 3.1121 - acc: 0.6276 - val_loss: 7.2812 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.692 \t val: 0.012\n",
      "Epoch 231/300\n",
      " - 1s - loss: 3.1044 - acc: 0.6285 - val_loss: 7.2846 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.692 \t val: 0.012\n",
      "Epoch 232/300\n",
      " - 1s - loss: 3.1179 - acc: 0.6294 - val_loss: 7.3101 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.692 \t val: 0.012\n",
      "Epoch 233/300\n",
      " - 1s - loss: 3.1271 - acc: 0.6281 - val_loss: 7.3277 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.693 \t val: 0.012\n",
      "Epoch 234/300\n",
      " - 1s - loss: 3.0835 - acc: 0.6281 - val_loss: 7.2967 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.695 \t val: 0.012\n",
      "Epoch 235/300\n",
      " - 1s - loss: 3.1087 - acc: 0.6253 - val_loss: 7.3365 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.693 \t val: 0.012\n",
      "Epoch 236/300\n",
      " - 1s - loss: 3.1083 - acc: 0.6268 - val_loss: 7.3354 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.694 \t val: 0.012\n",
      "Epoch 237/300\n",
      " - 1s - loss: 3.0977 - acc: 0.6294 - val_loss: 7.3459 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.695 \t val: 0.012\n",
      "Epoch 238/300\n",
      " - 1s - loss: 3.0689 - acc: 0.6289 - val_loss: 7.4289 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.694 \t val: 0.012\n",
      "Epoch 239/300\n",
      " - 1s - loss: 3.0640 - acc: 0.6345 - val_loss: 7.3789 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.696 \t val: 0.012\n",
      "Epoch 240/300\n",
      " - 1s - loss: 3.0811 - acc: 0.6310 - val_loss: 7.4045 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.695 \t val: 0.012\n",
      "Epoch 241/300\n",
      " - 1s - loss: 3.0606 - acc: 0.6343 - val_loss: 7.4161 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.698 \t val: 0.012\n",
      "Epoch 242/300\n",
      " - 1s - loss: 3.0662 - acc: 0.6337 - val_loss: 7.4191 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.698 \t val: 0.012\n",
      "Epoch 243/300\n",
      " - 1s - loss: 3.0609 - acc: 0.6313 - val_loss: 7.4415 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.698 \t val: 0.012\n",
      "Epoch 244/300\n",
      " - 1s - loss: 3.0375 - acc: 0.6342 - val_loss: 7.4968 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.698 \t val: 0.012\n",
      "Epoch 245/300\n",
      " - 1s - loss: 3.0626 - acc: 0.6343 - val_loss: 7.5255 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.698 \t val: 0.012\n",
      "Epoch 246/300\n",
      " - 1s - loss: 3.0707 - acc: 0.6331 - val_loss: 7.5215 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.699 \t val: 0.012\n",
      "Epoch 247/300\n",
      " - 1s - loss: 3.0557 - acc: 0.6361 - val_loss: 7.5286 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.699 \t val: 0.012\n",
      "Epoch 248/300\n",
      " - 1s - loss: 3.0506 - acc: 0.6299 - val_loss: 7.5769 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.698 \t val: 0.012\n",
      "Epoch 249/300\n",
      " - 1s - loss: 3.0387 - acc: 0.6331 - val_loss: 7.5446 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.698 \t val: 0.012\n",
      "Epoch 250/300\n",
      " - 1s - loss: 3.0425 - acc: 0.6340 - val_loss: 7.5564 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.699 \t val: 0.012\n",
      "Epoch 251/300\n",
      " - 1s - loss: 2.9921 - acc: 0.6421 - val_loss: 7.5930 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.699 \t val: 0.012\n",
      "Epoch 252/300\n",
      " - 1s - loss: 3.0338 - acc: 0.6362 - val_loss: 7.6026 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.701 \t val: 0.012\n",
      "Epoch 253/300\n",
      " - 1s - loss: 3.0462 - acc: 0.6350 - val_loss: 7.4877 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.703 \t val: 0.012\n",
      "Epoch 254/300\n",
      " - 1s - loss: 3.0470 - acc: 0.6357 - val_loss: 7.5713 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.700 \t val: 0.012\n",
      "Epoch 255/300\n",
      " - 1s - loss: 3.0391 - acc: 0.6395 - val_loss: 7.5412 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.702 \t val: 0.012\n",
      "Epoch 256/300\n",
      " - 1s - loss: 3.0457 - acc: 0.6366 - val_loss: 7.5690 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.702 \t val: 0.012\n",
      "Epoch 257/300\n",
      " - 1s - loss: 3.0461 - acc: 0.6372 - val_loss: 7.5690 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.702 \t val: 0.012\n",
      "Epoch 258/300\n",
      " - 1s - loss: 3.0038 - acc: 0.6435 - val_loss: 7.5699 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.703 \t val: 0.012\n",
      "Epoch 259/300\n",
      " - 1s - loss: 3.0097 - acc: 0.6381 - val_loss: 7.5437 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.704 \t val: 0.012\n",
      "Epoch 260/300\n",
      " - 1s - loss: 3.0210 - acc: 0.6400 - val_loss: 7.5391 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.705 \t val: 0.012\n",
      "Epoch 261/300\n",
      " - 1s - loss: 3.0130 - acc: 0.6384 - val_loss: 7.5677 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.704 \t val: 0.012\n",
      "Epoch 262/300\n",
      " - 1s - loss: 2.9941 - acc: 0.6421 - val_loss: 7.5036 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.704 \t val: 0.012\n",
      "Epoch 263/300\n",
      " - 1s - loss: 2.9995 - acc: 0.6377 - val_loss: 7.5608 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.705 \t val: 0.012\n",
      "Epoch 264/300\n",
      " - 1s - loss: 2.9748 - acc: 0.6451 - val_loss: 7.6545 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.704 \t val: 0.012\n",
      "Epoch 265/300\n",
      " - 1s - loss: 2.9852 - acc: 0.6424 - val_loss: 7.7146 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.705 \t val: 0.012\n",
      "Epoch 266/300\n",
      " - 1s - loss: 2.9850 - acc: 0.6405 - val_loss: 7.6670 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.706 \t val: 0.012\n",
      "Epoch 267/300\n",
      " - 1s - loss: 2.9616 - acc: 0.6443 - val_loss: 7.6740 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.706 \t val: 0.012\n",
      "Epoch 268/300\n",
      " - 1s - loss: 2.9882 - acc: 0.6441 - val_loss: 7.6904 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.706 \t val: 0.012\n",
      "Epoch 269/300\n",
      " - 1s - loss: 2.9591 - acc: 0.6450 - val_loss: 7.6825 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.707 \t val: 0.012\n",
      "Epoch 270/300\n",
      " - 1s - loss: 2.9930 - acc: 0.6443 - val_loss: 7.6888 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.706 \t val: 0.012\n",
      "Epoch 271/300\n",
      " - 1s - loss: 2.9396 - acc: 0.6464 - val_loss: 7.6832 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.707 \t val: 0.012\n",
      "Epoch 272/300\n",
      " - 1s - loss: 2.9491 - acc: 0.6472 - val_loss: 7.6410 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.707 \t val: 0.012\n",
      "Epoch 273/300\n",
      " - 1s - loss: 2.9197 - acc: 0.6479 - val_loss: 7.6409 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.709 \t val: 0.012\n",
      "Epoch 274/300\n",
      " - 1s - loss: 2.9456 - acc: 0.6459 - val_loss: 7.6391 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.709 \t val: 0.012\n",
      "Epoch 275/300\n",
      " - 1s - loss: 2.9568 - acc: 0.6505 - val_loss: 7.6126 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.710 \t val: 0.012\n",
      "Epoch 276/300\n",
      " - 1s - loss: 2.9437 - acc: 0.6492 - val_loss: 7.6162 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.710 \t val: 0.012\n",
      "Epoch 277/300\n",
      " - 1s - loss: 2.9548 - acc: 0.6478 - val_loss: 7.7324 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.710 \t val: 0.012\n",
      "Epoch 278/300\n",
      " - 1s - loss: 2.9476 - acc: 0.6471 - val_loss: 7.7650 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.709 \t val: 0.012\n",
      "Epoch 279/300\n",
      " - 1s - loss: 2.9506 - acc: 0.6481 - val_loss: 7.6731 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.712 \t val: 0.012\n",
      "Epoch 280/300\n",
      " - 1s - loss: 2.9313 - acc: 0.6516 - val_loss: 7.7032 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.711 \t val: 0.012\n",
      "Epoch 281/300\n",
      " - 1s - loss: 2.9525 - acc: 0.6464 - val_loss: 7.7440 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.711 \t val: 0.012\n",
      "Epoch 282/300\n",
      " - 1s - loss: 2.9295 - acc: 0.6530 - val_loss: 7.7716 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.710 \t val: 0.012\n",
      "Epoch 283/300\n",
      " - 1s - loss: 2.9239 - acc: 0.6475 - val_loss: 7.8001 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.712 \t val: 0.012\n",
      "Epoch 284/300\n",
      " - 1s - loss: 2.9582 - acc: 0.6536 - val_loss: 7.7943 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.712 \t val: 0.012\n",
      "Epoch 285/300\n",
      " - 1s - loss: 2.9465 - acc: 0.6485 - val_loss: 7.7660 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.712 \t val: 0.012\n",
      "Epoch 286/300\n",
      " - 1s - loss: 2.9108 - acc: 0.6515 - val_loss: 7.7276 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.711 \t val: 0.012\n",
      "Epoch 287/300\n",
      " - 1s - loss: 2.9477 - acc: 0.6473 - val_loss: 7.7110 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.712 \t val: 0.012\n",
      "Epoch 288/300\n",
      " - 1s - loss: 2.9221 - acc: 0.6527 - val_loss: 7.7129 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.712 \t val: 0.012\n",
      "Epoch 289/300\n",
      " - 1s - loss: 2.9013 - acc: 0.6549 - val_loss: 7.8086 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.713 \t val: 0.012\n",
      "Epoch 290/300\n",
      " - 1s - loss: 2.9109 - acc: 0.6553 - val_loss: 7.8543 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.713 \t val: 0.012\n",
      "Epoch 291/300\n",
      " - 1s - loss: 2.9165 - acc: 0.6509 - val_loss: 7.7921 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.714 \t val: 0.012\n",
      "Epoch 292/300\n",
      " - 1s - loss: 2.9161 - acc: 0.6554 - val_loss: 7.8164 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.715 \t val: 0.012\n",
      "Epoch 293/300\n",
      " - 1s - loss: 2.9053 - acc: 0.6544 - val_loss: 7.8079 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.715 \t val: 0.012\n",
      "Epoch 294/300\n",
      " - 1s - loss: 2.8876 - acc: 0.6540 - val_loss: 7.8540 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.715 \t val: 0.012\n",
      "Epoch 295/300\n",
      " - 1s - loss: 2.9156 - acc: 0.6550 - val_loss: 7.8575 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.715 \t val: 0.012\n",
      "Epoch 296/300\n",
      " - 1s - loss: 2.9124 - acc: 0.6549 - val_loss: 7.8261 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.715 \t val: 0.012\n",
      "Epoch 297/300\n",
      " - 1s - loss: 2.8865 - acc: 0.6576 - val_loss: 7.8214 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.715 \t val: 0.012\n",
      "Epoch 298/300\n",
      " - 1s - loss: 2.8677 - acc: 0.6577 - val_loss: 7.8734 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.716 \t val: 0.012\n",
      "Epoch 299/300\n",
      " - 1s - loss: 2.8872 - acc: 0.6549 - val_loss: 7.9873 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.715 \t val: 0.012\n",
      "Epoch 300/300\n",
      " - 1s - loss: 2.9084 - acc: 0.6544 - val_loss: 7.9930 - val_acc: 0.0123\n",
      "\n",
      "Balanced Accuracy - train: 0.716 \t val: 0.012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe916c76048>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, kernel_initializer=\"uniform\", input_shape = (96,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, kernel_initializer=\"uniform\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "adam = Adam(lr = 0.00001)\n",
    "final_y_train = one_hot(final_y_train, 3)\n",
    "final_y_test = one_hot(final_y_test, 3)\n",
    "\n",
    "model.compile(loss=w_cat_crossentropy, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(final_X_train, final_y_train, epochs=300, batch_size=128, validation_data=(final_X_test, final_y_test), \n",
    "          verbose=2, shuffle=True, callbacks=CALLBACKS)\n",
    "\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#model.fit(final_X_train, y_train_one_hot, epochs=500, batch_size=128, validation_data=(final_X_test, y_val_one_hot), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.31  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.57  0.07] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.6   0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.36  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.6   0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.6   0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.54  0.11] [ 0.  0.  1.]\n",
      "[ 0.37  0.55  0.08] [ 0.  0.  1.]\n",
      "[ 0.37  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.37  0.56  0.07] [ 0.  1.  0.]\n",
      "[ 0.33  0.55  0.12] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.32  0.61  0.07] [ 0.  1.  0.]\n",
      "[ 0.32  0.56  0.12] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.36  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.08] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.3   0.62  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.37  0.52  0.11] [ 0.  0.  1.]\n",
      "[ 0.36  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.36  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.37  0.54  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 0.  0.  1.]\n",
      "[ 0.38  0.54  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.37  0.54  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.36  0.55  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.36  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.6   0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.36  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.3   0.61  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.08] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.31  0.6   0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.54  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.38  0.54  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.36  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.62  0.06] [ 0.  1.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.37  0.53  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.54  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.32  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.6   0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.37  0.55  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.59  0.07] [ 0.  1.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 0.  0.  1.]\n",
      "[ 0.36  0.58  0.06] [ 0.  1.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.54  0.11] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.6   0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.36  0.56  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.31  0.6   0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.3  0.6  0.1] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.36  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.31  0.61  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.56  0.13] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.59  0.07] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.55  0.12] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.11] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.6   0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.37  0.54  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.42  0.49  0.08] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.38  0.54  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.12] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.54  0.11] [ 0.  0.  1.]\n",
      "[ 0.37  0.55  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.54  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.54  0.13] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.39  0.52  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.38  0.54  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.37  0.55  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.56  0.12] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.37  0.54  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.6   0.08] [ 0.  0.  1.]\n",
      "[ 0.36  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.36  0.54  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.6   0.07] [ 0.  1.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.31  0.6   0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.6   0.08] [ 0.  0.  1.]\n",
      "[ 0.31  0.58  0.1 ] [ 0.  1.  0.]\n",
      "[ 0.36  0.55  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.55  0.13] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.36  0.54  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.38  0.54  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.57  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.3   0.61  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.31  0.58  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.59  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.36  0.58  0.06] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.54  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.37  0.55  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.56  0.13] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.37  0.55  0.08] [ 0.  0.  1.]\n",
      "[ 0.31  0.62  0.07] [ 0.  1.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.36  0.56  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.54  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 0.  0.  1.]\n",
      "[ 0.34  0.53  0.13] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.54  0.12] [ 1.  0.  0.]\n",
      "[ 0.37  0.5   0.12] [ 0.  0.  1.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.38  0.54  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.61  0.07] [ 0.  1.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.31  0.6   0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.6   0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.54  0.12] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.39  0.52  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.36  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.36  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.08] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.36  0.55  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.56  0.08] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.37  0.53  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.59  0.08] [ 0.  1.  0.]\n",
      "[ 0.35  0.54  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.58  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.36  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.36  0.58  0.06] [ 0.  1.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.37  0.54  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.38  0.56  0.06] [ 0.  1.  0.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.4   0.51  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.33  0.54  0.13] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.31  0.6   0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.55  0.12] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.59  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.3   0.63  0.07] [ 0.  1.  0.]\n",
      "[ 0.32  0.6   0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.09] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.55  0.12] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.54  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.57  0.11] [ 0.  0.  1.]\n",
      "[ 0.37  0.54  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.11] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.32  0.59  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 1.  0.  0.]\n",
      "[ 0.33  0.59  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.32  0.57  0.12] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.32  0.58  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.36  0.55  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.57  0.08] [ 1.  0.  0.]\n",
      "[ 0.35  0.56  0.08] [ 0.  0.  1.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 0.  0.  1.]\n",
      "[ 0.35  0.57  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.58  0.09] [ 1.  0.  0.]\n",
      "[ 0.37  0.54  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.55  0.12] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.36  0.55  0.08] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.32  0.6   0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.55  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.35  0.55  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.55  0.12] [ 1.  0.  0.]\n",
      "[ 0.33  0.58  0.09] [ 0.  0.  1.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "[ 0.34  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.33  0.56  0.1 ] [ 1.  0.  0.]\n",
      "[ 0.31  0.6   0.09] [ 1.  0.  0.]\n",
      "[ 0.33  0.57  0.11] [ 1.  0.  0.]\n",
      "[ 0.34  0.58  0.08] [ 0.  0.  1.]\n",
      "[ 0.35  0.56  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 0.  0.  1.]\n",
      "[ 0.34  0.57  0.09] [ 1.  0.  0.]\n",
      "Confusion matrix, without normalization\n",
      "[[  0 645   0]\n",
      " [  0  13   0]\n",
      " [  0 399   0]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEYCAYAAADLZOR0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9a141c128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEYCAYAAAApuP8NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe91695f780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = model.predict(final_X_test)\n",
    "\n",
    "for pred, label in zip(preds, final_y_test):\n",
    "    print(pred, label)\n",
    "\n",
    "# decode one-hot to single labels\n",
    "preds = [ np.argmax(pred, axis = 0) for pred in preds ]\n",
    "labels = [ np.argmax(label, axis = 0) for label in final_y_test ]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(labels, preds)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"non-frail\", \"pre-frail\", \"frail\"],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"non-frail\", \"pre-frail\", \"frail\"], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
